"builtin.module"() ({
  "func.func"() <{function_type = (index, vector<18x6x6xi64>) -> index, sym_name = "func1", sym_visibility = "nested"}> ({
  ^bb0(%arg0: index, %arg1: vector<18x6x6xi64>):
    %0 = "arith.constant"() <{value = -4827 : i16}> : () -> i16
    %1 = "arith.constant"() <{value = 6.284000e+03 : f16}> : () -> f16
    %2 = "arith.constant"() <{value = 0x4E166186 : f32}> : () -> f32
    %3 = "arith.constant"() <{value = -25820 : i16}> : () -> i16
    %4 = "arith.constant"() <{value = true}> : () -> i1
    %5 = "arith.constant"() <{value = 1955529960 : i64}> : () -> i64
    %6 = "arith.constant"() <{value = 2.328000e+03 : f16}> : () -> f16
    %7 = "arith.constant"() <{value = true}> : () -> i1
    %8 = "arith.constant"() <{value = 0x4D688185 : f32}> : () -> f32
    %9 = "arith.constant"() <{value = true}> : () -> i1
    %10 = "arith.constant"() <{value = 2007147171 : i32}> : () -> i32
    %11 = "arith.constant"() <{value = 1.0746775E+9 : f32}> : () -> f32
    %12 = "arith.constant"() <{value = 0x4E0672E3 : f32}> : () -> f32
    %13 = "arith.constant"() <{value = -23368 : i16}> : () -> i16
    %14 = "arith.constant"() <{value = 231394892 : i32}> : () -> i32
    %15 = "arith.constant"() <{value = 1270722384 : i32}> : () -> i32
    %16 = "arith.constant"() <{value = 0 : index}> : () -> index
    %17 = "arith.constant"() <{value = 1 : index}> : () -> index
    %18 = "arith.constant"() <{value = 2 : index}> : () -> index
    %19 = "arith.constant"() <{value = 3 : index}> : () -> index
    %20 = "arith.constant"() <{value = 4 : index}> : () -> index
    %21 = "arith.constant"() <{value = 5 : index}> : () -> index
    %22 = "arith.constant"() <{value = 6 : index}> : () -> index
    %23 = "arith.constant"() <{value = 7 : index}> : () -> index
    %24 = "arith.constant"() <{value = 8 : index}> : () -> index
    %25 = "arith.constant"() <{value = 9 : index}> : () -> index
    %26 = "arith.constant"() <{value = 10 : index}> : () -> index
    %27 = "arith.constant"() <{value = 11 : index}> : () -> index
    %28 = "arith.constant"() <{value = 12 : index}> : () -> index
    %29 = "arith.constant"() <{value = 13 : index}> : () -> index
    %30 = "arith.constant"() <{value = 14 : index}> : () -> index
    %31 = "arith.constant"() <{value = 15 : index}> : () -> index
    %32 = "arith.constant"() <{value = 16 : index}> : () -> index
    %33 = "arith.constant"() <{value = 17 : index}> : () -> index
    %34 = "arith.constant"() <{value = 18 : index}> : () -> index
    %35 = "arith.constant"() <{value = 19 : index}> : () -> index
    %36 = "arith.constant"() <{value = 20 : index}> : () -> index
    %37 = "arith.constant"() <{value = 21 : index}> : () -> index
    %38 = "arith.constant"() <{value = 22 : index}> : () -> index
    %39 = "arith.constant"() <{value = 23 : index}> : () -> index
    %40 = "arith.constant"() <{value = 24 : index}> : () -> index
    %41 = "arith.constant"() <{value = 25 : index}> : () -> index
    %42 = "arith.constant"() <{value = 26 : index}> : () -> index
    %43 = "arith.constant"() <{value = 27 : index}> : () -> index
    %44 = "arith.constant"() <{value = 28 : index}> : () -> index
    %45 = "arith.constant"() <{value = 29 : index}> : () -> index
    %46 = "arith.constant"() <{value = 30 : index}> : () -> index
    %47 = "arith.constant"() <{value = 31 : index}> : () -> index
    %48 = "tensor.empty"(%26) : (index) -> tensor<?xf16>
    %49 = "tensor.empty"(%40) : (index) -> tensor<?xi16>
    %50 = "tensor.empty"(%24, %36, %18) : (index, index, index) -> tensor<?x?x?xi16>
    %51 = "tensor.empty"() : () -> tensor<6xi32>
    %52 = "tensor.empty"(%19) : (index) -> tensor<?x18x6xi16>
    %53 = "tensor.empty"(%23) : (index) -> tensor<?xi32>
    %54 = "tensor.empty"() : () -> tensor<6xi16>
    %55 = "tensor.empty"(%24, %21, %24) : (index, index, index) -> tensor<?x?x?xi64>
    %56 = "tensor.empty"() : () -> tensor<6xi64>
    %57 = "tensor.empty"(%18, %46) : (index, index) -> tensor<?x?x17xi32>
    %58 = "tensor.empty"() : () -> tensor<6x22x17xf16>
    %59 = "tensor.empty"(%34, %24, %23) : (index, index, index) -> tensor<?x?x?xi16>
    %60 = "tensor.empty"(%35) : (index) -> tensor<?xi64>
    %61 = "tensor.empty"(%29, %35, %34) : (index, index, index) -> tensor<?x?x?xi16>
    %62 = "tensor.empty"() : () -> tensor<18x6x6xi1>
    %63 = "tensor.empty"() : () -> tensor<17x18x6xf32>
    %64 = "memref.alloc"(%33, %42) <{operandSegmentSizes = array<i32: 2, 0>}> : (index, index) -> memref<?x?x6xf16>
    %65 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<6x22x17xf32>
    %66 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<6x22x17xi16>
    %67 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<18x6x6xf16>
    %68 = "memref.alloc"(%40) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xf32>
    %69 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<6x22x17xi64>
    %70 = "memref.alloc"(%38, %32, %arg0) <{operandSegmentSizes = array<i32: 3, 0>}> : (index, index, index) -> memref<?x?x?xf32>
    %71 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<17x18x6xi32>
    %72 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<6xf16>
    %73 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<6x22x17xf16>
    %74 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<6xi64>
    %75 = "memref.alloc"(%43) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?x18x6xi1>
    %76 = "memref.alloc"(%24) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xf16>
    %77 = "memref.alloc"(%42, %23, %19) <{operandSegmentSizes = array<i32: 3, 0>}> : (index, index, index) -> memref<?x?x?xi32>
    %78 = "memref.alloc"(%35, %25, %19) <{operandSegmentSizes = array<i32: 3, 0>}> : (index, index, index) -> memref<?x?x?xi1>
    %79 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<18x6x6xi32>
    %80 = "tensor.dim"(%61, %18) : (tensor<?x?x?xi16>, index) -> index
    %81 = "spirv.CL.exp"(%12) : (f32) -> f32
    %82 = "spirv.GL.Cosh"(%11) : (f32) -> f32
    %83 = "spirv.LogicalNot"(%4) : (i1) -> i1
    %84 = "spirv.CL.rsqrt"(%11) : (f32) -> f32
    %85 = "math.rsqrt"(%63) <{fastmath = #arith.fastmath<none>}> : (tensor<17x18x6xf32>) -> tensor<17x18x6xf32>
    %86 = "spirv.GL.FClamp"(%6, %1, %6) : (f16, f16, f16) -> f16
    %87 = "spirv.CL.rsqrt"(%12) : (f32) -> f32
    %88 = "spirv.CL.sin"(%87) : (f32) -> f32
    %89 = "index.and"(%47, %41) : (index, index) -> index
    %90 = "arith.muli"(%4, %7) : (i1, i1) -> i1
    %91 = "index.and"(%21, %46) : (index, index) -> index
    %92 = "bufferization.clone"(%79) : (memref<18x6x6xi32>) -> memref<18x6x6xi32>
    %93 = "spirv.LogicalEqual"(%4, %7) : (i1, i1) -> i1
    %94 = "tensor.generate"(%28) ({
    ^bb0(%arg2: index):
      %210 = "arith.xori"(%93, %9) : (i1, i1) -> i1
      %211 = "index.castu"(%44) : (index) -> i32
      %212 = "vector.broadcast"(%84) : (f32) -> vector<1xf32>
      %213 = "vector.insert"(%84, %212) <{static_position = array<i64: 0>}> : (f32, vector<1xf32>) -> vector<1xf32>
      %214 = "index.and"(%27, %22) : (index, index) -> index
      "tensor.yield"(%81) : (f32) -> ()
    }) : (index) -> tensor<?xf32>
    %95 = "spirv.CL.fma"(%81, %84, %81) : (f32, f32, f32) -> f32
    %96 = "tensor.from_elements"(%1, %6, %86, %1, %86, %86, %86, %1, %6, %6, %6, %6, %86, %1, %1, %6, %86, %6, %6, %6, %6, %6, %86, %86, %6, %1, %6, %6, %6, %86, %6, %6, %1, %86, %1, %6, %1, %86, %1, %1, %1, %6, %6, %1, %1, %86, %86, %86, %86, %6, %86, %86, %1, %6, %86, %86, %86, %1, %86, %1, %6, %6, %1, %1, %86, %6, %6, %1, %1, %6, %6, %86, %86, %1, %6, %86, %1, %86, %86, %1, %6, %86, %1, %1, %1, %86, %6, %6, %6, %86, %1, %6, %6, %6, %6, %1, %6, %6, %6, %1, %6, %6, %6, %86, %6, %6, %86, %6, %86, %86, %6, %86, %1, %1, %1, %1, %86, %1, %1, %86, %6, %86, %6, %86, %6, %6, %86, %86, %86, %1, %1, %86, %1, %86, %6, %1, %6, %6, %86, %1, %86, %86, %1, %1, %6, %6, %86, %6, %6, %86, %1, %1, %86, %1, %86, %6, %6, %6, %1, %1, %86, %6, %86, %86, %6, %1, %1, %6, %86, %86, %6, %86, %6, %6, %6, %86, %6, %1, %86, %1, %6, %6, %6, %86, %6, %86, %1, %86, %6, %1, %1, %6, %1, %6, %6, %1, %1, %86, %6, %6, %1, %1, %86, %6, %6, %6, %86, %86, %1, %1, %86, %86, %6, %6, %86, %1, %86, %1, %1, %1, %1, %1, %86, %86, %1, %1, %86, %86, %86, %1, %86, %86, %6, %6, %86, %1, %1, %6, %1, %6, %6, %6, %86, %86, %1, %1, %1, %86, %1, %1, %1, %1, %1, %1, %1, %1, %86, %1, %6, %86, %1, %1, %86, %86, %1, %86, %86, %1, %86, %86, %6, %86, %1, %1, %1, %1, %86, %1, %6, %86, %1, %1, %1, %86, %86, %1, %1, %86, %1, %86, %86, %6, %6, %1, %6, %6, %1, %1, %6, %6, %86, %6, %6, %86, %1, %6, %6, %1, %6, %86, %86, %1, %86, %1, %1, %86, %6, %1, %6, %86, %86, %6, %1, %1, %1, %86, %1, %86, %1, %6, %6, %1, %1, %86, %6, %86, %6, %86, %86, %6, %1, %1, %86, %1, %1, %6, %6, %86, %86, %86, %86, %6, %86, %6, %86, %1, %6, %86, %6, %86, %86, %1, %6, %6, %1, %86, %6, %86, %6, %86, %6, %6, %1, %86, %1, %6, %1, %86, %86, %1, %1, %86, %6, %86, %1, %1, %86, %86, %1, %6, %1, %1, %1, %6, %86, %6, %1, %1, %6, %1, %86, %1, %86, %1, %1, %86, %86, %6, %6, %6, %86, %86, %1, %1, %1, %6, %86, %6, %6, %86, %6, %6, %6, %1, %86, %86, %86, %86, %1, %6, %6, %86, %6, %1, %1, %1, %6, %6, %86, %1, %1, %6, %86, %6, %6, %6, %1, %86, %86, %1, %1, %86, %1, %86, %6, %1, %1, %6, %6, %1, %86, %86, %86, %86, %1, %6, %86, %86, %6, %86, %86, %1, %86, %6, %6, %86, %1, %6, %86, %1, %86, %6, %86, %6, %86, %6, %1, %86, %1, %6, %1, %1, %6, %86, %86, %1, %6, %6, %86, %6, %6, %86, %1, %1, %1, %6, %86, %86, %6, %86, %86, %86, %1, %6, %6, %1, %1, %6, %1, %1, %1, %86, %86, %6, %6, %1, %6, %1, %6, %1, %86, %86, %6, %86, %1, %6, %86, %86, %86, %6, %86, %86, %1, %1, %1, %86, %86, %1, %86, %86, %6, %1, %86, %6, %6, %86, %86, %1, %6, %1, %86, %6, %86, %86, %86, %6, %1, %1, %86, %6, %1, %6, %86, %1, %6, %6, %6, %6, %1, %6, %6, %1, %6, %86, %1, %6, %6, %1, %1, %86, %6, %6, %86, %1, %86, %6, %6, %86, %1, %1, %1, %6, %1, %86, %1, %86, %86, %1, %6, %6, %86, %1, %1, %6, %1, %86, %86, %1, %86, %6, %1, %1, %6, %1, %86, %6, %1, %86, %6, %6, %1, %6, %6, %86, %6, %1, %86, %86, %6, %86, %1, %1, %6, %86, %86, %6, %1, %86) : (f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16) -> tensor<18x6x6xf16>
    %97 = "spirv.GL.FClamp"(%87, %84, %95) : (f32, f32, f32) -> f32
    %98 = "arith.minsi"(%93, %83) : (i1, i1) -> i1
    "bufferization.dealloc_tensor"(%96) : (tensor<18x6x6xf16>) -> ()
    %99 = "bufferization.to_tensor"(%72) : (memref<6xf16>) -> tensor<6xf16>
    %100 = "tensor.insert"(%15, %51, %17) : (i32, tensor<6xi32>, index) -> tensor<6xi32>
    %101 = "spirv.CL.fabs"(%97) : (f32) -> f32
    %102 = "tensor.rank"(%61) : (tensor<?x?x?xi16>) -> index
    %103 = "arith.negf"(%97) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %104 = "vector.broadcast"(%1) : (f16) -> vector<6x22x17xf16>
    %105 = "vector.transpose"(%104) <{transp = [2, 1, 0]}> : (vector<6x22x17xf16>) -> vector<17x22x6xf16>
    %106 = "spirv.IsNan"(%82) : (f32) -> i1
    %107 = "index.sizeof"() : () -> index
    %108 = "index.floordivs"(%43, %107) : (index, index) -> index
    %109 = "tensor.generate"(%22) ({
    ^bb0(%arg2: index):
      %210 = "index.shru"(%47, %47) : (index, index) -> index
      %211 = "vector.load"(%70, %16, %16, %16) : (memref<?x?x?xf32>, index, index, index) -> vector<17x18x6xf32>
      %212 = "bufferization.clone"(%65) : (memref<6x22x17xf32>) -> memref<6x22x17xf32>
      %213 = "math.atan"(%81) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      "tensor.yield"(%6) : (f16) -> ()
    }) : (index) -> tensor<?xf16>
    %110 = "spirv.CL.fma"(%86, %6, %1) : (f16, f16, f16) -> f16
    %111 = "spirv.FOrdLessThanEqual"(%97, %11) : (f32, f32) -> i1
    %112 = "arith.cmpi"(%7, %7) <{predicate = 4 : i64}> : (i1, i1) -> i1
    %113 = "spirv.BitCount"(%15) : (i32) -> i32
    %114 = "affine.apply"(%29, %89) <{map = affine_map<(d0, d1) -> (d0 + d0 - d1 + 4)>}> : (index, index) -> index
    %115 = "spirv.FOrdLessThanEqual"(%95, %8) : (f32, f32) -> i1
    %116 = "spirv.CL.exp"(%1) : (f16) -> f16
    %117 = "math.powf"(%110, %116) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
    %118 = "vector.broadcast"(%116) : (f16) -> vector<6xf16>
    %119 = "vector.multi_reduction"(%104, %118) <{kind = #vector.kind<add>, reduction_dims = [1, 2]}> : (vector<6x22x17xf16>, vector<6xf16>) -> vector<6xf16>
    "memref.assume_alignment"(%66) <{alignment = 1 : i32}> : (memref<6x22x17xi16>) -> ()
    %120 = "memref.alloca"(%17) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?x6x6xi1>
    %121 = "spirv.GL.SMax"(%14, %14) : (i32, i32) -> i32
    %122 = "memref.realloc"(%76) : (memref<?xf16>) -> memref<22xf16>
    %123 = "arith.ceildivsi"(%14, %15) : (i32, i32) -> i32
    %124 = "bufferization.to_tensor"(%65) : (memref<6x22x17xf32>) -> tensor<6x22x17xf32>
    %125 = "spirv.CL.pow"(%88, %12) : (f32, f32) -> f32
    %126 = "math.copysign"(%58, %58) <{fastmath = #arith.fastmath<none>}> : (tensor<6x22x17xf16>, tensor<6x22x17xf16>) -> tensor<6x22x17xf16>
    %127 = "spirv.FUnordGreaterThan"(%1, %1) : (f16, f16) -> i1
    %128 = "spirv.CL.u_min"(%113, %10) : (i32, i32) -> i32
    %129 = "spirv.LogicalNotEqual"(%93, %115) : (i1, i1) -> i1
    %130 = "tensor.from_elements"(%6, %1, %6, %86, %6, %110) : (f16, f16, f16, f16, f16, f16) -> tensor<6xf16>
    %131 = "spirv.GL.Floor"(%8) : (f32) -> f32
    %132 = "math.fma"(%2, %97, %81) <{fastmath = #arith.fastmath<none>}> : (f32, f32, f32) -> f32
    %133 = "vector.broadcast"(%128) : (i32) -> vector<2xi32>
    %134 = "spirv.BitFieldUExtract"(%133, %113, %113) : (vector<2xi32>, i32, i32) -> vector<2xi32>
    %135 = "memref.alloc"(%47) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?x22x17xf32>
    %136 = "spirv.IsNan"(%116) : (f16) -> i1
    %137 = "spirv.CL.floor"(%2) : (f32) -> f32
    "memref.copy"(%75, %75) : (memref<?x18x6xi1>, memref<?x18x6xi1>) -> ()
    %138 = "spirv.CL.fmin"(%81, %131) : (f32, f32) -> f32
    %139 = "bufferization.to_memref"(%51) : (tensor<6xi32>) -> memref<6xi32>
    %140 = "vector.broadcast"(%1) : (f16) -> vector<22x17xf16>
    %141 = "vector.multi_reduction"(%104, %140) <{kind = #vector.kind<minf>, reduction_dims = [0]}> : (vector<6x22x17xf16>, vector<22x17xf16>) -> vector<22x17xf16>
    %142 = "spirv.CL.cos"(%2) : (f32) -> f32
    %143 = "arith.floordivsi"(%10, %113) : (i32, i32) -> i32
    %144 = "spirv.CL.exp"(%87) : (f32) -> f32
    %145 = "spirv.GL.SMin"(%13, %13) : (i16, i16) -> i16
    %146 = "spirv.CL.rsqrt"(%110) : (f16) -> f16
    %147 = "index.maxu"(%16, %32) : (index, index) -> index
    %148 = "math.ctlz"(%62) : (tensor<18x6x6xi1>) -> tensor<18x6x6xi1>
    %149 = "tensor.extract"(%57, %16, %16, %27) : (tensor<?x?x17xi32>, index, index, index) -> i32
    %150 = "math.fma"(%58, %58, %58) <{fastmath = #arith.fastmath<none>}> : (tensor<6x22x17xf16>, tensor<6x22x17xf16>, tensor<6x22x17xf16>) -> tensor<6x22x17xf16>
    %151 = "spirv.CL.fabs"(%142) : (f32) -> f32
    %152 = "spirv.FOrdLessThanEqual"(%95, %97) : (f32, f32) -> i1
    %153 = "spirv.BitwiseAnd"(%133, %133) : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %154 = "spirv.FUnordLessThanEqual"(%1, %110) : (f16, f16) -> i1
    %155 = "vector.matrix_multiply"(%118, %118) <{lhs_columns = 6 : i32, lhs_rows = 1 : i32, rhs_columns = 1 : i32}> : (vector<6xf16>, vector<6xf16>) -> vector<1xf16>
    %156 = "spirv.BitFieldInsert"(%133, %133, %113, %121) : (vector<2xi32>, vector<2xi32>, i32, i32) -> vector<2xi32>
    %157 = "arith.constant"() <{value = 2.09500147E+9 : f32}> : () -> f32
    %158 = "bufferization.to_tensor"(%71) : (memref<17x18x6xi32>) -> tensor<17x18x6xi32>
    %159 = "affine.vector_load"(%73, %114, %25, %91) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> : (memref<6x22x17xf16>, index, index, index) -> vector<22xf16>
    %160 = "spirv.CL.u_min"(%13, %145) : (i16, i16) -> i16
    %161 = "tensor.from_elements"(%160, %0, %0, %3, %160, %3, %3, %13, %13, %160, %3, %13, %3, %145, %13, %13, %160, %3, %13, %3, %160, %160, %13, %0, %13, %3, %160, %13, %13, %160, %13, %145, %13, %160, %13, %145, %160, %160, %13, %0, %13, %145, %13, %13, %0, %13, %145, %13, %145, %3, %145, %160, %0, %145, %160, %160, %145, %160, %13, %145, %0, %13, %145, %160, %160, %160, %145, %160, %3, %160, %145, %145, %145, %145, %145, %13, %145, %3, %13, %160, %145, %145, %160, %13, %3, %160, %13, %160, %145, %0, %145, %160, %160, %3, %0, %13, %3, %145, %0, %160, %0, %160, %145, %0, %145, %3, %0, %13, %145, %145, %3, %145, %160, %145, %145, %0, %145, %160, %145, %145, %13, %13, %3, %13, %13, %0, %3, %13, %145, %145, %13, %145, %13, %160, %0, %0, %13, %13, %3, %0, %0, %13, %160, %3, %160, %0, %145, %145, %0, %160, %0, %3, %3, %3, %0, %145, %160, %145, %160, %3, %0, %3, %145, %3, %145, %160, %160, %13, %0, %145, %3, %13, %13, %145, %13, %13, %145, %13, %3, %3, %3, %13, %13, %145, %160, %0, %145, %145, %160, %13, %145, %0, %145, %0, %0, %3, %13, %0, %13, %3, %160, %145, %13, %160, %145, %3, %13, %3, %13, %3, %145, %3, %13, %3, %145, %13, %13, %160, %0, %13, %13, %0, %3, %0, %160, %145, %145, %13, %0, %145, %13, %145, %145, %13, %13, %160, %0, %13, %145, %3, %3, %145, %160, %145, %0, %145, %160, %13, %13, %0, %13, %160, %0, %145, %160, %0, %3, %13, %145, %145, %145, %160, %0, %160, %145, %160, %3, %145, %0, %13, %3, %160, %160, %0, %13, %160, %160, %160, %0, %13, %3, %0, %160, %145, %3, %3, %145, %0, %145, %0, %145, %3, %0, %3, %0, %13, %0, %0, %3, %160, %0, %145, %160, %3, %0, %13, %0, %3, %160, %145, %3, %0, %0, %13, %0, %3, %160, %145, %13, %160, %0, %3, %145, %13, %160, %145, %160, %0, %160, %3, %160, %3, %160, %3, %13, %160, %145, %3, %160, %160, %3, %145, %0, %160, %0, %3, %145, %0, %145, %0, %3, %13, %160, %13, %0, %0, %160, %3, %13, %3, %145, %13, %13, %13, %0, %3, %0, %13, %0, %145, %13, %160, %13, %145, %0, %3, %160, %3, %0, %145, %0, %3, %13, %160, %13, %0, %0, %145, %0, %3, %160, %3, %3, %0, %3, %160, %3, %13, %3, %13, %0, %160, %0, %3, %3, %3, %0, %13, %13, %145, %3, %3, %160, %13, %160, %0, %13, %145, %0, %145, %145, %0, %160, %160, %0, %13, %0, %160, %13, %145, %145, %160, %3, %13, %145, %13, %0, %145, %145, %145, %3, %160, %145, %3, %0, %160, %145, %13, %160, %0, %145, %160, %145, %145, %13, %3, %0, %3, %13, %3, %3, %145, %145, %145, %160, %160, %160, %13, %160, %13, %3, %145, %0, %3, %0, %160, %145, %0, %145, %0, %3, %145, %145, %3, %3, %3, %3, %3, %145, %0, %145, %0, %145, %0, %160, %13, %160, %3, %0, %3, %145, %145, %0, %13, %3, %160, %13, %145, %0, %160, %13, %145, %13, %0, %160, %13, %145, %0, %13, %145, %160, %160, %13, %13, %3, %13, %3, %13, %160, %0, %13, %160, %3, %160, %13, %0, %3, %145, %0, %13, %13, %0, %3, %145, %160, %0, %160, %145, %3, %0, %3, %145, %145, %160, %0, %0, %0, %0, %0, %13, %3, %3, %3, %3, %13, %160, %160, %160, %13, %160, %160, %145, %0, %145, %13, %0, %0, %145, %13, %3, %0, %145, %145, %160, %3, %13, %0, %13, %3, %0, %0, %3, %160, %145, %145, %145, %145, %160, %160, %3, %160, %160, %160, %3, %13, %3, %13, %160, %145, %145, %13, %0, %13, %160, %145, %160, %0, %13, %13, %160, %160, %3, %160, %145, %3, %3, %145, %145, %0, %160, %13, %3, %13, %160, %145, %0, %145, %3, %0, %0, %3, %145, %160, %0, %160, %145, %13, %160, %3, %0, %145, %3, %145, %0, %3, %0, %13, %160, %13, %160, %13, %3, %3, %3, %3, %160, %0, %3, %13, %13, %160, %0, %0, %145, %0, %3, %160, %13, %0, %160, %160, %145, %160, %145, %0, %160, %13, %145, %145, %145, %145, %3, %3, %160, %13, %13, %13, %13, %13, %145, %145, %145, %0, %3, %0, %145, %145, %160, %3, %160, %0, %13, %145, %3, %0, %160, %13, %0, %13, %13, %0, %13, %3, %13, %13, %145, %13, %0, %3, %0, %145, %145, %0, %145, %160, %13, %3, %13, %160, %3, %13, %145, %0, %160, %3, %3, %3, %13, %145, %145, %3, %13, %145, %0, %160, %160, %0, %0, %13, %160, %0, %0, %3, %160, %0, %13, %13, %13, %160, %3, %13, %160, %160, %3, %3, %3, %160, %160, %160, %13, %0, %160, %145, %145, %145, %145, %160, %145, %0, %145, %145, %13, %13, %3, %13, %145, %145, %0, %13, %145, %160, %160, %0, %160, %0, %3, %145, %160, %13, %0, %13, %160, %13, %0, %160, %13, %3, %145, %13, %145, %160, %160, %13, %3, %145, %3, %0, %3, %3, %145, %145, %3, %145, %13, %0, %3, %0, %3, %145, %13, %3, %145, %145, %13, %3, %3, %160, %145, %13, %13, %3, %0, %3, %0, %3, %13, %0, %0, %0, %160, %0, %145, %3, %3, %3, %160, %13, %160, %0, %13, %3, %3, %160, %145, %3, %145, %0, %13, %13, %145, %0, %3, %0, %160, %160, %160, %145, %160, %145, %13, %0, %160, %145, %160, %0, %3, %145, %160, %13, %160, %0, %13, %13, %13, %145, %145, %13, %13, %3, %3, %13, %0, %13, %13, %13, %3, %3, %145, %145, %13, %145, %145, %160, %0, %145, %13, %160, %160, %13, %13, %160, %145, %0, %160, %13, %145, %0, %145, %13, %160, %0, %160, %145, %160, %145, %3, %0, %13, %145, %160, %3, %3, %13, %145, %145, %0, %160, %160, %13, %145, %3, %160, %145, %145, %160, %13, %0, %0, %3, %160, %13, %0, %145, %160, %145, %145, %0, %3, %13, %160, %145, %145, %3, %160, %0, %160, %0, %13, %0, %0, %13, %3, %160, %0, %145, %145, %145, %13, %3, %13, %145, %145, %3, %0, %145, %160, %160, %160, %160, %160, %13, %160, %160, %160, %0, %160, %0, %160, %3, %13, %145, %3, %3, %3, %13, %13, %0, %160, %0, %0, %0, %0, %145, %3, %145, %3, %13, %3, %3, %145, %13, %160, %160, %13, %160, %145, %3, %13, %145, %0, %0, %0, %160, %145, %145, %13, %13, %0, %13, %160, %0, %13, %13, %145, %145, %145, %3, %13, %160, %160, %13, %0, %160, %145, %160, %0, %160, %160, %0, %0, %0, %3, %13, %145, %13, %0, %0, %145, %0, %3, %145, %3, %145, %13, %160, %0, %3, %3, %0, %13, %160, %145, %13, %3, %3, %13, %160, %160, %0, %160, %13, %160, %3, %145, %3, %0, %13, %160, %0, %145, %160, %13, %3, %0, %160, %0, %3, %0, %160, %0, %145, %160, %160, %145, %160, %0, %13, %145, %145, %13, %145, %160, %3, %160, %0, %13, %145, %160, %0, %160, %145, %3, %13, %3, %3, %0, %160, %160, %13, %160, %145, %160, %13, %0, %160, %3, %0, %145, %145, %13, %160, %3, %160, %13, %0, %0, %160, %3, %3, %0, %145, %145, %0, %3, %3, %0, %0, %145, %13, %145, %3, %160, %3, %3, %13, %160, %160, %160, %145, %145, %3, %160, %145, %160, %145, %145, %3, %145, %145, %145, %13, %145, %145, %145, %160, %13, %13, %145, %0, %0, %145, %160, %145, %145, %0, %13, %145, %145, %160, %160, %160, %0, %3, %0, %0, %13, %13, %3, %3, %145, %0, %13, %0, %13, %145, %160, %3, %13, %13, %13, %160, %13, %145, %160, %0, %145, %160, %145, %13, %13, %3, %0, %160, %13, %0, %0, %145, %3, %145, %160, %145, %145, %160, %160, %0, %145, %13, %3, %0, %160, %13, %3, %145, %0, %13, %145, %145, %13, %13, %160, %145, %3, %160, %3, %0, %145, %3, %3, %13, %13, %145, %13, %145, %145, %160, %160, %3, %0, %3, %145, %3, %3, %13, %0, %0, %0, %145, %0, %0, %0, %160, %13, %3, %160, %0, %3, %13, %145, %0, %3, %160, %145, %145, %0, %145, %0, %160, %3, %3, %3, %13, %145, %13, %145, %13, %3, %0, %0, %3, %160, %160, %0, %160, %160, %3, %13, %3, %160, %160, %160, %160, %0, %3, %13, %13, %0, %160, %145, %3, %3, %0, %13, %145, %160, %3, %145, %0, %13, %160, %3, %13, %3, %3, %13, %3, %3, %160, %160, %13, %13, %3, %13, %3, %145, %13, %145, %13, %160, %3, %13, %0, %13, %13, %13, %0, %0, %3, %3, %13, %0, %13, %160, %160, %3, %145, %0, %13, %3, %13, %0, %160, %145, %160, %13, %160, %13, %145, %3, %3, %13, %3, %145, %145, %0, %3, %145, %160, %3, %0, %13, %160, %13, %160, %0, %3, %0, %3, %3, %145, %0, %0, %0, %145, %3, %13, %13, %13, %13, %145, %160, %0, %13, %3, %0, %3, %160, %3, %0, %0, %160, %145, %145, %3, %160, %145, %3, %160, %145, %13, %13, %145, %160, %145, %0, %13, %145, %145, %13, %13, %13, %160, %160, %13, %145, %0, %3, %145, %13, %160, %13, %3, %0, %3, %3, %0, %13, %0, %13, %160, %0, %13, %13, %160, %145, %145, %3, %13, %3, %13, %160, %3, %13, %3, %145, %13, %160, %13, %160, %160, %145, %160, %0, %0, %3, %145, %13, %13, %13, %160, %160, %3, %13, %13, %13, %145, %13, %145, %160, %145, %160, %0, %13, %145, %3, %145, %13, %3, %0, %13, %13, %160, %0, %145, %3, %3, %0, %13, %13, %160, %3, %0, %3, %3, %0, %145, %3, %145, %0, %13, %145, %160, %13, %0, %160, %3, %145, %13, %3, %145, %145, %160, %160, %3, %160, %160, %145, %160, %0, %160, %3, %13, %160, %145, %145, %3, %160, %145, %160, %160, %0, %0, %3, %145, %0, %0, %3, %160, %145, %3, %145, %160, %160, %3, %0, %13, %13, %145, %145, %160, %13, %13, %13, %3, %3, %13, %0, %145, %160, %160, %145, %145, %160, %160, %3, %13, %13, %3, %145, %160, %0, %13, %3, %145, %0, %3, %145, %160, %13, %0, %3, %13, %160, %160, %160, %0, %145, %160, %160, %13, %160, %145, %160, %13, %13, %160, %145, %13, %3, %3, %145, %3, %13, %0, %160, %0, %0, %0, %3, %160, %145, %0, %0, %13, %3, %0, %0, %3, %3, %145, %145, %145, %145, %13, %160, %160, %160, %0, %3, %13, %145, %0, %160, %13, %160, %0, %13, %0, %0, %0, %3, %145, %0, %0, %145, %13, %0, %0, %145, %0, %3, %0, %160, %13, %145, %160, %160, %0, %13, %13, %13, %0, %0, %160, %160, %145, %3, %3, %160, %3, %0, %145, %13, %145, %3, %160, %0, %145, %13, %0, %160, %160, %0, %0, %145, %13, %13, %13, %160, %3, %0, %145, %13, %0, %0, %13, %0, %145, %145, %160, %13, %160, %145, %3, %13, %145, %160, %160, %145, %0, %145, %3, %0, %13, %3, %160, %145, %0, %145, %3, %3, %145, %3, %145, %13, %3, %0, %13, %0, %160, %13, %13, %13, %0, %13, %13, %13, %13, %0, %3, %145, %145, %145, %0, %145, %13, %145, %13, %145, %0, %3, %0, %160, %3, %13, %13, %145) : (i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16) -> tensor<17x18x6xi16>
    %162 = "math.copysign"(%88, %138) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %163 = "spirv.CL.round"(%11) : (f32) -> f32
    %164 = "spirv.INotEqual"(%0, %145) : (i16, i16) -> i1
    %165 = "bufferization.to_memref"(%56) : (tensor<6xi64>) -> memref<6xi64>
    "memref.assume_alignment"(%73) <{alignment = 4 : i32}> : (memref<6x22x17xf16>) -> ()
    %166 = "math.round"(%48) <{fastmath = #arith.fastmath<none>}> : (tensor<?xf16>) -> tensor<?xf16>
    %167 = "spirv.BitFieldUExtract"(%133, %15, %160) : (vector<2xi32>, i32, i16) -> vector<2xi32>
    %168 = "memref.alloc"(%42, %108, %43) <{operandSegmentSizes = array<i32: 3, 0>}> : (index, index, index) -> memref<?x?x?xi16>
    "linalg.transpose"(%61, %168) <{permutation = array<i64: 2, 0, 1>}> ({
    ^bb0(%arg2: i16, %arg3: i16):
      "linalg.yield"(%arg2) : (i16) -> ()
    }) : (tensor<?x?x?xi16>, memref<?x?x?xi16>) -> ()
    %169 = "arith.ceildivsi"(%129, %127) : (i1, i1) -> i1
    %170 = "vector.broadcast"(%83) : (i1) -> vector<22xi1>
    "vector.compressstore"(%64, %16, %16, %17, %170, %159) : (memref<?x?x6xf16>, index, index, index, vector<22xi1>, vector<22xf16>) -> ()
    %171 = "bufferization.to_tensor"(%65) : (memref<6x22x17xf32>) -> tensor<6x22x17xf32>
    %172 = "spirv.GL.Log"(%82) : (f32) -> f32
    %173 = "bufferization.to_memref"(%60) : (tensor<?xi64>) -> memref<?xi64>
    %174 = "vector.transpose"(%133) <{transp = [0]}> : (vector<2xi32>) -> vector<2xi32>
    %175 = "spirv.BitReverse"(%121) : (i32) -> i32
    %176 = "tensor.splat"(%172) : (f32) -> tensor<18x6x6xf32>
    %177 = "spirv.CL.s_max"(%149, %149) : (i32, i32) -> i32
    %178 = "index.bool.constant"() <{value = false}> : () -> i1
    %179 = "vector.load"(%78, %16, %16, %16) : (memref<?x?x?xi1>, index, index, index) -> vector<18x6x6xi1>
    %180 = "arith.constant"() <{value = true}> : () -> i1
    %181 = "math.log1p"(%124) <{fastmath = #arith.fastmath<none>}> : (tensor<6x22x17xf32>) -> tensor<6x22x17xf32>
    %182 = "spirv.FUnordGreaterThanEqual"(%172, %84) : (f32, f32) -> i1
    %183 = "arith.shrui"(%3, %0) : (i16, i16) -> i16
    %184 = "math.tan"(%87) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %185 = "index.shl"(%47, %41) : (index, index) -> index
    %186 = "spirv.IsInf"(%82) : (f32) -> i1
    %187 = "spirv.CL.cos"(%116) : (f16) -> f16
    %188 = "spirv.CL.u_min"(%121, %113) : (i32, i32) -> i32
    %189 = "spirv.BitwiseAnd"(%133, %133) : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %190 = "index.sizeof"() : () -> index
    %191 = "memref.load"(%72, %20) <{nontemporal = false}> : (memref<6xf16>, index) -> f16
    "memref.store"(%10, %92, %32, %18, %20) <{nontemporal = false}> : (i32, memref<18x6x6xi32>, index, index, index) -> ()
    %192 = "vector.transpose"(%140) <{transp = [1, 0]}> : (vector<22x17xf16>) -> vector<17x22xf16>
    %193 = "arith.negf"(%8) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %194 = "arith.ceildivsi"(%127, %154) : (i1, i1) -> i1
    %195 = "index.divu"(%102, %27) : (index, index) -> index
    %196 = "arith.xori"(%175, %128) : (i32, i32) -> i32
    %197 = "spirv.GL.UMax"(%14, %10) : (i32, i32) -> i32
    %198 = "spirv.ULessThan"(%177, %121) : (i32, i32) -> i1
    %199 = "arith.addf"(%116, %6) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
    %200 = "spirv.GL.SAbs"(%13) : (i16) -> i16
    %201 = "spirv.LogicalNotEqual"(%154, %7) : (i1, i1) -> i1
    %202 = "tensor.empty"() : () -> tensor<6x6xf16>
    %203 = "tensor.empty"() : () -> tensor<6x17xf16>
    %204 = "tensor.empty"() : () -> tensor<6x17xf16>
    %205 = "linalg.matmul"(%202, %203, %204) <{operandSegmentSizes = array<i32: 2, 1>}> ({
    ^bb0(%arg2: f16, %arg3: f16, %arg4: f16):
      %210 = "arith.mulf"(%arg2, %arg3) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
      %211 = "arith.addf"(%arg4, %210) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
      "linalg.yield"(%211) : (f16) -> ()
    }) {linalg.memoized_indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>]} : (tensor<6x6xf16>, tensor<6x17xf16>, tensor<6x17xf16>) -> tensor<6x17xf16>
    %206 = "spirv.CL.cos"(%1) : (f16) -> f16
    %207 = "arith.minsi"(%182, %111) : (i1, i1) -> i1
    %208 = "spirv.GL.FClamp"(%138, %142, %12) : (f32, f32, f32) -> f32
    %209 = "spirv.ULessThanEqual"(%133, %133) : (vector<2xi32>, vector<2xi32>) -> vector<2xi1>
    "vector.print"(%104) <{punctuation = #vector.punctuation<newline>}> : (vector<6x22x17xf16>) -> ()
    "vector.print"(%118) <{punctuation = #vector.punctuation<newline>}> : (vector<6xf16>) -> ()
    "vector.print"(%133) <{punctuation = #vector.punctuation<newline>}> : (vector<2xi32>) -> ()
    "vector.print"(%140) <{punctuation = #vector.punctuation<newline>}> : (vector<22x17xf16>) -> ()
    "vector.print"(%155) <{punctuation = #vector.punctuation<newline>}> : (vector<1xf16>) -> ()
    "vector.print"(%159) <{punctuation = #vector.punctuation<newline>}> : (vector<22xf16>) -> ()
    "vector.print"(%179) <{punctuation = #vector.punctuation<newline>}> : (vector<18x6x6xi1>) -> ()
    "vector.print"(%0) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%1) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%2) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%3) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%4) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%5) <{punctuation = #vector.punctuation<newline>}> : (i64) -> ()
    "vector.print"(%6) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%7) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%8) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%9) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%10) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%11) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%12) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%13) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%14) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%15) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%81) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%82) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%83) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%84) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%86) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%87) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%88) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%93) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%95) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%97) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%101) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%106) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%110) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%111) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%113) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%115) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%116) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%121) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%125) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%127) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%128) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%129) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%131) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%136) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%137) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%138) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%142) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%144) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%145) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%146) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%149) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%151) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%152) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%154) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%160) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%163) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%164) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%172) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%175) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%177) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%178) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%182) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%186) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%187) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%188) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%197) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%198) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%200) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%201) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%206) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%208) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "func.return"(%32) : (index) -> ()
  }) : () -> ()
  "func.func"() <{function_type = (vector<17x18x6xf16>, i16) -> (), sym_name = "func2", sym_visibility = "nested"}> ({
  ^bb0(%arg0: vector<17x18x6xf16>, %arg1: i16):
    %0 = "arith.constant"() <{value = -4827 : i16}> : () -> i16
    %1 = "arith.constant"() <{value = 6.284000e+03 : f16}> : () -> f16
    %2 = "arith.constant"() <{value = 0x4E166186 : f32}> : () -> f32
    %3 = "arith.constant"() <{value = -25820 : i16}> : () -> i16
    %4 = "arith.constant"() <{value = true}> : () -> i1
    %5 = "arith.constant"() <{value = 1955529960 : i64}> : () -> i64
    %6 = "arith.constant"() <{value = 2.328000e+03 : f16}> : () -> f16
    %7 = "arith.constant"() <{value = true}> : () -> i1
    %8 = "arith.constant"() <{value = 0x4D688185 : f32}> : () -> f32
    %9 = "arith.constant"() <{value = true}> : () -> i1
    %10 = "arith.constant"() <{value = 2007147171 : i32}> : () -> i32
    %11 = "arith.constant"() <{value = 1.0746775E+9 : f32}> : () -> f32
    %12 = "arith.constant"() <{value = 0x4E0672E3 : f32}> : () -> f32
    %13 = "arith.constant"() <{value = -23368 : i16}> : () -> i16
    %14 = "arith.constant"() <{value = 231394892 : i32}> : () -> i32
    %15 = "arith.constant"() <{value = 1270722384 : i32}> : () -> i32
    %16 = "arith.constant"() <{value = 0 : index}> : () -> index
    %17 = "arith.constant"() <{value = 1 : index}> : () -> index
    %18 = "arith.constant"() <{value = 2 : index}> : () -> index
    %19 = "arith.constant"() <{value = 3 : index}> : () -> index
    %20 = "arith.constant"() <{value = 4 : index}> : () -> index
    %21 = "arith.constant"() <{value = 5 : index}> : () -> index
    %22 = "arith.constant"() <{value = 6 : index}> : () -> index
    %23 = "arith.constant"() <{value = 7 : index}> : () -> index
    %24 = "arith.constant"() <{value = 8 : index}> : () -> index
    %25 = "arith.constant"() <{value = 9 : index}> : () -> index
    %26 = "arith.constant"() <{value = 10 : index}> : () -> index
    %27 = "arith.constant"() <{value = 11 : index}> : () -> index
    %28 = "arith.constant"() <{value = 12 : index}> : () -> index
    %29 = "arith.constant"() <{value = 13 : index}> : () -> index
    %30 = "arith.constant"() <{value = 14 : index}> : () -> index
    %31 = "arith.constant"() <{value = 15 : index}> : () -> index
    %32 = "arith.constant"() <{value = 16 : index}> : () -> index
    %33 = "arith.constant"() <{value = 17 : index}> : () -> index
    %34 = "arith.constant"() <{value = 18 : index}> : () -> index
    %35 = "arith.constant"() <{value = 19 : index}> : () -> index
    %36 = "arith.constant"() <{value = 20 : index}> : () -> index
    %37 = "arith.constant"() <{value = 21 : index}> : () -> index
    %38 = "arith.constant"() <{value = 22 : index}> : () -> index
    %39 = "arith.constant"() <{value = 23 : index}> : () -> index
    %40 = "arith.constant"() <{value = 24 : index}> : () -> index
    %41 = "arith.constant"() <{value = 25 : index}> : () -> index
    %42 = "arith.constant"() <{value = 26 : index}> : () -> index
    %43 = "arith.constant"() <{value = 27 : index}> : () -> index
    %44 = "arith.constant"() <{value = 28 : index}> : () -> index
    %45 = "arith.constant"() <{value = 29 : index}> : () -> index
    %46 = "arith.constant"() <{value = 30 : index}> : () -> index
    %47 = "arith.constant"() <{value = 31 : index}> : () -> index
    %48 = "tensor.empty"(%18) : (index) -> tensor<?xf16>
    %49 = "tensor.empty"(%31) : (index) -> tensor<?xi16>
    %50 = "tensor.empty"(%43, %16, %35) : (index, index, index) -> tensor<?x?x?xi16>
    %51 = "tensor.empty"() : () -> tensor<6xi32>
    %52 = "tensor.empty"(%35) : (index) -> tensor<?x18x6xi16>
    %53 = "tensor.empty"(%21) : (index) -> tensor<?xi32>
    %54 = "tensor.empty"() : () -> tensor<6xi16>
    %55 = "tensor.empty"(%45, %32, %34) : (index, index, index) -> tensor<?x?x?xi64>
    %56 = "tensor.empty"() : () -> tensor<6xi64>
    %57 = "tensor.empty"(%45, %38) : (index, index) -> tensor<?x?x17xi32>
    %58 = "tensor.empty"() : () -> tensor<6x22x17xf16>
    %59 = "tensor.empty"(%26, %25, %25) : (index, index, index) -> tensor<?x?x?xi16>
    %60 = "tensor.empty"(%34) : (index) -> tensor<?xi64>
    %61 = "tensor.empty"(%42, %40, %24) : (index, index, index) -> tensor<?x?x?xi16>
    %62 = "tensor.empty"() : () -> tensor<18x6x6xi1>
    %63 = "tensor.empty"() : () -> tensor<17x18x6xf32>
    %64 = "memref.alloc"(%31, %34) <{operandSegmentSizes = array<i32: 2, 0>}> : (index, index) -> memref<?x?x6xf16>
    %65 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<6x22x17xf32>
    %66 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<6x22x17xi16>
    %67 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<18x6x6xf16>
    %68 = "memref.alloc"(%42) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xf32>
    %69 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<6x22x17xi64>
    %70 = "memref.alloc"(%30, %35, %37) <{operandSegmentSizes = array<i32: 3, 0>}> : (index, index, index) -> memref<?x?x?xf32>
    %71 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<17x18x6xi32>
    %72 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<6xf16>
    %73 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<6x22x17xf16>
    %74 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<6xi64>
    %75 = "memref.alloc"(%37) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?x18x6xi1>
    %76 = "memref.alloc"(%44) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xf16>
    %77 = "memref.alloc"(%35, %28, %16) <{operandSegmentSizes = array<i32: 3, 0>}> : (index, index, index) -> memref<?x?x?xi32>
    %78 = "memref.alloc"(%32, %31, %22) <{operandSegmentSizes = array<i32: 3, 0>}> : (index, index, index) -> memref<?x?x?xi1>
    %79 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<18x6x6xi32>
    %80 = "bufferization.to_tensor"(%74) : (memref<6xi64>) -> tensor<6xi64>
    %81 = "spirv.GL.Tan"(%12) : (f32) -> f32
    %82 = "bufferization.to_tensor"(%73) : (memref<6x22x17xf16>) -> tensor<6x22x17xf16>
    %83 = "vector.create_mask"(%26, %44, %28) : (index, index, index) -> vector<17x18x6xi1>
    %84 = "arith.minsi"(%15, %15) : (i32, i32) -> i32
    %85 = "spirv.GL.UMax"(%14, %14) : (i32, i32) -> i32
    %86 = "spirv.GL.FMin"(%8, %8) : (f32, f32) -> f32
    %87 = "spirv.UGreaterThanEqual"(%13, %13) : (i16, i16) -> i1
    %88 = "index.shru"(%17, %16) : (index, index) -> index
    %89 = "spirv.GL.Cosh"(%6) : (f16) -> f16
    %90 = "arith.mulf"(%12, %86) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %91 = "spirv.BitReverse"(%85) : (i32) -> i32
    %92 = "math.fma"(%63, %63, %63) <{fastmath = #arith.fastmath<none>}> : (tensor<17x18x6xf32>, tensor<17x18x6xf32>, tensor<17x18x6xf32>) -> tensor<17x18x6xf32>
    %93 = "spirv.FOrdLessThanEqual"(%12, %8) : (f32, f32) -> i1
    %94 = "arith.ceildivsi"(%85, %15) : (i32, i32) -> i32
    %95 = "spirv.CL.rsqrt"(%8) : (f32) -> f32
    %96 = "spirv.FOrdGreaterThan"(%11, %12) : (f32, f32) -> i1
    %97 = "index.floordivs"(%31, %24) : (index, index) -> index
    %98 = "index.divu"(%43, %97) : (index, index) -> index
    %99 = "math.exp2"(%48) <{fastmath = #arith.fastmath<none>}> : (tensor<?xf16>) -> tensor<?xf16>
    %100 = "affine.min"(%88, %37, %25, %34) <{map = affine_map<(d0, d1, d2, d3) -> (d0 floordiv 4)>}> : (index, index, index, index) -> index
    %101 = "spirv.CL.pow"(%12, %81) : (f32, f32) -> f32
    %102 = "spirv.GL.Asin"(%89) : (f16) -> f16
    %103 = "spirv.IsNan"(%1) : (f16) -> i1
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (22, 6, 22)>}> ({
    ^bb0(%arg2: index, %arg3: index, %arg4: index):
      %206 = "arith.addf"(%81, %2) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      "affine.yield"() : () -> ()
    }) : () -> ()
    %104 = "spirv.GL.UMax"(%13, %arg1) : (i16, i16) -> i16
    %105 = "arith.shli"(%103, %7) : (i1, i1) -> i1
    "bufferization.dealloc_tensor"(%82) : (tensor<6x22x17xf16>) -> ()
    %106 = "index.castu"(%29) : (index) -> i32
    %107 = "spirv.GL.FMax"(%102, %1) : (f16, f16) -> f16
    %108 = "spirv.GL.Tanh"(%81) : (f32) -> f32
    %109 = "spirv.ULessThanEqual"(%85, %10) : (i32, i32) -> i1
    %110 = "vector.extract_strided_slice"(%83) <{offsets = [10, 4], sizes = [6, 14], strides = [1, 1]}> : (vector<17x18x6xi1>) -> vector<6x14x6xi1>
    %111 = "bufferization.to_tensor"(%79) : (memref<18x6x6xi32>) -> tensor<18x6x6xi32>
    %112 = "math.sqrt"(%101) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %113 = "arith.minui"(%96, %7) : (i1, i1) -> i1
    %114 = "tensor.extract"(%58, %19, %17, %32) : (tensor<6x22x17xf16>, index, index, index) -> f16
    %115 = "math.exp"(%102) <{fastmath = #arith.fastmath<none>}> : (f16) -> f16
    %116 = "arith.cmpi"(%104, %3) <{predicate = 8 : i64}> : (i16, i16) -> i1
    "memref.store"(%91, %79, %17, %20, %19) <{nontemporal = false}> : (i32, memref<18x6x6xi32>, index, index, index) -> ()
    %117 = "index.castu"(%100) : (index) -> i32
    %118 = "spirv.CL.sin"(%89) : (f16) -> f16
    %119 = "spirv.GL.FSign"(%102) : (f16) -> f16
    "affine.store"(%93, %78, %25, %46, %18) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> : (i1, memref<?x?x?xi1>, index, index, index) -> ()
    %120 = "arith.shrui"(%93, %103) : (i1, i1) -> i1
    %121 = "tensor.from_elements"(%12, %11, %95, %81, %8, %95) : (f32, f32, f32, f32, f32, f32) -> tensor<6xf32>
    %122 = "spirv.LogicalEqual"(%7, %109) : (i1, i1) -> i1
    %123 = "arith.ceildivsi"(%9, %93) : (i1, i1) -> i1
    "memref.store"(%12, %70, %16, %16, %16) <{nontemporal = false}> : (f32, memref<?x?x?xf32>, index, index, index) -> ()
    %124 = "spirv.FUnordLessThanEqual"(%12, %101) : (f32, f32) -> i1
    %125 = "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [8], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (18, 6, 22)>}> ({
    ^bb0(%arg2: index, %arg3: index, %arg4: index):
      %206 = "vector.broadcast"(%86) : (f32) -> vector<6xf32>
      %207 = "vector.fma"(%206, %206, %206) : (vector<6xf32>, vector<6xf32>, vector<6xf32>) -> vector<6xf32>
      "affine.yield"(%10) : (i32) -> ()
    }) : () -> memref<18x6x22xi32>
    %126 = "spirv.GL.Floor"(%8) : (f32) -> f32
    %127 = "spirv.IsNan"(%108) : (f32) -> i1
    %128 = "math.tanh"(%126) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %129 = "spirv.UGreaterThanEqual"(%15, %15) : (i32, i32) -> i1
    %130 = "spirv.CL.cos"(%11) : (f32) -> f32
    %131 = "spirv.GL.RoundEven"(%86) : (f32) -> f32
    %132 = "spirv.CL.sin"(%6) : (f16) -> f16
    %133 = "math.fpowi"(%118, %91) <{fastmath = #arith.fastmath<none>}> : (f16, i32) -> f16
    %134 = "spirv.CL.round"(%131) : (f32) -> f32
    %135 = "math.fma"(%82, %58, %58) <{fastmath = #arith.fastmath<none>}> : (tensor<6x22x17xf16>, tensor<6x22x17xf16>, tensor<6x22x17xf16>) -> tensor<6x22x17xf16>
    %136 = "math.tan"(%101) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %137 = "math.absi"(%3) : (i16) -> i16
    %138 = "vector.broadcast"(%127) : (i1) -> vector<14x6xi1>
    %139 = "vector.insert"(%138, %110) <{static_position = array<i64: 2>}> : (vector<14x6xi1>, vector<6x14x6xi1>) -> vector<6x14x6xi1>
    %140 = "arith.divsi"(%87, %93) : (i1, i1) -> i1
    %141 = "vector.broadcast"(%103) : (i1) -> vector<17xi1>
    "affine.vector_store"(%141, %78, %100, %46, %37) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> : (vector<17xi1>, memref<?x?x?xi1>, index, index, index) -> ()
    %142 = "spirv.GL.Sin"(%12) : (f32) -> f32
    %143 = "spirv.FNegate"(%81) : (f32) -> f32
    %144 = "spirv.GL.FMax"(%114, %6) : (f16, f16) -> f16
    %145 = "math.cttz"(%124) : (i1) -> i1
    %146 = "vector.broadcast"(%14) : (i32) -> vector<2xi32>
    %147 = "spirv.BitFieldInsert"(%146, %146, %14, %13) : (vector<2xi32>, vector<2xi32>, i32, i16) -> vector<2xi32>
    %148 = "spirv.FUnordEqual"(%2, %95) : (f32, f32) -> i1
    %149 = "arith.shli"(%93, %103) : (i1, i1) -> i1
    %150 = "spirv.GL.FMax"(%11, %8) : (f32, f32) -> f32
    %151 = "spirv.GL.FSign"(%131) : (f32) -> f32
    %152 = "math.fpowi"(%131, %14) <{fastmath = #arith.fastmath<none>}> : (f32, i32) -> f32
    %153 = "spirv.LogicalNot"(%87) : (i1) -> i1
    %154 = "arith.divui"(%104, %arg1) : (i16, i16) -> i16
    %155 = "spirv.CL.sqrt"(%107) : (f16) -> f16
    %156 = "tensor.extract"(%82, %17, %25, %31) : (tensor<6x22x17xf16>, index, index, index) -> f16
    %157 = "bufferization.to_memref"(%53) : (tensor<?xi32>) -> memref<?xi32>
    %158 = "spirv.CL.s_min"(%91, %85) : (i32, i32) -> i32
    %159 = "index.castu"(%97) : (index) -> i32
    %160 = "spirv.CL.sin"(%8) : (f32) -> f32
    %161 = "spirv.GL.Sinh"(%8) : (f32) -> f32
    %162 = "vector.extract"(%83) <{static_position = array<i64: 6>}> : (vector<17x18x6xi1>) -> vector<18x6xi1>
    %163 = "vector.broadcast"(%96) : (i1) -> vector<17x17xi1>
    %164 = "vector.outerproduct"(%141, %141, %163) <{kind = #vector.kind<maxui>}> : (vector<17xi1>, vector<17xi1>, vector<17x17xi1>) -> vector<17x17xi1>
    %165 = "spirv.SLessThanEqual"(%15, %15) : (i32, i32) -> i1
    %166 = "spirv.GL.Atan"(%102) : (f16) -> f16
    %167 = "tensor.splat"(%2) : (f32) -> tensor<6x22x17xf32>
    %168 = "spirv.FOrdGreaterThanEqual"(%142, %150) : (f32, f32) -> i1
    %169 = "spirv.CL.erf"(%151) : (f32) -> f32
    %170 = "index.divu"(%45, %38) : (index, index) -> index
    %171 = "spirv.CL.u_max"(%13, %13) : (i16, i16) -> i16
    %172 = "math.expm1"(%58) <{fastmath = #arith.fastmath<none>}> : (tensor<6x22x17xf16>) -> tensor<6x22x17xf16>
    %173 = "arith.ceildivsi"(%14, %85) : (i32, i32) -> i32
    %174 = "arith.negf"(%143) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %175 = "spirv.BitwiseOr"(%146, %146) : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %176 = "index.maxs"(%21, %36) : (index, index) -> index
    %177 = "spirv.BitFieldUExtract"(%146, %14, %10) : (vector<2xi32>, i32, i32) -> vector<2xi32>
    %178 = "tensor.splat"(%169) : (f32) -> tensor<18x6x6xf32>
    %179 = "spirv.CL.rsqrt"(%161) : (f32) -> f32
    %180 = "spirv.CL.fma"(%101, %150, %169) : (f32, f32, f32) -> f32
    %181 = "tensor.empty"() : () -> tensor<6xi32>
    %182 = "linalg.map"(%51, %51, %181) ({
    ^bb0(%arg2: i32, %arg3: i32):
      %206 = "affine.if"(%42, %31, %31, %35) ({
        %239 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<6xf32>
        %240 = "vector.broadcast"(%126) : (f32) -> vector<18xf32>
        %241 = "vector.broadcast"(%109) : (i1) -> vector<18xi1>
        %242 = "vector.maskedload"(%65, %19, %22, %32, %241, %240) : (memref<6x22x17xf32>, index, index, index, vector<18xi1>, vector<18xf32>) -> vector<18xf32>
        %243 = "arith.constant"() <{value = false}> : () -> i1
        %244 = "arith.remf"(%155, %107) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
        %245 = "math.copysign"(%160, %108) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %246 = "arith.negf"(%11) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %247 = "index.maxu"(%26, %46) : (index, index) -> index
        %248 = "index.and"(%39, %33) : (index, index) -> index
        "affine.yield"(%65) : (memref<6x22x17xf32>) -> ()
      }, {
        %239 = "index.divu"(%18, %27) : (index, index) -> index
        %240 = "tensor.extract"(%178, %20, %21, %19) : (tensor<18x6x6xf32>, index, index, index) -> f32
        %241 = "math.tanh"(%1) <{fastmath = #arith.fastmath<none>}> : (f16) -> f16
        %242 = "index.and"(%29, %25) : (index, index) -> index
        %243 = "index.add"(%36, %47) : (index, index) -> index
        %244 = "memref.cast"(%68) : (memref<?xf32>) -> memref<?xf32>
        %245 = "arith.negf"(%151) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        "affine.store"(%5, %69, %30, %17, %21) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> : (i64, memref<6x22x17xi64>, index, index, index) -> ()
        "affine.yield"(%65) : (memref<6x22x17xf32>) -> ()
      }) {condition = affine_set<(d0, d1, d2, d3) : ((-(d0 mod 8) - d0) ceildiv 4 >= 0, -d0 == 0)>} : (index, index, index, index) -> memref<6x22x17xf32>
      %207 = "affine.min"(%19, %97, %97, %20) <{map = affine_map<(d0, d1, d2)[s0] -> (d0 * 1024 + d0 floordiv 8 - 2)>}> : (index, index, index, index) -> index
      %208 = "arith.subi"(%153, %122) : (i1, i1) -> i1
      %209 = "index.and"(%37, %35) : (index, index) -> index
      %210 = "arith.remf"(%1, %166) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
      %211 = "vector.create_mask"(%36, %44, %47) : (index, index, index) -> vector<18x6x6xi1>
      %212 = "vector.broadcast"(%101) : (f32) -> vector<6xf32>
      %213 = "vector.fma"(%212, %212, %212) : (vector<6xf32>, vector<6xf32>, vector<6xf32>) -> vector<6xf32>
      %214 = "arith.addf"(%6, %102) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
      %215 = "arith.remui"(%14, %91) : (i32, i32) -> i32
      %216 = "vector.broadcast"(%153) : (i1) -> vector<6xi1>
      %217 = "vector.insert"(%216, %138) <{static_position = array<i64: 7>}> : (vector<6xi1>, vector<14x6xi1>) -> vector<14x6xi1>
      %218 = "math.absi"(%0) : (i16) -> i16
      %219 = "math.floor"(%8) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      %220 = "arith.cmpf"(%119, %1) <{predicate = 9 : i64}> : (f16, f16) -> i1
      %221 = "tensor.insert"(%158, %53, %16) : (i32, tensor<?xi32>, index) -> tensor<?xi32>
      %222 = "vector.flat_transpose"(%213) <{columns = 2 : i32, rows = 3 : i32}> : (vector<6xf32>) -> vector<6xf32>
      %223 = "tensor.cast"(%52) : (tensor<?x18x6xi16>) -> tensor<6x18x6xi16>
      %224 = "math.powf"(%155, %107) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
      %225 = "index.shrs"(%18, %45) : (index, index) -> index
      %226 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<18x6x6xi64>
      %227 = "math.ctlz"(%96) : (i1) -> i1
      %228 = "index.sub"(%32, %31) : (index, index) -> index
      %229 = "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [7], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (18, 18, 17)>}> ({
      ^bb0(%arg4: index, %arg5: index, %arg6: index):
        %239 = "index.divu"(%42, %24) : (index, index) -> index
        "affine.yield"(%13) : (i16) -> ()
      }) : () -> memref<18x18x17xi16>
      %230 = "affine.apply"(%209, %40, %30) <{map = affine_map<(d0, d1, d2) -> (d2 * -2)>}> : (index, index, index) -> index
      %231 = "index.maxu"(%23, %25) : (index, index) -> index
      %232 = "index.castu"(%47) : (index) -> i32
      %233 = "memref.cast"(%157) : (memref<?xi32>) -> memref<?xi32>
      %234 = "tensor.expand_shape"(%62) <{reassociation = [[0], [1], [2, 3]]}> : (tensor<18x6x6xi1>) -> tensor<18x6x6x1xi1>
      %235 = "vector.maskedload"(%65, %16, %16, %21, %216, %212) : (memref<6x22x17xf32>, index, index, index, vector<6xi1>, vector<6xf32>) -> vector<6xf32>
      %236 = "vector.matrix_multiply"(%235, %213) <{lhs_columns = 6 : i32, lhs_rows = 1 : i32, rhs_columns = 1 : i32}> : (vector<6xf32>, vector<6xf32>) -> vector<1xf32>
      %237 = "tensor.insert"(%3, %54, %20) : (i16, tensor<6xi16>, index) -> tensor<6xi16>
      "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (10)>}> ({
      ^bb0(%arg4: index):
        "affine.yield"() : () -> ()
      }) : () -> ()
      %238 = "arith.shli"(%arg1, %104) : (i16, i16) -> i16
      "linalg.yield"(%158) : (i32) -> ()
    }) : (tensor<6xi32>, tensor<6xi32>, tensor<6xi32>) -> tensor<6xi32>
    %183 = "arith.constant"() <{value = false}> : () -> i1
    %184 = "spirv.FOrdLessThanEqual"(%107, %166) : (f16, f16) -> i1
    %185 = "spirv.GL.SMin"(%14, %14) : (i32, i32) -> i32
    %186 = "spirv.CL.ceil"(%101) : (f32) -> f32
    %187 = "spirv.GL.Sqrt"(%114) : (f16) -> f16
    "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (104)>}> ({
    ^bb0(%arg2: index):
      "affine.yield"() : () -> ()
    }) : () -> ()
    %188 = "index.shru"(%27, %36) : (index, index) -> index
    %189 = "math.cos"(%82) <{fastmath = #arith.fastmath<none>}> : (tensor<6x22x17xf16>) -> tensor<6x22x17xf16>
    %190 = "spirv.GL.Log"(%180) : (f32) -> f32
    %191 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<6x22x17xi32>
    %192 = "vector.multi_reduction"(%138, %7) <{kind = #vector.kind<maxui>, reduction_dims = [0, 1]}> : (vector<14x6xi1>, i1) -> i1
    %193 = "scf.while"(%63) ({
    ^bb0(%arg2: tensor<17x18x6xf32>):
      %206 = "vector.broadcast"(%160) : (f32) -> vector<17xf32>
      "vector.compressstore"(%68, %16, %141, %206) : (memref<?xf32>, index, vector<17xi1>, vector<17xf32>) -> ()
      %207 = "math.round"(%101) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      %208 = "tensor.insert"(%13, %59, %16, %16, %16) : (i16, tensor<?x?x?xi16>, index, index, index) -> tensor<?x?x?xi16>
      %209 = "index.bool.constant"() <{value = false}> : () -> i1
      "scf.if"(%153) ({
        %213 = "linalg.transpose"(%50, %59) <{permutation = array<i64: 2, 0, 1>}> ({
        ^bb0(%arg3: i16, %arg4: i16):
          "linalg.yield"(%arg3) : (i16) -> ()
        }) : (tensor<?x?x?xi16>, tensor<?x?x?xi16>) -> tensor<?x?x?xi16>
        %214 = "index.or"(%43, %21) : (index, index) -> index
        %215 = "index.shrs"(%176, %188) : (index, index) -> index
        %216 = "vector.extract"(%141) <{static_position = array<i64: 13>}> : (vector<17xi1>) -> i1
        %217 = "index.shru"(%100, %21) : (index, index) -> index
        %218 = "memref.alloca"(%23) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?x18x6xi64>
        %219 = "math.tanh"(%190) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        "memref.copy"(%67, %67) : (memref<18x6x6xf16>, memref<18x6x6xf16>) -> ()
        "scf.yield"() : () -> ()
      }, {
      }) : (i1) -> ()
      %210 = "index.floordivs"(%37, %17) : (index, index) -> index
      %211 = "arith.ceildivsi"(%96, %109) : (i1, i1) -> i1
      %212 = "tensor.splat"(%107) : (f16) -> tensor<18x6x6xf16>
      "scf.condition"(%122, %arg2) : (i1, tensor<17x18x6xf32>) -> ()
    }, {
    ^bb0(%arg2: tensor<17x18x6xf32>):
      %206 = "math.floor"(%187) <{fastmath = #arith.fastmath<none>}> : (f16) -> f16
      %207 = "vector.broadcast"(%96) : (i1) -> vector<6x14xi1>
      %208 = "vector.mask"(%110) ({
        %223 = "vector.multi_reduction"(%110, %207) <{kind = #vector.kind<minsi>, reduction_dims = [2]}> : (vector<6x14x6xi1>, vector<6x14xi1>) -> vector<6x14xi1>
        "vector.yield"(%223) : (vector<6x14xi1>) -> ()
      }) : (vector<6x14x6xi1>) -> vector<6x14xi1>
      %209 = "bufferization.to_tensor"(%71) : (memref<17x18x6xi32>) -> tensor<17x18x6xi32>
      %210 = "math.copysign"(%167, %167) <{fastmath = #arith.fastmath<none>}> : (tensor<6x22x17xf32>, tensor<6x22x17xf32>) -> tensor<6x22x17xf32>
      %211 = "math.log"(%160) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      %212 = "arith.minsi"(%127, %153) : (i1, i1) -> i1
      %213 = "arith.ceildivsi"(%96, %184) : (i1, i1) -> i1
      %214 = "vector.broadcast"(%129) : (i1) -> vector<18xi1>
      %215 = "vector.multi_reduction"(%162, %214) <{kind = #vector.kind<minsi>, reduction_dims = [1]}> : (vector<18x6xi1>, vector<18xi1>) -> vector<18xi1>
      %216 = "arith.floordivsi"(%148, %184) : (i1, i1) -> i1
      %217 = "scf.index_switch"(%25) <{cases = array<i64: 1>}> ({
        %223 = "memref.cast"(%69) : (memref<6x22x17xi64>) -> memref<6x?x17xi64>
        %224 = "index.shrs"(%17, %36) : (index, index) -> index
        %225 = "tensor.empty"() : () -> tensor<2x2xf32>
        %226 = "tensor.empty"() : () -> tensor<4xf32>
        %227 = "tensor.unpack"(%225, %226, %18) <{inner_dims_pos = array<i64: 0>, outer_dims_perm = array<i64: 0>, static_inner_tiles = array<i64: -9223372036854775808>}> : (tensor<2x2xf32>, tensor<4xf32>, index) -> tensor<4xf32>
        %228 = "vector.extract"(%207) <{static_position = array<i64: 0>}> : (vector<6x14xi1>) -> vector<14xi1>
        "memref.assume_alignment"(%79) <{alignment = 16 : i32}> : (memref<18x6x6xi32>) -> ()
        %229 = "memref.alloc"(%45, %25, %19) <{operandSegmentSizes = array<i32: 3, 0>}> : (index, index, index) -> memref<?x?x?xi64>
        "linalg.transpose"(%55, %229) <{permutation = array<i64: 2, 0, 1>}> ({
        ^bb0(%arg3: i64, %arg4: i64):
          "linalg.yield"(%arg3) : (i64) -> ()
        }) : (tensor<?x?x?xi64>, memref<?x?x?xi64>) -> ()
        %230 = "tensor.splat"(%185) : (i32) -> tensor<18x6x6xi32>
        %231 = "index.ceildivu"(%98, %45) : (index, index) -> index
        %232 = "vector.load"(%70, %16, %16, %16) : (memref<?x?x?xf32>, index, index, index) -> vector<6x22x17xf32>
        %233 = "arith.minsi"(%129, %148) : (i1, i1) -> i1
        %234 = "index.add"(%29, %47) : (index, index) -> index
        %235 = "affine.apply"(%18, %224, %231) <{map = affine_map<(d0, d1)[s0] -> (((d1 floordiv 32 - d0) ceildiv 128) mod 8)>}> : (index, index, index) -> index
        %236 = "index.sub"(%22, %18) : (index, index) -> index
        %237 = "index.sizeof"() : () -> index
        %238 = "arith.xori"(%5, %5) : (i64, i64) -> i64
        %239 = "affine.vector_load"(%75, %18, %22, %34) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> : (memref<?x18x6xi1>, index, index, index) -> vector<18xi1>
        %240 = "tensor.empty"(%237) : (index) -> tensor<?x22x17xi16>
        "scf.yield"(%240) : (tensor<?x22x17xi16>) -> ()
      }, {
        %223 = "index.divu"(%18, %97) : (index, index) -> index
        %224 = "index.and"(%31, %41) : (index, index) -> index
        %225 = "vector.broadcast"(%81) : (f32) -> vector<6xf32>
        %226 = "vector.broadcast"(%103) : (i1) -> vector<6xi1>
        %227 = "vector.maskedload"(%65, %18, %29, %23, %226, %225) : (memref<6x22x17xf32>, index, index, index, vector<6xi1>, vector<6xf32>) -> vector<6xf32>
        %228 = "index.shrs"(%176, %223) : (index, index) -> index
        %229 = "vector.broadcast"(%81) : (f32) -> vector<6x6xf32>
        %230 = "vector.outerproduct"(%227, %227, %229) <{kind = #vector.kind<add>}> : (vector<6xf32>, vector<6xf32>, vector<6x6xf32>) -> vector<6x6xf32>
        %231 = "index.bool.constant"() <{value = false}> : () -> i1
        %232 = "arith.divf"(%190, %131) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %233 = "linalg.transpose"(%50, %61) <{permutation = array<i64: 2, 0, 1>}> ({
        ^bb0(%arg3: i16, %arg4: i16):
          "linalg.yield"(%arg3) : (i16) -> ()
        }) : (tensor<?x?x?xi16>, tensor<?x?x?xi16>) -> tensor<?x?x?xi16>
        %234 = "vector.matrix_multiply"(%226, %226) <{lhs_columns = 6 : i32, lhs_rows = 1 : i32, rhs_columns = 1 : i32}> : (vector<6xi1>, vector<6xi1>) -> vector<1xi1>
        %235 = "bufferization.to_tensor"(%66) : (memref<6x22x17xi16>) -> tensor<6x22x17xi16>
        %236 = "arith.remf"(%166, %118) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
        %237 = "tensor.empty"(%188, %224, %42) : (index, index, index) -> tensor<?x?x?x22xi64>
        %238 = "linalg.broadcast"(%55, %237) <{dimensions = array<i64: 3>}> ({
        ^bb0(%arg3: i64, %arg4: i64):
          "linalg.yield"(%arg3) : (i64) -> ()
        }) : (tensor<?x?x?xi64>, tensor<?x?x?x22xi64>) -> tensor<?x?x?x22xi64>
        %239 = "math.tan"(%118) <{fastmath = #arith.fastmath<none>}> : (f16) -> f16
        %240 = "index.floordivs"(%42, %22) : (index, index) -> index
        %241 = "arith.divui"(%129, %93) : (i1, i1) -> i1
        %242 = "vector.broadcast"(%33) : (index) -> vector<22xindex>
        %243 = "vector.broadcast"(%165) : (i1) -> vector<22xi1>
        %244 = "vector.broadcast"(%130) : (f32) -> vector<22xf32>
        "vector.scatter"(%70, %16, %16, %16, %242, %243, %244) : (memref<?x?x?xf32>, index, index, index, vector<22xindex>, vector<22xi1>, vector<22xf32>) -> ()
        %245 = "tensor.empty"(%240) : (index) -> tensor<?x22x17xi16>
        "scf.yield"(%245) : (tensor<?x22x17xi16>) -> ()
      }) : (index) -> tensor<?x22x17xi16>
      %218 = "arith.mulf"(%190, %160) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      %219 = "math.roundeven"(%134) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      %220 = "arith.addf"(%1, %155) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
      "memref.assume_alignment"(%65) <{alignment = 2 : i32}> : (memref<6x22x17xf32>) -> ()
      %221 = "vector.multi_reduction"(%146, %158) <{kind = #vector.kind<and>, reduction_dims = [0]}> : (vector<2xi32>, i32) -> i32
      %222 = "index.maxs"(%17, %170) : (index, index) -> index
      "scf.yield"(%arg2) : (tensor<17x18x6xf32>) -> ()
    }) : (tensor<17x18x6xf32>) -> tensor<17x18x6xf32>
    %194 = "spirv.ULessThanEqual"(%5, %5) : (i64, i64) -> i1
    %195 = "spirv.CL.round"(%180) : (f32) -> f32
    %196 = "math.sqrt"(%108) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %197 = "spirv.FOrdNotEqual"(%131, %12) : (f32, f32) -> i1
    %198 = "spirv.GL.SClamp"(%146, %146, %146) : (vector<2xi32>, vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %199 = "spirv.CL.erf"(%179) : (f32) -> f32
    %200 = "vector.insert"(%15, %146) <{static_position = array<i64: 1>}> : (i32, vector<2xi32>) -> vector<2xi32>
    %201 = "vector.extract"(%146) <{static_position = array<i64: 1>}> : (vector<2xi32>) -> i32
    %202 = "tensor.insert"(%119, %82, %17, %26, %26) : (f16, tensor<6x22x17xf16>, index, index, index) -> tensor<6x22x17xf16>
    %203 = "arith.divf"(%179, %11) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %204 = "index.castu"(%122) : (i1) -> index
    %205 = "spirv.FOrdNotEqual"(%142, %199) : (f32, f32) -> i1
    "vector.print"(%83) <{punctuation = #vector.punctuation<newline>}> : (vector<17x18x6xi1>) -> ()
    "vector.print"(%110) <{punctuation = #vector.punctuation<newline>}> : (vector<6x14x6xi1>) -> ()
    "vector.print"(%138) <{punctuation = #vector.punctuation<newline>}> : (vector<14x6xi1>) -> ()
    "vector.print"(%141) <{punctuation = #vector.punctuation<newline>}> : (vector<17xi1>) -> ()
    "vector.print"(%146) <{punctuation = #vector.punctuation<newline>}> : (vector<2xi32>) -> ()
    "vector.print"(%162) <{punctuation = #vector.punctuation<newline>}> : (vector<18x6xi1>) -> ()
    "vector.print"(%arg1) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%0) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%1) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%2) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%3) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%4) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%5) <{punctuation = #vector.punctuation<newline>}> : (i64) -> ()
    "vector.print"(%6) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%7) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%8) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%9) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%10) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%11) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%12) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%13) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%14) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%15) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%81) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%85) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%86) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%87) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%89) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%91) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%93) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%95) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%96) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%101) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%102) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%103) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%104) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%107) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%108) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%109) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%114) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%118) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%119) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%122) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%124) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%126) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%127) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%129) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%130) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%131) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%132) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%134) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%142) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%143) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%144) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%148) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%150) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%151) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%153) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%155) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%156) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%158) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%160) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%161) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%165) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%166) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%168) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%169) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%171) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%179) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%180) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%184) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%185) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%186) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%187) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%190) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%192) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%194) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%195) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%197) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%199) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%205) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "func.return"() : () -> ()
  }) : () -> ()
}) : () -> ()
