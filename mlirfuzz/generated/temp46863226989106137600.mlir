"builtin.module"() ({
  "func.func"() <{function_type = () -> (), sym_name = "func1", sym_visibility = "private"}> ({
    %0 = "arith.constant"() <{value = 0x4E0CD9CF : f32}> : () -> f32
    %1 = "arith.constant"() <{value = 1.09581914E+9 : f32}> : () -> f32
    %2 = "arith.constant"() <{value = false}> : () -> i1
    %3 = "arith.constant"() <{value = false}> : () -> i1
    %4 = "arith.constant"() <{value = 29053 : i16}> : () -> i16
    %5 = "arith.constant"() <{value = 635697735 : i32}> : () -> i32
    %6 = "arith.constant"() <{value = 0x4DD449FE : f32}> : () -> f32
    %7 = "arith.constant"() <{value = 1.92379226E+9 : f32}> : () -> f32
    %8 = "arith.constant"() <{value = -3272 : i16}> : () -> i16
    %9 = "arith.constant"() <{value = 1056808192 : i32}> : () -> i32
    %10 = "arith.constant"() <{value = 1616561925 : i32}> : () -> i32
    %11 = "arith.constant"() <{value = 1935052110 : i32}> : () -> i32
    %12 = "arith.constant"() <{value = false}> : () -> i1
    %13 = "arith.constant"() <{value = 806696163 : i32}> : () -> i32
    %14 = "arith.constant"() <{value = 4.668000e+03 : f16}> : () -> f16
    %15 = "arith.constant"() <{value = 1109543139 : i32}> : () -> i32
    %16 = "arith.constant"() <{value = 0 : index}> : () -> index
    %17 = "arith.constant"() <{value = 1 : index}> : () -> index
    %18 = "arith.constant"() <{value = 2 : index}> : () -> index
    %19 = "arith.constant"() <{value = 3 : index}> : () -> index
    %20 = "arith.constant"() <{value = 4 : index}> : () -> index
    %21 = "arith.constant"() <{value = 5 : index}> : () -> index
    %22 = "arith.constant"() <{value = 6 : index}> : () -> index
    %23 = "arith.constant"() <{value = 7 : index}> : () -> index
    %24 = "arith.constant"() <{value = 8 : index}> : () -> index
    %25 = "arith.constant"() <{value = 9 : index}> : () -> index
    %26 = "arith.constant"() <{value = 10 : index}> : () -> index
    %27 = "arith.constant"() <{value = 11 : index}> : () -> index
    %28 = "arith.constant"() <{value = 12 : index}> : () -> index
    %29 = "arith.constant"() <{value = 13 : index}> : () -> index
    %30 = "arith.constant"() <{value = 14 : index}> : () -> index
    %31 = "arith.constant"() <{value = 15 : index}> : () -> index
    %32 = "arith.constant"() <{value = 16 : index}> : () -> index
    %33 = "arith.constant"() <{value = 17 : index}> : () -> index
    %34 = "arith.constant"() <{value = 18 : index}> : () -> index
    %35 = "arith.constant"() <{value = 19 : index}> : () -> index
    %36 = "arith.constant"() <{value = 20 : index}> : () -> index
    %37 = "arith.constant"() <{value = 21 : index}> : () -> index
    %38 = "arith.constant"() <{value = 22 : index}> : () -> index
    %39 = "arith.constant"() <{value = 23 : index}> : () -> index
    %40 = "arith.constant"() <{value = 24 : index}> : () -> index
    %41 = "arith.constant"() <{value = 25 : index}> : () -> index
    %42 = "arith.constant"() <{value = 26 : index}> : () -> index
    %43 = "arith.constant"() <{value = 27 : index}> : () -> index
    %44 = "arith.constant"() <{value = 28 : index}> : () -> index
    %45 = "arith.constant"() <{value = 29 : index}> : () -> index
    %46 = "arith.constant"() <{value = 30 : index}> : () -> index
    %47 = "arith.constant"() <{value = 31 : index}> : () -> index
    %48 = "tensor.empty"(%22) : (index) -> tensor<?x28x11xf16>
    %49 = "tensor.empty"(%37) : (index) -> tensor<?xi32>
    %50 = "tensor.empty"() : () -> tensor<32xf32>
    %51 = "tensor.empty"() : () -> tensor<32xi64>
    %52 = "tensor.empty"(%40) : (index) -> tensor<?xi16>
    %53 = "tensor.empty"(%43) : (index) -> tensor<?x28x11xi1>
    %54 = "tensor.empty"(%24) : (index) -> tensor<?x28x11xi64>
    %55 = "tensor.empty"() : () -> tensor<11xf32>
    %56 = "tensor.empty"() : () -> tensor<11x28x11xi64>
    %57 = "tensor.empty"(%26) : (index) -> tensor<?xi1>
    %58 = "tensor.empty"() : () -> tensor<28xf32>
    %59 = "tensor.empty"() : () -> tensor<11xf32>
    %60 = "tensor.empty"() : () -> tensor<11xi16>
    %61 = "tensor.empty"(%16) : (index) -> tensor<?xf16>
    %62 = "tensor.empty"() : () -> tensor<28xi64>
    %63 = "tensor.empty"() : () -> tensor<11xi16>
    %64 = "memref.alloc"(%21) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?x28x11xi64>
    %65 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<28xf16>
    %66 = "memref.alloc"(%40) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xf32>
    %67 = "memref.alloc"(%37) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xf32>
    %68 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11x28x11xi1>
    %69 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11xf16>
    %70 = "memref.alloc"(%32) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?x28x11xf32>
    %71 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<28xf32>
    %72 = "memref.alloc"(%39) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xi32>
    %73 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11x28x11xi32>
    %74 = "memref.alloc"(%45) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xi1>
    %75 = "memref.alloc"(%28) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xf16>
    %76 = "memref.alloc"(%20, %17, %20) <{operandSegmentSizes = array<i32: 3, 0>}> : (index, index, index) -> memref<?x?x?xi32>
    %77 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11x28x11xi16>
    %78 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11x28x11xi32>
    %79 = "memref.alloc"(%26) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xi16>
    %80 = "spirv.CL.fabs"(%1) : (f32) -> f32
    "vector.warp_execute_on_lane_0"(%16) <{warp_size = 32 : i64}> ({
      %220 = "index.sub"(%25, %33) : (index, index) -> index
      %221 = "index.or"(%28, %220) : (index, index) -> index
      %222 = "vector.broadcast"(%15) : (i32) -> vector<32xi32>
      %223 = "vector.transpose"(%222) <{transp = [0]}> : (vector<32xi32>) -> vector<32xi32>
      %224 = "affine.if"(%42, %26, %36) ({
        %229 = "vector.broadcast"(%14) : (f16) -> vector<12x12xf16>
        %230 = "vector.broadcast"(%14) : (f16) -> vector<12xf16>
        %231:2 = "vector.scan"(%229, %230) <{inclusive = false, kind = #vector.kind<maxf>, reduction_dim = 1 : i64}> : (vector<12x12xf16>, vector<12xf16>) -> (vector<12x12xf16>, vector<12xf16>)
        %232 = "index.ceildivs"(%46, %45) : (index, index) -> index
        %233 = "arith.cmpf"(%0, %0) <{predicate = 7 : i64}> : (f32, f32) -> i1
        %234 = "math.atan2"(%6, %7) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %235 = "arith.remf"(%7, %0) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %236 = "memref.alloca"(%46) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xi16>
        %237 = "math.exp2"(%50) <{fastmath = #arith.fastmath<none>}> : (tensor<32xf32>) -> tensor<32xf32>
        %238 = "arith.divui"(%2, %2) : (i1, i1) -> i1
        %239 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11xf32>
        "affine.yield"(%239) : (memref<11xf32>) -> ()
      }, {
        %229 = "tensor.empty"() : () -> tensor<12x28xi16>
        %230 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<28x11xi16>
        %231 = "tensor.empty"() : () -> tensor<12x11xi16>
        %232 = "linalg.matmul"(%229, %230, %231) <{operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg0: i16, %arg1: i16, %arg2: i16):
          %243 = "arith.muli"(%arg0, %arg1) : (i16, i16) -> i16
          %244 = "arith.addi"(%arg2, %243) : (i16, i16) -> i16
          "linalg.yield"(%244) : (i16) -> ()
        }) {linalg.memoized_indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>]} : (tensor<12x28xi16>, memref<28x11xi16>, tensor<12x11xi16>) -> tensor<12x11xi16>
        %233 = "arith.subi"(%9, %15) : (i32, i32) -> i32
        %234 = "vector.broadcast"(%0) : (f32) -> vector<32x28xf32>
        %235 = "vector.broadcast"(%1) : (f32) -> vector<32xf32>
        %236:2 = "vector.scan"(%234, %235) <{inclusive = false, kind = #vector.kind<maxf>, reduction_dim = 1 : i64}> : (vector<32x28xf32>, vector<32xf32>) -> (vector<32x28xf32>, vector<32xf32>)
        %237 = "arith.remsi"(%5, %15) : (i32, i32) -> i32
        %238 = "arith.remsi"(%11, %5) : (i32, i32) -> i32
        %239 = "vector.create_mask"(%17) : (index) -> vector<32xi1>
        %240 = "math.fpowi"(%80, %11) <{fastmath = #arith.fastmath<none>}> : (f32, i32) -> f32
        %241 = "arith.ori"(%5, %9) : (i32, i32) -> i32
        %242 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11xf32>
        "affine.yield"(%242) : (memref<11xf32>) -> ()
      }) {condition = affine_set<(d0, d1, d2) : (0 == 0)>} : (index, index, index) -> memref<11xf32>
      %225 = "vector.transfer_read"(%48, %26, %16, %27, %14) <{operandSegmentSizes = array<i32: 1, 3, 1, 0>, permutation_map = affine_map<(d0, d1, d2) -> ()>}> : (tensor<?x28x11xf16>, index, index, index, f16) -> vector<f16>
      %226 = "tensor.cast"(%56) : (tensor<11x28x11xi64>) -> tensor<?x?x?xi64>
      %227 = "tensor.splat"(%2) : (i1) -> tensor<11x28x11xi1>
      %228 = "index.or"(%34, %36) : (index, index) -> index
      "vector.yield"() : () -> ()
    }) : (index) -> ()
    %81 = "math.atan"(%55) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>) -> tensor<11xf32>
    %82 = "arith.constant"() <{value = 0 : i16}> : () -> i16
    %83 = "vector.transfer_read"(%63, %41, %82) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, permutation_map = affine_map<(d0) -> ()>}> : (tensor<11xi16>, index, i16) -> vector<i16>
    %84 = "spirv.CL.exp"(%7) : (f32) -> f32
    "memref.alloca_scope"() ({
      %220 = "vector.broadcast"(%8) : (i16) -> vector<12xi16>
      %221 = "vector.matrix_multiply"(%220, %220) <{lhs_columns = 12 : i32, lhs_rows = 1 : i32, rhs_columns = 1 : i32}> : (vector<12xi16>, vector<12xi16>) -> vector<1xi16>
      %222 = "arith.addf"(%7, %0) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      %223 = "vector.insert"(%4, %220) <{static_position = array<i64: 3>}> : (i16, vector<12xi16>) -> vector<12xi16>
      %224 = "math.rsqrt"(%7) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      %225 = "math.fma"(%1, %80, %1) <{fastmath = #arith.fastmath<none>}> : (f32, f32, f32) -> f32
      %226 = "index.divs"(%36, %30) : (index, index) -> index
      %227 = "math.copysign"(%14, %14) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
      %228 = "tensor.empty"() : () -> tensor<28x28xi64>
      %229 = "linalg.broadcast"(%62, %228) <{dimensions = array<i64: 1>}> ({
      ^bb0(%arg0: i64, %arg1: i64):
        "linalg.yield"(%arg0) : (i64) -> ()
      }) : (tensor<28xi64>, tensor<28x28xi64>) -> tensor<28x28xi64>
      "vector.print"(%220) <{punctuation = #vector.punctuation<newline>}> : (vector<12xi16>) -> ()
      %230 = "scf.index_switch"(%34) <{cases = array<i64: 1>}> ({
        %254 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<28xi64>
        %255 = "arith.addf"(%80, %84) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %256 = "tensor.cast"(%63) : (tensor<11xi16>) -> tensor<?xi16>
        %257 = "arith.cmpf"(%0, %0) <{predicate = 9 : i64}> : (f32, f32) -> i1
        %258 = "vector.splat"(%19) : (index) -> vector<32xindex>
        %259 = "math.roundeven"(%48) <{fastmath = #arith.fastmath<none>}> : (tensor<?x28x11xf16>) -> tensor<?x28x11xf16>
        %260 = "tensor.extract"(%61, %16) : (tensor<?xf16>, index) -> f16
        %261 = "arith.minsi"(%9, %9) : (i32, i32) -> i32
        %262 = "vector.extract"(%220) <{static_position = array<i64: 4>}> : (vector<12xi16>) -> i16
        %263 = "memref.atomic_rmw"(%10, %78, %17, %38, %22) <{kind = 7 : i64}> : (i32, memref<11x28x11xi32>, index, index, index) -> i32
        %264 = "math.ctlz"(%8) : (i16) -> i16
        %265 = "index.shl"(%37, %33) : (index, index) -> index
        %266 = "math.absi"(%53) : (tensor<?x28x11xi1>) -> tensor<?x28x11xi1>
        %267 = "math.round"(%58) <{fastmath = #arith.fastmath<none>}> : (tensor<28xf32>) -> tensor<28xf32>
        %268 = "arith.remui"(%2, %2) : (i1, i1) -> i1
        %269 = "arith.muli"(%9, %11) : (i32, i32) -> i32
        %270 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11xi16>
        "scf.yield"(%270) : (memref<11xi16>) -> ()
      }, {
        %254 = "math.atan2"(%58, %58) <{fastmath = #arith.fastmath<none>}> : (tensor<28xf32>, tensor<28xf32>) -> tensor<28xf32>
        %255 = "math.powf"(%58, %58) <{fastmath = #arith.fastmath<none>}> : (tensor<28xf32>, tensor<28xf32>) -> tensor<28xf32>
        %256 = "math.powf"(%0, %80) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        "memref.copy"(%65, %65) : (memref<28xf16>, memref<28xf16>) -> ()
        %257 = "math.ceil"(%50) <{fastmath = #arith.fastmath<none>}> : (tensor<32xf32>) -> tensor<32xf32>
        "linalg.transpose"(%65, %65) <{permutation = array<i64: 0>}> ({
        ^bb0(%arg0: f16, %arg1: f16):
          "linalg.yield"(%arg0) : (f16) -> ()
        }) : (memref<28xf16>, memref<28xf16>) -> ()
        %258 = "tensor.extract"(%57, %16) : (tensor<?xi1>, index) -> i1
        %259 = "math.round"(%55) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>) -> tensor<11xf32>
        %260 = "tensor.empty"() : () -> tensor<28xi32>
        %261 = "math.fpowi"(%58, %260) <{fastmath = #arith.fastmath<none>}> : (tensor<28xf32>, tensor<28xi32>) -> tensor<28xf32>
        %262 = "vector.shuffle"(%221, %220) <{mask = [0, 1, 3, 6, 7, 8, 10]}> : (vector<1xi16>, vector<12xi16>) -> vector<7xi16>
        %263 = "math.tan"(%58) <{fastmath = #arith.fastmath<none>}> : (tensor<28xf32>) -> tensor<28xf32>
        %264 = "math.round"(%59) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>) -> tensor<11xf32>
        %265 = "index.casts"(%11) : (i32) -> index
        %266 = "math.ctpop"(%62) : (tensor<28xi64>) -> tensor<28xi64>
        %267 = "math.ceil"(%59) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>) -> tensor<11xf32>
        %268 = "index.sub"(%26, %44) : (index, index) -> index
        %269 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11xi16>
        "scf.yield"(%269) : (memref<11xi16>) -> ()
      }) : (index) -> memref<11xi16>
      %231 = "math.round"(%50) <{fastmath = #arith.fastmath<none>}> : (tensor<32xf32>) -> tensor<32xf32>
      %232 = "affine.max"(%16, %28, %19, %32) <{map = affine_map<(d0, d1, d2, d3) -> (-d1 - (d1 ceildiv 16 + 2))>}> : (index, index, index, index) -> index
      %233 = "math.ceil"(%61) <{fastmath = #arith.fastmath<none>}> : (tensor<?xf16>) -> tensor<?xf16>
      %234 = "arith.addi"(%13, %10) : (i32, i32) -> i32
      %235 = "arith.addi"(%8, %8) : (i16, i16) -> i16
      %236 = "scf.while"(%55) ({
      ^bb0(%arg0: tensor<11xf32>):
        %254 = "index.and"(%226, %25) : (index, index) -> index
        "vector.print"(%220) <{punctuation = #vector.punctuation<newline>}> : (vector<12xi16>) -> ()
        %255 = "index.divs"(%42, %25) : (index, index) -> index
        %256 = "memref.alloc"(%23) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?x28xi1>
        "linalg.broadcast"(%57, %256) <{dimensions = array<i64: 1>}> ({
        ^bb0(%arg1: i1, %arg2: i1):
          "linalg.yield"(%arg1) : (i1) -> ()
        }) : (tensor<?xi1>, memref<?x28xi1>) -> ()
        %257 = "memref.alloca"(%16) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xi1>
        %258 = "affine.max"(%46, %31, %40, %24) <{map = affine_map<(d0, d1, d2)[s0] -> (d0 * 2)>}> : (index, index, index, index) -> index
        %259 = "math.ctlz"(%49) : (tensor<?xi32>) -> tensor<?xi32>
        %260 = "math.atan2"(%0, %80) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        "scf.condition"(%2, %55) : (i1, tensor<11xf32>) -> ()
      }, {
      ^bb0(%arg0: tensor<11xf32>):
        %254 = "math.tan"(%61) <{fastmath = #arith.fastmath<none>}> : (tensor<?xf16>) -> tensor<?xf16>
        %255 = "memref.load"(%79, %16) <{nontemporal = false}> : (memref<?xi16>, index) -> i16
        %256 = "affine.max"(%43) <{map = affine_map<(d0) -> (d0 + 8)>}> : (index) -> index
        %257 = "tensor.from_elements"(%11, %13, %5, %13, %10, %9, %5, %9, %5, %13, %10) : (i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> tensor<11xi32>
        %258 = "memref.load"(%70, %16, %35, %16) <{nontemporal = false}> : (memref<?x28x11xf32>, index, index, index) -> f32
        %259 = "math.ctlz"(%4) : (i16) -> i16
        %260 = "vector.insertelement"(%8, %220, %34) : (i16, vector<12xi16>, index) -> vector<12xi16>
        %261 = "math.round"(%58) <{fastmath = #arith.fastmath<none>}> : (tensor<28xf32>) -> tensor<28xf32>
        %262 = "vector.reduction"(%220) <{fastmath = #arith.fastmath<none>, kind = #vector.kind<maxui>}> : (vector<12xi16>) -> i16
        %263 = "vector.broadcast"(%2) : (i1) -> vector<12xi1>
        %264 = "vector.mask"(%263) ({
          %271 = "vector.multi_reduction"(%220, %220) <{kind = #vector.kind<maxsi>, reduction_dims = []}> : (vector<12xi16>, vector<12xi16>) -> vector<12xi16>
          "vector.yield"(%271) : (vector<12xi16>) -> ()
        }) : (vector<12xi1>) -> vector<12xi16>
        %265 = "affine.load"(%71, %32) <{map = affine_map<(d0) -> (d0)>}> : (memref<28xf32>, index) -> f32
        %266 = "tensor.from_elements"(%2, %12, %3, %2, %12, %12, %3, %12, %3, %2, %2, %3, %3, %3, %3, %2, %3, %3, %12, %2, %3, %3, %3, %2, %3, %3, %2, %12, %2, %3, %2, %3, %3, %3, %2, %3, %3, %2, %3, %12, %12, %3, %2, %3, %2, %12, %3, %12, %3, %2, %2, %12, %3, %12, %3, %12, %12, %2, %3, %3, %2, %2, %2, %3, %3, %3, %2, %3, %3, %3, %12, %12, %3, %2, %3, %12, %12, %12, %3, %3, %12, %12, %2, %12, %12, %2, %3, %12, %12, %2, %12, %12, %12, %12, %12, %3, %12, %12, %2, %2, %3, %2, %3, %2, %12, %2, %3, %12, %3, %2, %2, %2, %12, %12, %2, %2, %2, %3, %3, %12, %2, %12, %12, %3, %2, %3, %2, %12, %3, %12, %12, %12, %3, %3, %2, %12, %3, %2, %12, %2, %3, %2, %3, %12, %2, %2, %12, %12, %12, %12, %2, %12, %3, %3, %3, %3, %12, %12, %3, %12, %12, %12, %3, %2, %12, %2, %3, %12, %12, %3, %12, %3, %12, %12, %2, %12, %12, %3, %12, %12, %3, %12, %3, %12, %2, %12, %12, %3, %3, %2, %3, %2, %2, %3, %2, %3, %2, %3, %2, %2, %12, %2, %2, %12, %2, %2, %2, %2, %2, %2, %12, %3, %12, %2, %2, %12, %12, %2, %12, %2, %12, %2, %3, %2, %2, %3, %2, %12, %12, %2, %3, %2, %3, %2, %3, %3, %12, %12, %2, %2, %12, %2, %2, %3, %12, %3, %2, %12, %2, %3, %2, %2, %12, %2, %2, %2, %12, %2, %3, %12, %3, %3, %12, %12, %2, %12, %3, %12, %2, %3, %2, %12, %3, %2, %3, %2, %2, %3, %2, %3, %3, %3, %2, %12, %2, %2, %12, %3, %12, %12, %3, %3, %2, %12, %3, %12, %2, %12, %12, %2, %3, %12, %2, %2, %12, %2, %12, %12, %3, %12, %2, %3, %2, %2, %2, %12, %3, %12, %12, %12, %2, %2, %2, %12, %2, %3, %3, %2, %3, %3, %12, %2, %3, %3, %3, %3, %12, %3, %2, %12, %3, %3, %12, %2, %3, %2, %2, %12, %12, %3, %3, %2, %3, %12, %12, %3, %12, %2, %12, %3, %3, %3, %3, %3, %2, %2, %2, %2, %12, %12, %3, %3, %3, %2, %2, %3, %3, %2, %12, %3, %3, %12, %12, %3, %2, %2, %2, %2, %2, %12, %2, %2, %2, %3, %12, %12, %3, %12, %2, %3, %12, %12, %12, %12, %2, %12, %3, %2, %12, %2, %3, %2, %12, %3, %2, %3, %3, %12, %12, %12, %12, %2, %2, %3, %3, %2, %12, %12, %12, %2, %3, %12, %12, %12, %3, %3, %3, %3, %3, %3, %12, %3, %12, %12, %3, %12, %12, %12, %12, %2, %3, %12, %12, %3, %2, %12, %2, %12, %2, %2, %12, %3, %12, %2, %2, %3, %12, %12, %3, %12, %2, %3, %12, %2, %3, %12, %2, %2, %3, %12, %3, %2, %2, %2, %12, %3, %3, %3, %3, %12, %12, %3, %3, %12, %12, %12, %2, %3, %3, %3, %3, %12, %3, %2, %2, %12, %3, %3, %12, %3, %2, %3, %2, %2, %2, %12, %3, %3, %12, %3, %12, %12, %3, %3, %12, %12, %2, %3, %3, %2, %12, %3, %12, %12, %3, %12, %2, %3, %3, %3, %3, %2, %2, %2, %2, %12, %3, %2, %12, %2, %12, %3, %12, %3, %12, %12, %2, %2, %3, %12, %2, %3, %12, %3, %3, %12, %12, %12, %2, %12, %12, %12, %12, %12, %3, %3, %12, %2, %3, %3, %2, %2, %3, %12, %12, %2, %12, %12, %12, %2, %2, %2, %3, %3, %2, %12, %3, %3, %3, %2, %2, %3, %12, %3, %3, %3, %12, %3, %12, %2, %2, %12, %12, %2, %12, %12, %12, %2, %2, %12, %3, %12, %3, %12, %2, %12, %2, %2, %2, %2, %3, %12, %2, %2, %2, %2, %12, %2, %12, %12, %2, %2, %2, %12, %3, %3, %2, %12, %2, %12, %3, %3, %2, %12, %2, %2, %3, %12, %3, %3, %3, %12, %2, %3, %3, %12, %3, %2, %3, %12, %2, %3, %12, %12, %12, %3, %3, %3, %3, %2, %3, %12, %12, %12, %2, %2, %2, %12, %3, %2, %2, %3, %2, %2, %2, %3, %3, %2, %3, %12, %2, %3, %2, %2, %3, %2, %12, %3, %12, %3, %12, %3, %12, %12, %2, %12, %3, %12, %12, %2, %2, %3, %2, %3, %2, %2, %2, %2, %3, %2, %2, %2, %12, %2, %12, %12, %3, %12, %12, %2, %12, %3, %2, %2, %12, %2, %12, %2, %12, %3, %12, %2, %12, %3, %3, %12, %3, %2, %12, %12, %3, %3, %12, %12, %12, %3, %2, %3, %12, %12, %12, %3, %3, %12, %2, %3, %3, %12, %2, %12, %2, %12, %3, %12, %12, %2, %2, %2, %2, %12, %12, %2, %3, %2, %2, %3, %2, %3, %2, %12, %2, %2, %12, %12, %2, %12, %12, %12, %3, %12, %2, %12, %12, %3, %2, %2, %2, %3, %12, %12, %2, %12, %3, %2, %3, %3, %2, %12, %12, %3, %12, %2, %3, %12, %2, %2, %2, %2, %2, %2, %3, %2, %2, %12, %12, %12, %2, %12, %12, %2, %12, %2, %3, %12, %2, %3, %2, %2, %12, %12, %12, %3, %3, %12, %2, %3, %2, %12, %2, %2, %3, %3, %3, %2, %2, %2, %3, %12, %12, %3, %3, %3, %2, %12, %2, %3, %3, %3, %12, %3, %3, %12, %2, %3, %3, %2, %12, %3, %3, %2, %3, %3, %2, %12, %2, %3, %2, %2, %12, %12, %3, %12, %2, %12, %2, %2, %12, %3, %2, %3, %3, %2, %2, %3, %2, %2, %12, %3, %3, %2, %12, %2, %3, %12, %12, %3, %2, %3, %3, %2, %12, %3, %3, %12, %2, %12, %12, %2, %3, %3, %2, %3, %3, %2, %3, %3, %12, %12, %12, %2, %3, %2, %3, %3, %2, %3, %2, %3, %3, %12, %12, %3, %2, %3, %12, %3, %3, %3, %2, %2, %2, %2, %3, %12, %2, %2, %3, %12, %12, %3, %2, %3, %12, %3, %3, %3, %3, %3, %3, %12, %12, %2, %2, %2, %12, %2, %12, %3, %3, %3, %3, %12, %2, %12, %3, %3, %12, %2, %3, %2, %12, %12, %2, %2, %2, %3, %12, %3, %12, %2, %2, %3, %3, %2, %2, %2, %3, %2, %3, %3, %2, %2, %12, %2, %2, %12, %2, %2, %2, %3, %3, %12, %12, %2, %12, %12, %3, %3, %12, %3, %12, %12, %12, %3, %2, %12, %12, %3, %2, %12, %2, %3, %2, %3, %2, %3, %2, %2, %12, %3, %3, %2, %3, %3, %3, %3, %2, %3, %2, %12, %12, %2, %2, %2, %3, %12, %2, %2, %12, %12, %12, %2, %12, %2, %12, %2, %2, %3, %12, %2, %3, %12, %12, %2, %12, %2, %12, %12, %12, %2, %12, %2, %2, %2, %3, %12, %3, %3, %3, %2, %2, %2, %12, %12, %12, %2, %3, %2, %3, %12, %3, %12, %3, %3, %3, %3, %12, %2, %3, %12, %3, %12, %12, %12, %2, %2, %3, %3, %3, %3, %2, %2, %3, %3, %3, %3, %2, %2, %12, %2, %2, %12, %3, %2, %12, %3, %12, %3, %2, %3, %12, %2, %2, %3, %12, %2, %12, %2, %3, %3, %3, %12, %3, %12, %12, %12, %12, %2, %2, %12, %2, %12, %3, %2, %12, %3, %12, %3, %12, %3, %12, %2, %12, %12, %2, %2, %12, %2, %12, %3, %12, %12, %12, %12, %3, %2, %2, %12, %2, %12, %2, %12, %3, %3, %3, %3, %12, %3, %12, %2, %12, %2, %12, %12, %3, %3, %3, %12, %3, %12, %3, %2, %2, %3, %2, %3, %3, %12, %2, %2, %3, %12, %3, %3, %12, %2, %2, %3, %12, %12, %2, %3, %2, %12, %3, %3, %12, %3, %3, %3, %3, %12, %12, %2, %2, %12, %2, %12, %3, %2, %3, %12, %12, %3, %12, %3, %2, %2, %3, %3, %12, %3, %12, %12, %12, %12, %2, %2, %12, %12, %3, %2, %3, %3, %2, %12, %12, %2, %12, %2, %12, %3, %12, %2, %3, %12, %3, %12, %3, %3, %2, %12, %2, %3, %2, %2, %2, %2, %12, %2, %12, %3, %2, %3, %12, %12, %3, %2, %3, %12, %2, %3, %2, %12, %2, %3, %3, %3, %2, %12, %2, %3, %12, %12, %12, %2, %12, %3, %3, %2, %12, %12, %12, %12, %2, %12, %2, %3, %2, %3, %3, %2, %12, %2, %2, %3, %12, %12, %3, %2, %2, %12, %3, %3, %2, %3, %3, %12, %3, %3, %12, %3, %2, %12, %12, %2, %3, %12, %3, %12, %12, %12, %2, %2, %2, %3, %2, %3, %12, %3, %12, %2, %2, %2, %3, %3, %12, %3, %3, %3, %12, %3, %12, %2, %12, %12, %3, %3, %12, %12, %12, %12, %2, %3, %12, %2, %2, %2, %2, %2, %2, %3, %2, %12, %3, %3, %12, %3, %12, %3, %12, %2, %2, %2, %3, %2, %3, %3, %2, %3, %3, %2, %2, %2, %3, %2, %2, %2, %2, %3, %12, %2, %2, %2, %12, %12, %3, %12, %2, %12, %3, %12, %3, %12, %3, %12, %3, %3, %12, %2, %2, %2, %3, %2, %12, %3, %12, %12, %12, %3, %3, %3, %3, %2, %12, %2, %3, %12, %12, %2, %12, %3, %12, %2, %12, %3, %3, %12, %2, %3, %12, %12, %2, %12, %2, %2, %3, %2, %12, %3, %3, %2, %12, %3, %3, %2, %3, %3, %12, %3, %3, %3, %2, %2, %12, %12, %12, %3, %2, %12, %3, %3, %3, %12, %12, %12, %3, %2, %12, %12, %3, %2, %12, %12, %12, %3, %3, %2, %12, %2, %3, %2, %12, %3, %2, %3, %12, %3, %3, %2, %3, %3, %2, %12, %2, %2, %12, %2, %3, %12, %12, %2, %12, %2, %3, %12, %3, %3, %2, %3, %12, %2, %12, %3, %12, %12, %2, %3, %2, %3, %12, %3, %12, %2, %3, %12, %12, %3, %3, %3, %3, %2, %12, %3, %2, %2, %2, %2, %2, %12, %12, %12, %3, %12, %2, %12, %2, %2, %2, %12, %12, %3, %2, %2, %3, %3, %3, %2, %12, %12, %12, %2, %12, %3, %2, %2, %3, %2, %12, %3, %2, %3, %12, %3, %2, %12, %12, %12, %3, %3, %2, %3, %12, %3, %2, %12, %3, %12, %3, %12, %3, %3, %2, %12, %3, %12, %2, %3, %12, %2, %12, %12, %2, %3, %12, %12, %2, %12, %12, %3, %2, %3, %3, %2, %2, %2, %12, %12, %12, %2, %12, %3, %12, %3, %2, %12, %12, %3, %3, %2, %2, %3, %3, %3, %2, %12, %3, %2, %12, %3, %3, %3, %3, %2, %12, %12, %12, %3, %2, %12, %3, %12, %12, %3, %3, %12, %2, %3, %3, %12, %12, %2, %12, %12, %3, %3, %3, %3, %2, %12, %2, %3, %12, %2, %2, %2, %2, %12, %12, %2, %2, %2, %3, %3, %2, %2, %2, %3, %12, %12, %3, %2, %3, %2, %2, %12, %12, %12, %3, %3, %2, %12, %12, %12, %2, %2, %12, %12, %3, %2, %3, %12, %12, %3, %3, %2, %12, %3, %3, %12, %3, %3, %2, %12, %2, %12, %2, %12, %3, %12, %3, %12, %3, %2, %2, %12, %2, %2, %3, %3, %3, %2, %12, %2, %12, %12, %3, %12, %2, %12, %2, %2, %3, %12, %12, %12, %12, %2, %12, %3, %2, %2, %3, %3, %3, %12, %3, %2, %2, %2, %12, %12, %2, %12, %2, %2, %3, %2, %12, %2, %3, %2, %12, %2, %2, %3, %2, %3, %2, %12, %2, %3, %12, %12, %3, %2, %12, %3, %2, %3, %2, %3, %2, %3, %12, %2, %2, %12, %3, %3, %3, %2, %3, %12, %2, %2, %12, %2, %2, %2, %3, %12, %2, %2, %12, %3, %12, %2, %2, %3, %12, %2, %12, %3, %3, %12, %3, %3, %12, %2, %3, %2, %2, %12, %3, %2, %2, %12, %3, %2, %2, %12, %3, %3, %3, %2, %3, %12, %2, %2, %3, %2, %3, %3, %3, %12, %2, %2, %12, %2, %12, %3, %2, %2, %2, %12, %12, %2, %2, %2, %12, %3, %12, %3, %12, %12, %2, %12, %3, %3, %3, %3, %3, %3, %2, %12, %12, %12, %3, %12, %12, %2, %3, %2, %12, %12, %12, %12, %12, %3, %2, %12, %2, %2, %2, %12, %2, %3, %12, %2, %2, %12, %3, %3, %2, %12, %2, %12, %3, %3, %2, %3, %2, %12, %12, %3, %3, %3, %12, %3, %2, %2, %12, %3, %2, %2, %3, %3, %3, %2, %2, %12, %2, %3, %12, %2, %2, %2, %3, %2, %12, %2, %12, %12, %12, %12, %3, %3, %12, %3, %12, %2, %2, %2, %12, %12, %3, %12, %3, %2, %3, %12, %2, %12, %2, %3, %12, %12, %2, %12, %3, %3, %12, %2, %12, %12, %12, %12, %3, %12, %12, %12, %2, %2, %12, %3, %2, %2, %12, %2, %3, %12, %2, %12, %3, %3, %2, %12, %3, %2, %12, %2, %3, %2, %12, %12, %2, %3, %12, %3, %2, %3, %2, %12, %12, %3, %12, %2, %2, %2, %2, %3, %2, %2, %3, %3, %2, %12, %3, %2, %3, %2, %3, %12, %2, %3, %12, %2, %3, %3, %2, %12, %3, %12, %12, %3, %2, %3, %12, %3, %2, %2, %2, %12, %3, %2, %12, %12, %12, %2, %3, %12, %3, %2, %12, %3, %2, %12, %12, %12, %12, %2, %2, %12, %12, %12, %2, %12, %2, %12, %2, %2, %2, %3, %3, %12, %2, %12, %2, %2, %2, %2, %12, %2, %2, %12, %12, %2, %2, %3, %3, %12, %2, %12, %3, %2, %12, %12, %3, %12, %3, %12, %12, %12, %12, %12, %2, %2, %12, %12, %12, %3, %12, %3, %2, %2, %2, %2, %2, %12, %3, %12, %3, %2, %3, %2, %3, %2, %2, %3, %2, %3, %2, %3, %3, %2, %3, %3, %12, %3, %2, %3, %2, %12, %3, %2, %2, %3, %2, %2, %12, %3, %12, %3, %2, %12, %2, %3, %2, %12, %3, %2, %2, %2, %2, %3, %12, %3, %3, %2, %2, %3, %2, %12, %2, %2, %3, %2, %3, %12, %2, %2, %12, %12, %3, %12, %3, %3, %3, %2, %12, %3, %2, %2, %3, %3, %2, %3, %3, %3, %12, %3, %12, %3, %3, %3, %2, %2, %12, %2, %2, %3, %3, %2, %3, %12, %3, %2, %2, %2, %2, %3, %12, %12, %3, %3, %12, %2, %12, %2, %3, %2, %2, %12, %3, %2, %3, %3, %3, %2, %2, %3, %3, %2, %3, %12, %2, %12, %12, %3, %3, %12, %12, %2, %2, %12, %2, %3, %3, %3, %2, %3, %2, %3, %2, %3, %2, %12, %3, %2, %12, %3, %2, %12, %3, %3, %2, %3, %12, %3, %3, %12, %12, %2, %2, %3, %2, %3, %12, %2, %2, %12, %3, %3, %2, %2, %3, %3, %2, %3, %2, %12, %2, %2, %12, %12, %2, %3, %12, %2, %12, %2, %3, %2, %2, %3, %12, %12, %2, %12, %12, %3, %2, %12, %12, %2, %2, %3, %12, %12, %3, %12, %2, %3, %3, %12, %2, %3, %12, %12, %2, %12, %3, %12, %3, %2, %12, %12, %2, %3, %3, %12, %3, %12, %3, %2, %2, %12, %2, %12, %12, %2, %3, %12, %3, %2, %3, %2, %3, %2, %2, %3, %12, %3, %2, %2, %12, %12, %12, %12, %3, %3, %3, %2, %2, %12, %3, %3, %2, %2, %3, %2, %3, %2, %12, %12, %12, %3, %12, %3, %2, %2, %12, %12, %12, %2, %12, %3, %2, %2, %2, %2, %3, %2, %2, %3, %3, %12, %12, %2, %12, %3, %12, %12, %12, %2, %2, %3, %3, %2, %3, %12, %3, %3, %2, %2, %12, %2, %2, %12, %3, %2, %12, %2, %3, %3, %2, %12, %12, %2, %12, %2, %3, %2, %3, %12, %2, %2, %3, %2, %3, %12, %12, %12, %2, %2, %3, %12, %3, %12, %2, %12, %3, %3, %3, %3, %2, %2, %12, %2, %2, %2, %2, %12, %2, %3, %2, %3, %12, %3, %2, %12, %2, %2, %12, %3, %3, %2, %2, %2, %3, %2, %3, %2, %2, %2, %12, %3, %3, %12, %12, %12, %2, %12, %12, %3, %3, %3, %3, %2, %2, %3, %12, %12, %12, %3, %12, %3, %2, %12, %12, %2, %3, %12, %3, %12, %2, %2, %12, %3, %12, %3, %2, %12, %2, %3, %3, %3, %12, %2, %12, %12, %2, %2, %2, %12, %12, %12, %2, %2, %3, %12, %2, %3, %12, %2, %3, %2, %12, %2, %12, %3, %12, %2, %2, %2, %2, %2, %2, %3, %3, %3, %3, %2, %2, %12, %12, %3, %3, %2, %3, %3, %12, %12, %12, %3, %2, %12, %3, %12, %2, %2, %3, %12, %12, %3, %3, %12, %12, %3, %3, %12, %2, %12, %12, %2, %12, %3, %3, %3, %2, %2, %3, %2, %12, %12, %3, %3, %12, %2, %2, %12, %2, %2, %3, %12, %12, %2, %3, %3, %2, %3, %2, %3, %3, %12, %12, %3, %12, %2, %3, %3, %12, %2, %12, %12, %3, %2, %12, %12, %12, %2, %2, %12, %12, %2, %12, %3, %3, %2, %12, %3, %12, %3, %3, %3, %3, %12, %3, %12, %3, %2, %12, %2, %3, %3, %12, %3, %3, %2, %2, %12, %12, %2, %2, %3, %12, %2, %3, %3, %2, %12, %12, %12, %3, %3, %3, %3, %3, %2, %3, %2, %3, %2, %2, %12, %12, %2, %3, %12, %2, %3, %3, %12, %2, %2, %12, %3, %12, %2, %12, %3, %2, %3, %2, %2, %3, %12, %12, %2, %12, %3, %2, %2, %2, %12, %12, %3, %3, %3, %2, %2, %2, %12, %3, %2, %3, %2, %12, %3, %12, %2, %3, %12, %2, %3, %2, %12, %12, %12, %3, %2, %12, %2, %3, %3, %12, %3, %3, %12, %2, %2, %12, %2, %2, %2, %12, %2, %3, %3, %12, %2, %2, %12, %12, %12, %12, %2, %3, %12, %12, %12, %2, %12, %12, %12, %12, %3, %12, %12, %2, %2, %2, %3, %3, %2, %2, %2, %12, %3, %3, %12, %12, %3, %12, %3, %2, %3, %2, %12, %2, %12, %3, %3, %12, %3, %3, %12, %2, %3, %2, %2, %12, %3, %3, %12, %2, %12, %3, %2, %12, %3, %2, %3, %12, %3, %3, %3, %2, %3, %2, %2, %12, %3, %3, %2, %3, %12, %12, %2, %12, %2, %2, %3, %12, %12, %2, %2, %3, %12, %3, %12, %12, %12, %3, %2, %12, %3, %3, %3, %12, %3, %3, %12, %12, %3, %3, %2, %3, %2, %12, %3, %12, %12, %3, %12, %12, %3, %3, %12, %12, %2, %2, %12, %12, %2, %12, %3, %12, %12, %12, %12, %3, %2, %3, %3, %12, %2, %3, %3, %3, %3, %2, %3, %3, %12, %2, %12, %12, %3, %3, %3, %2, %2, %12, %2, %12, %12, %3, %12, %3, %2, %2, %3, %3, %12, %3, %3, %3, %2, %12, %12, %12, %3, %2, %3, %12, %12, %2, %2, %3, %3, %12, %3, %2, %12, %3, %2, %3, %3, %12, %2, %12, %12, %2, %2, %12, %2, %12, %3, %3, %3, %3, %2, %2, %3, %2, %3, %2, %2, %2, %12, %12, %3, %3, %3, %3, %12, %3, %3, %2, %12, %12, %3, %3, %2, %3, %3, %3, %12, %2, %12, %3, %12, %12, %12, %12, %3, %12, %3, %2, %12, %3, %2, %3, %12, %3, %2, %3, %3, %12, %2, %12, %2, %2, %3, %3, %3, %3, %12, %12, %3, %12, %2, %12, %3, %12, %12, %3, %2, %2, %12, %2, %2, %12, %2, %2, %3, %12, %12, %3, %3, %12, %3, %2, %2, %3, %2, %12, %12, %12, %2, %2, %12, %2, %2, %12, %3, %12, %2, %3, %12, %2, %12, %2, %2, %12, %3, %12, %3, %3, %2, %3, %2, %3, %2, %3, %2, %2, %12, %2, %2, %3, %2, %2, %12, %2, %2, %3, %3, %3, %2, %12, %2, %3, %12, %2, %2, %2, %12, %12, %3, %2, %3, %2, %12, %3, %3, %2, %12, %2, %2, %2, %2, %3, %12, %3, %12, %12, %3, %3, %3, %3, %12, %12, %12, %2, %12, %3, %3, %2, %3, %3, %2, %2, %12, %12, %3, %12, %12, %3, %2, %2, %12, %3, %2, %3, %12, %12, %3, %2, %12, %12, %3, %2, %2, %2, %12, %2, %3, %12, %3, %2, %2, %12, %12, %12, %12, %12, %3, %2, %2, %12, %12, %12, %3, %2, %2, %12, %3, %2, %12, %3, %3, %12, %12, %12, %2, %2, %3, %12, %12, %2, %3, %12, %12, %2, %2, %2, %12, %12, %2, %12, %2, %2, %2, %12, %12, %2, %3, %12, %3, %3, %2, %12, %12, %12, %12, %2, %12, %2, %2, %3, %12, %2, %3, %2, %12) : (i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1, i1) -> tensor<11x28x11xi1>
        %267 = "index.sizeof"() : () -> index
        %268 = "tensor.empty"() : () -> tensor<11xi32>
        %269 = "math.fma"(%50, %50, %50) <{fastmath = #arith.fastmath<none>}> : (tensor<32xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<32xf32>
        %270 = "tensor.splat"(%6) : (f32) -> tensor<11x28x11xf32>
        "scf.yield"(%55) : (tensor<11xf32>) -> ()
      }) : (tensor<11xf32>) -> tensor<11xf32>
      %237 = "math.exp"(%55) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>) -> tensor<11xf32>
      %238 = "scf.parallel"(%30, %28, %34, %69) <{operandSegmentSizes = array<i32: 1, 1, 1, 1>}> ({
      ^bb0(%arg0: index):
        %254 = "tensor.rank"(%52) : (tensor<?xi16>) -> index
        %255 = "math.ceil"(%48) <{fastmath = #arith.fastmath<none>}> : (tensor<?x28x11xf16>) -> tensor<?x28x11xf16>
        %256 = "arith.cmpi"(%9, %9) <{predicate = 8 : i64}> : (i32, i32) -> i1
        %257 = "math.atan2"(%55, %55) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>, tensor<11xf32>) -> tensor<11xf32>
        %258 = "vector.extract_strided_slice"(%220) <{offsets = [3], sizes = [2], strides = [1]}> : (vector<12xi16>) -> vector<2xi16>
        %259 = "arith.andi"(%5, %9) : (i32, i32) -> i32
        %260 = "memref.alloca"(%226) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xf32>
        %261 = "linalg.copy"(%61, %61) <{operandSegmentSizes = array<i32: 1, 1>}> ({
        ^bb0(%arg1: f16, %arg2: f16):
          "linalg.yield"(%arg1) : (f16) -> ()
        }) : (tensor<?xf16>, tensor<?xf16>) -> tensor<?xf16>
        %262 = "arith.cmpi"(%11, %11) <{predicate = 9 : i64}> : (i32, i32) -> i1
        %263 = "index.maxu"(%30, %42) : (index, index) -> index
        %264 = "tensor.splat"(%80) : (f32) -> tensor<32xf32>
        %265 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11x11x28xi64>
        "linalg.transpose"(%56, %265) <{permutation = array<i64: 2, 0, 1>}> ({
        ^bb0(%arg1: i64, %arg2: i64):
          "linalg.yield"(%arg1) : (i64) -> ()
        }) : (tensor<11x28x11xi64>, memref<11x11x28xi64>) -> ()
        %266 = "index.bool.constant"() <{value = false}> : () -> i1
        %267 = "vector.shuffle"(%220, %221) <{mask = [0, 2, 4, 5, 7, 9]}> : (vector<12xi16>, vector<1xi16>) -> vector<6xi16>
        %268 = "vector.transpose"(%221) <{transp = [0]}> : (vector<1xi16>) -> vector<1xi16>
        %269 = "math.round"(%50) <{fastmath = #arith.fastmath<none>}> : (tensor<32xf32>) -> tensor<32xf32>
        "scf.reduce"(%69) ({
        ^bb0(%arg1: memref<11xf16>, %arg2: memref<11xf16>):
          %270 = "index.xor"(%263, %29) : (index, index) -> index
          %271 = "tensor.extract"(%58, %21) : (tensor<28xf32>, index) -> f32
          %272 = "memref.atomic_rmw"(%271, %70, %16, %40, %23) <{kind = 9 : i64}> : (f32, memref<?x28x11xf32>, index, index, index) -> f32
          %273 = "arith.shrui"(%266, %2) : (i1, i1) -> i1
          %274 = "math.atan2"(%84, %0) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          %275 = "memref.alloc"(%16, %47, %36) <{operandSegmentSizes = array<i32: 3, 0>}> : (index, index, index) -> memref<?x?x?xi16>
          %276 = "math.log10"(%84) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
          %277 = "arith.addi"(%266, %12) : (i1, i1) -> i1
          "scf.reduce.return"(%arg1) : (memref<11xf16>) -> ()
        }) : (memref<11xf16>) -> ()
        "scf.yield"() : () -> ()
      }) : (index, index, index, memref<11xf16>) -> memref<11xf16>
      %239 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xf32>
      %240 = "arith.cmpi"(%11, %9) <{predicate = 9 : i64}> : (i32, i32) -> i1
      %241 = "vector.splat"(%29) : (index) -> vector<11x28x11xindex>
      "vector.print"(%221) <{punctuation = #vector.punctuation<newline>}> : (vector<1xi16>) -> ()
      %242 = "bufferization.clone"(%65) : (memref<28xf16>) -> memref<28xf16>
      %243 = "memref.alloca_scope"() ({
        %254 = "vector.create_mask"(%17, %20, %25) : (index, index, index) -> vector<11x28x11xi1>
        %255 = "memref.cast"(%65) : (memref<28xf16>) -> memref<?xf16>
        %256 = "arith.cmpi"(%8, %8) <{predicate = 1 : i64}> : (i16, i16) -> i1
        %257 = "math.round"(%0) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %258 = "arith.cmpf"(%14, %14) <{predicate = 0 : i64}> : (f16, f16) -> i1
        %259 = "math.round"(%61) <{fastmath = #arith.fastmath<none>}> : (tensor<?xf16>) -> tensor<?xf16>
        %260 = "vector.broadcast"(%6) : (f32) -> vector<11xf32>
        %261 = "vector.fma"(%260, %260, %260) : (vector<11xf32>, vector<11xf32>, vector<11xf32>) -> vector<11xf32>
        %262 = "math.ceil"(%48) <{fastmath = #arith.fastmath<none>}> : (tensor<?x28x11xf16>) -> tensor<?x28x11xf16>
        %263 = "memref.alloc"(%22) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xi32>
        %264 = "arith.remsi"(%9, %10) : (i32, i32) -> i32
        %265 = "vector.broadcast"(%43) : (index) -> vector<11xindex>
        %266 = "vector.broadcast"(%3) : (i1) -> vector<11xi1>
        %267 = "vector.broadcast"(%14) : (f16) -> vector<11xf16>
        "vector.scatter"(%69, %26, %265, %266, %267) : (memref<11xf16>, index, vector<11xindex>, vector<11xi1>, vector<11xf16>) -> ()
        %268 = "bufferization.clone"(%73) : (memref<11x28x11xi32>) -> memref<11x28x11xi32>
        %269 = "arith.muli"(%12, %2) : (i1, i1) -> i1
        %270 = "arith.remsi"(%3, %12) : (i1, i1) -> i1
        %271 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xf16>
        %272 = "math.atan"(%14) <{fastmath = #arith.fastmath<none>}> : (f16) -> f16
        %273 = "arith.floordivsi"(%3, %3) : (i1, i1) -> i1
        %274 = "index.and"(%40, %39) : (index, index) -> index
        %275 = "math.atan2"(%58, %58) <{fastmath = #arith.fastmath<none>}> : (tensor<28xf32>, tensor<28xf32>) -> tensor<28xf32>
        %276 = "vector.splat"(%42) : (index) -> vector<32xindex>
        %277 = "math.powf"(%14, %14) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
        %278 = "linalg.copy"(%54, %54) <{operandSegmentSizes = array<i32: 1, 1>}> ({
        ^bb0(%arg0: i64, %arg1: i64):
          "linalg.yield"(%arg0) : (i64) -> ()
        }) : (tensor<?x28x11xi64>, tensor<?x28x11xi64>) -> tensor<?x28x11xi64>
        %279 = "bufferization.clone"(%78) : (memref<11x28x11xi32>) -> memref<11x28x11xi32>
        %280 = "math.atan2"(%55, %59) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>, tensor<11xf32>) -> tensor<11xf32>
        "memref.copy"(%72, %72) : (memref<?xi32>, memref<?xi32>) -> ()
        %281 = "memref.load"(%78, %23, %17, %18) <{nontemporal = false}> : (memref<11x28x11xi32>, index, index, index) -> i32
        %282 = "math.absi"(%10) : (i32) -> i32
        %283 = "math.sqrt"(%6) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %284 = "math.exp2"(%80) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %285 = "index.sizeof"() : () -> index
        %286 = "index.shru"(%37, %45) : (index, index) -> index
        %287 = "vector.flat_transpose"(%260) <{columns = 11 : i32, rows = 1 : i32}> : (vector<11xf32>) -> vector<11xf32>
        %288 = "tensor.empty"() : () -> tensor<11x28x11xi16>
        "memref.alloca_scope.return"(%288) : (tensor<11x28x11xi16>) -> ()
      }) : () -> tensor<11x28x11xi16>
      %244 = "math.cttz"(%62) : (tensor<28xi64>) -> tensor<28xi64>
      %245 = "math.fma"(%84, %84, %6) <{fastmath = #arith.fastmath<none>}> : (f32, f32, f32) -> f32
      %246 = "math.atan2"(%59, %55) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>, tensor<11xf32>) -> tensor<11xf32>
      %247 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %248 = "vector.broadcast"(%247) : (i64) -> vector<i64>
      %249 = "vector.transfer_write"(%248, %54, %38, %34, %24) <{operandSegmentSizes = array<i32: 1, 1, 3, 0>, permutation_map = affine_map<(d0, d1, d2) -> ()>}> : (vector<i64>, tensor<?x28x11xi64>, index, index, index) -> tensor<?x28x11xi64>
      %250 = "vector.shuffle"(%220, %220) <{mask = [0, 1, 2, 3, 6, 8, 11, 12, 15, 19, 20, 21, 22]}> : (vector<12xi16>, vector<12xi16>) -> vector<13xi16>
      %251 = "linalg.copy"(%49, %49) <{operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg0: i32, %arg1: i32):
        "linalg.yield"(%arg0) : (i32) -> ()
      }) : (tensor<?xi32>, tensor<?xi32>) -> tensor<?xi32>
      %252 = "math.roundeven"(%50) <{fastmath = #arith.fastmath<none>}> : (tensor<32xf32>) -> tensor<32xf32>
      %253 = "math.ipowi"(%62, %62) : (tensor<28xi64>, tensor<28xi64>) -> tensor<28xi64>
      "memref.alloca_scope.return"() : () -> ()
    }) : () -> ()
    %85 = "bufferization.to_memref"(%53) : (tensor<?x28x11xi1>) -> memref<?x28x11xi1>
    %86 = "memref.load"(%73, %18, %28, %20) <{nontemporal = false}> : (memref<11x28x11xi32>, index, index, index) -> i32
    %87 = "spirv.GL.UClamp"(%8, %8, %4) : (i16, i16, i16) -> i16
    %88 = "arith.remf"(%84, %6) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %89 = "spirv.CL.log"(%0) : (f32) -> f32
    %90 = "math.ipowi"(%5, %13) : (i32, i32) -> i32
    %91 = "spirv.SGreaterThanEqual"(%10, %10) : (i32, i32) -> i1
    %92 = "spirv.CL.cos"(%6) : (f32) -> f32
    %93 = "arith.cmpf"(%0, %1) <{predicate = 10 : i64}> : (f32, f32) -> i1
    %94 = "vector.broadcast"(%15) : (i32) -> vector<2xi32>
    %95 = "spirv.BitFieldInsert"(%94, %94, %9, %4) : (vector<2xi32>, vector<2xi32>, i32, i16) -> vector<2xi32>
    %96 = "spirv.CL.sin"(%1) : (f32) -> f32
    %97 = "tensor.cast"(%60) : (tensor<11xi16>) -> tensor<?xi16>
    %98 = "tensor.empty"() : () -> tensor<i16>
    %99 = "linalg.dot"(%60, %63, %98) <{operandSegmentSizes = array<i32: 2, 1>}> ({
    ^bb0(%arg0: i16, %arg1: i16, %arg2: i16):
      %220 = "arith.muli"(%arg0, %arg1) : (i16, i16) -> i16
      %221 = "arith.addi"(%arg2, %220) : (i16, i16) -> i16
      "linalg.yield"(%221) : (i16) -> ()
    }) {linalg.memoized_indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>]} : (tensor<11xi16>, tensor<11xi16>, tensor<i16>) -> tensor<i16>
    %100 = "spirv.BitwiseXor"(%94, %94) : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %101 = "arith.divui"(%2, %12) : (i1, i1) -> i1
    %102 = "vector.broadcast"(%14) : (f16) -> vector<f16>
    "vector.transfer_write"(%102, %75, %34) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, permutation_map = affine_map<(d0) -> ()>}> : (vector<f16>, memref<?xf16>, index) -> ()
    %103 = "spirv.BitwiseAnd"(%94, %94) : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %104 = "memref.atomic_rmw"(%9, %73, %23, %42, %21) <{kind = 2 : i64}> : (i32, memref<11x28x11xi32>, index, index, index) -> i32
    %105 = "math.powf"(%50, %50) <{fastmath = #arith.fastmath<none>}> : (tensor<32xf32>, tensor<32xf32>) -> tensor<32xf32>
    %106 = "vector.extract_strided_slice"(%94) <{offsets = [0], sizes = [1], strides = [1]}> : (vector<2xi32>) -> vector<1xi32>
    %107 = "index.shrs"(%22, %22) : (index, index) -> index
    %108 = "spirv.CL.rsqrt"(%84) : (f32) -> f32
    %109 = "index.castu"(%28) : (index) -> i32
    %110 = "arith.remf"(%0, %6) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %111 = "spirv.GL.Tanh"(%84) : (f32) -> f32
    %112 = "spirv.CL.erf"(%7) : (f32) -> f32
    %113 = "spirv.UGreaterThan"(%10, %5) : (i32, i32) -> i1
    %114 = "spirv.FOrdLessThanEqual"(%1, %6) : (f32, f32) -> i1
    "memref.store"(%80, %70, %16, %24, %25) <{nontemporal = false}> : (f32, memref<?x28x11xf32>, index, index, index) -> ()
    %115 = "math.exp"(%59) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>) -> tensor<11xf32>
    %116 = "vector.extract_strided_slice"(%94) <{offsets = [0], sizes = [1], strides = [1]}> : (vector<2xi32>) -> vector<1xi32>
    %117 = "memref.atomic_rmw"(%87, %79, %16) <{kind = 2 : i64}> : (i16, memref<?xi16>, index) -> i16
    %118 = "spirv.FNegate"(%112) : (f32) -> f32
    %119 = "index.sub"(%26, %18) : (index, index) -> index
    %120 = "spirv.GL.Exp"(%112) : (f32) -> f32
    %121 = "math.fma"(%111, %6, %120) <{fastmath = #arith.fastmath<none>}> : (f32, f32, f32) -> f32
    %122 = "spirv.BitCount"(%15) : (i32) -> i32
    %123 = "spirv.CL.rsqrt"(%111) : (f32) -> f32
    %124 = "tensor.extract"(%50, %43) : (tensor<32xf32>, index) -> f32
    %125 = "vector.broadcast"(%91) : (i1) -> vector<1xi1>
    %126 = "vector.mask"(%125) ({
      %220 = "vector.multi_reduction"(%106, %116) <{kind = #vector.kind<or>, reduction_dims = []}> : (vector<1xi32>, vector<1xi32>) -> vector<1xi32>
      "vector.yield"(%220) : (vector<1xi32>) -> ()
    }) : (vector<1xi1>) -> vector<1xi32>
    %127 = "spirv.CL.cos"(%123) : (f32) -> f32
    %128 = "math.expm1"(%89) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %129 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<28x28xi16>
    %130 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<28x32xi16>
    %131 = "tensor.empty"() : () -> tensor<28x32xi16>
    %132 = "linalg.matmul"(%129, %130, %131) <{operandSegmentSizes = array<i32: 2, 1>}> ({
    ^bb0(%arg0: i16, %arg1: i16, %arg2: i16):
      %220 = "arith.muli"(%arg0, %arg1) : (i16, i16) -> i16
      %221 = "arith.addi"(%arg2, %220) : (i16, i16) -> i16
      "linalg.yield"(%221) : (i16) -> ()
    }) {linalg.memoized_indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>]} : (memref<28x28xi16>, memref<28x32xi16>, tensor<28x32xi16>) -> tensor<28x32xi16>
    %133 = "bufferization.clone"(%77) : (memref<11x28x11xi16>) -> memref<11x28x11xi16>
    %134 = "arith.remsi"(%8, %4) : (i16, i16) -> i16
    %135 = "spirv.BitwiseXor"(%94, %94) : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %136 = "math.ipowi"(%131, %131) : (tensor<28x32xi16>, tensor<28x32xi16>) -> tensor<28x32xi16>
    %137 = "spirv.GL.FMax"(%120, %108) : (f32, f32) -> f32
    %138 = "spirv.CL.rsqrt"(%1) : (f32) -> f32
    %139 = "arith.addf"(%120, %138) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %140 = "math.sqrt"(%108) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %141 = "spirv.CL.tanh"(%6) : (f32) -> f32
    %142 = "arith.addi"(%9, %122) : (i32, i32) -> i32
    %143 = "spirv.GL.Acos"(%108) : (f32) -> f32
    %144 = "arith.cmpi"(%9, %5) <{predicate = 3 : i64}> : (i32, i32) -> i1
    "memref.tensor_store"(%52, %79) : (tensor<?xi16>, memref<?xi16>) -> ()
    %145 = "spirv.GL.UMax"(%87, %87) : (i16, i16) -> i16
    %146 = "tensor.extract"(%51, %28) : (tensor<32xi64>, index) -> i64
    %147 = "math.ctlz"(%97) : (tensor<?xi16>) -> tensor<?xi16>
    %148 = "spirv.CL.cos"(%138) : (f32) -> f32
    %149 = "math.fpowi"(%111, %15) <{fastmath = #arith.fastmath<none>}> : (f32, i32) -> f32
    %150 = "spirv.IEqual"(%145, %87) : (i16, i16) -> i1
    %151 = "bufferization.to_memref"(%61) : (tensor<?xf16>) -> memref<?xf16>
    %152 = "vector.shuffle"(%116, %116) <{mask = [0, 1]}> : (vector<1xi32>, vector<1xi32>) -> vector<2xi32>
    %153 = "bufferization.clone"(%68) : (memref<11x28x11xi1>) -> memref<11x28x11xi1>
    %154 = "tensor.empty"() : () -> tensor<i64>
    %155 = "linalg.dot"(%51, %51, %154) <{operandSegmentSizes = array<i32: 2, 1>}> ({
    ^bb0(%arg0: i64, %arg1: i64, %arg2: i64):
      %220 = "arith.muli"(%arg0, %arg1) : (i64, i64) -> i64
      %221 = "arith.addi"(%arg2, %220) : (i64, i64) -> i64
      "linalg.yield"(%221) : (i64) -> ()
    }) {linalg.memoized_indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>]} : (tensor<32xi64>, tensor<32xi64>, tensor<i64>) -> tensor<i64>
    %156 = "vector.broadcast"(%2) : (i1) -> vector<11x28x32xi1>
    %157 = "vector.broadcast"(%113) : (i1) -> vector<28x32xi1>
    %158:2 = "vector.scan"(%156, %157) <{inclusive = true, kind = #vector.kind<xor>, reduction_dim = 0 : i64}> : (vector<11x28x32xi1>, vector<28x32xi1>) -> (vector<11x28x32xi1>, vector<28x32xi1>)
    "memref.store"(%3, %68, %24, %28, %24) <{nontemporal = false}> : (i1, memref<11x28x11xi1>, index, index, index) -> ()
    %159 = "spirv.CL.fabs"(%14) : (f16) -> f16
    %160 = "spirv.CL.s_max"(%10, %11) : (i32, i32) -> i32
    %161 = "memref.cast"(%78) : (memref<11x28x11xi32>) -> memref<?x?x11xi32>
    %162 = "index.sub"(%42, %21) : (index, index) -> index
    %163 = "index.shrs"(%43, %45) : (index, index) -> index
    %164 = "vector.mask"(%125) ({
      %220 = "vector.multi_reduction"(%125, %125) <{kind = #vector.kind<maxui>, reduction_dims = []}> : (vector<1xi1>, vector<1xi1>) -> vector<1xi1>
      "vector.yield"(%220) : (vector<1xi1>) -> ()
    }) : (vector<1xi1>) -> vector<1xi1>
    %165 = "math.round"(%111) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %166 = "spirv.CL.tanh"(%92) : (f32) -> f32
    %167 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32x28xi16>
    %168 = "tensor.empty"() : () -> tensor<28x28xi16>
    %169 = "linalg.matmul"(%131, %167, %168) <{operandSegmentSizes = array<i32: 2, 1>}> ({
    ^bb0(%arg0: i16, %arg1: i16, %arg2: i16):
      %220 = "arith.muli"(%arg0, %arg1) : (i16, i16) -> i16
      %221 = "arith.addi"(%arg2, %220) : (i16, i16) -> i16
      "linalg.yield"(%221) : (i16) -> ()
    }) {linalg.memoized_indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>]} : (tensor<28x32xi16>, memref<32x28xi16>, tensor<28x28xi16>) -> tensor<28x28xi16>
    %170 = "math.tan"(%0) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %171 = "vector.broadcast"(%166) : (f32) -> vector<32xf32>
    %172 = "vector.fma"(%171, %171, %171) : (vector<32xf32>, vector<32xf32>, vector<32xf32>) -> vector<32xf32>
    %173 = "affine.if"(%47, %23) ({
      %220 = "affine.max"(%47) <{map = affine_map<(d0) -> ((d0 ceildiv 64) ceildiv 128)>}> : (index) -> index
      %221 = "math.atan"(%123) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      %222 = "math.roundeven"(%159) <{fastmath = #arith.fastmath<none>}> : (f16) -> f16
      %223 = "index.sizeof"() : () -> index
      %224 = "arith.xori"(%114, %2) : (i1, i1) -> i1
      %225 = "vector.transpose"(%94) <{transp = [0]}> : (vector<2xi32>) -> vector<2xi32>
      %226 = "tensor.from_elements"(%8, %4, %87, %87, %4, %145, %4, %4, %87, %8, %145) : (i16, i16, i16, i16, i16, i16, i16, i16, i16, i16, i16) -> tensor<11xi16>
      %227 = "vector.extract"(%94) <{static_position = array<i64: 0>}> : (vector<2xi32>) -> i32
      "affine.yield"(%159) : (f16) -> ()
    }, {
      %220 = "math.round"(%58) <{fastmath = #arith.fastmath<none>}> : (tensor<28xf32>) -> tensor<28xf32>
      %221 = "index.shrs"(%26, %107) : (index, index) -> index
      %222 = "vector.broadcast"(%159) : (f16) -> vector<12xf16>
      %223 = "vector.broadcast"(%113) : (i1) -> vector<12xi1>
      "vector.compressstore"(%65, %17, %223, %222) : (memref<28xf16>, index, vector<12xi1>, vector<12xf16>) -> ()
      %224 = "math.fma"(%50, %50, %50) <{fastmath = #arith.fastmath<none>}> : (tensor<32xf32>, tensor<32xf32>, tensor<32xf32>) -> tensor<32xf32>
      %225 = "vector.shuffle"(%171, %171) <{mask = [3, 4, 5, 6, 7, 8, 10, 13, 14, 15, 16, 19, 22, 24, 25, 30, 33, 36, 42, 43, 44, 46, 47, 48, 49, 50, 51, 54, 55, 56, 58, 59, 60, 63]}> : (vector<32xf32>, vector<32xf32>) -> vector<34xf32>
      %226 = "memref.cast"(%72) : (memref<?xi32>) -> memref<32xi32>
      "memref.assume_alignment"(%78) <{alignment = 8 : i32}> : (memref<11x28x11xi32>) -> ()
      %227 = "scf.index_switch"(%28) <{cases = array<i64: 1>}> ({
        %228 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<28xi1>
        %229 = "vector.shuffle"(%125, %125) <{mask = [1]}> : (vector<1xi1>, vector<1xi1>) -> vector<1xi1>
        %230 = "vector.mask"(%125) ({
          %245 = "vector.multi_reduction"(%125, %125) <{kind = #vector.kind<minsi>, reduction_dims = []}> : (vector<1xi1>, vector<1xi1>) -> vector<1xi1>
          "vector.yield"(%245) : (vector<1xi1>) -> ()
        }) : (vector<1xi1>) -> vector<1xi1>
        %231 = "arith.remf"(%137, %120) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %232 = "vector.splat"(%1) : (f32) -> vector<32xf32>
        %233 = "vector.mask"(%125) ({
          %245 = "vector.multi_reduction"(%106, %116) <{kind = #vector.kind<maxui>, reduction_dims = []}> : (vector<1xi32>, vector<1xi32>) -> vector<1xi32>
          "vector.yield"(%245) : (vector<1xi32>) -> ()
        }) : (vector<1xi1>) -> vector<1xi32>
        %234 = "math.log1p"(%55) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>) -> tensor<11xf32>
        %235 = "affine.max"(%20, %21, %162) <{map = affine_map<(d0, d1, d2) -> (d2 - 80)>}> : (index, index, index) -> index
        %236 = "memref.cast"(%79) : (memref<?xi16>) -> memref<?xi16>
        %237 = "arith.muli"(%11, %15) : (i32, i32) -> i32
        %238 = "arith.negf"(%124) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %239 = "affine.min"(%36, %24, %37, %163) <{map = affine_map<(d0, d1, d2)[s0] -> (d0 floordiv 64 + 4)>}> : (index, index, index, index) -> index
        %240 = "affine.vector_load"(%66, %119) <{map = affine_map<(d0) -> (d0)>}> : (memref<?xf32>, index) -> vector<28xf32>
        %241 = "arith.minui"(%10, %10) : (i32, i32) -> i32
        %242 = "index.divu"(%45, %221) : (index, index) -> index
        %243 = "index.or"(%25, %29) : (index, index) -> index
        %244 = "tensor.empty"(%17) : (index) -> tensor<?xi1>
        "scf.yield"(%244) : (tensor<?xi1>) -> ()
      }, {
        %228 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xf16>
        %229 = "vector.broadcast"(%159) : (f16) -> vector<11xf16>
        %230 = "vector.broadcast"(%150) : (i1) -> vector<11xi1>
        %231 = "vector.broadcast"(%11) : (i32) -> vector<11xi32>
        %232 = "vector.gather"(%228, %23, %231, %230, %229) : (memref<32xf16>, index, vector<11xi32>, vector<11xi1>, vector<11xf16>) -> vector<11xf16>
        %233 = "math.roundeven"(%89) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %234 = "tensor.rank"(%51) : (tensor<32xi64>) -> index
        %235 = "math.exp2"(%123) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %236 = "arith.remsi"(%150, %113) : (i1, i1) -> i1
        %237 = "arith.addf"(%84, %118) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %238 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<28xi32>
        %239 = "math.powf"(%120, %6) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %240 = "tensor.empty"() : () -> tensor<28xi32>
        %241 = "math.fpowi"(%58, %240) <{fastmath = #arith.fastmath<none>}> : (tensor<28xf32>, tensor<28xi32>) -> tensor<28xf32>
        %242 = "tensor.empty"() : () -> tensor<28xi64>
        %243 = "math.absi"(%15) : (i32) -> i32
        %244 = "arith.divf"(%0, %96) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %245 = "arith.cmpi"(%146, %146) <{predicate = 4 : i64}> : (i64, i64) -> i1
        %246 = "arith.ori"(%5, %15) : (i32, i32) -> i32
        %247 = "math.round"(%55) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>) -> tensor<11xf32>
        %248 = "arith.addi"(%5, %13) : (i32, i32) -> i32
        %249 = "tensor.empty"(%119) : (index) -> tensor<?xi1>
        "scf.yield"(%249) : (tensor<?xi1>) -> ()
      }) : (index) -> tensor<?xi1>
      "affine.yield"(%14) : (f16) -> ()
    }) {condition = affine_set<(d0, d1) : (d0 + d1 * 4096 >= 0, -d0 == 0, -d1 >= 0)>} : (index, index) -> f16
    %174 = "math.sqrt"(%0) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %175 = "memref.load"(%67, %16) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
    %176 = "spirv.GL.Sqrt"(%89) : (f32) -> f32
    %177 = "spirv.ULessThan"(%15, %15) : (i32, i32) -> i1
    %178 = "vector.insertelement"(%80, %171, %18) : (f32, vector<32xf32>, index) -> vector<32xf32>
    %179 = "arith.ceildivsi"(%145, %4) : (i16, i16) -> i16
    %180 = "math.ctlz"(%53) : (tensor<?x28x11xi1>) -> tensor<?x28x11xi1>
    %181 = "spirv.CL.fmin"(%84, %112) : (f32, f32) -> f32
    %182 = "index.divs"(%30, %107) : (index, index) -> index
    %183 = "spirv.CL.s_max"(%87, %8) : (i16, i16) -> i16
    %184 = "spirv.CL.rint"(%118) : (f32) -> f32
    %185 = "math.fpowi"(%96, %10) <{fastmath = #arith.fastmath<none>}> : (f32, i32) -> f32
    %186 = "spirv.GL.Fma"(%123, %143, %123) : (f32, f32, f32) -> f32
    %187 = "spirv.CL.erf"(%159) : (f16) -> f16
    %188 = "vector.splat"(%92) : (f32) -> vector<11x28x11xf32>
    %189 = "tensor.empty"() : () -> tensor<28xi16>
    %190 = "vector.broadcast"(%183) : (i16) -> vector<32xi16>
    %191 = "vector.broadcast"(%177) : (i1) -> vector<32xi1>
    %192 = "vector.broadcast"(%122) : (i32) -> vector<32xi32>
    %193 = "vector.gather"(%189, %44, %192, %191, %190) : (tensor<28xi16>, index, vector<32xi32>, vector<32xi1>, vector<32xi16>) -> vector<32xi16>
    %194 = "spirv.LogicalOr"(%2, %114) : (i1, i1) -> i1
    %195 = "vector.reduction"(%192) <{fastmath = #arith.fastmath<none>, kind = #vector.kind<maxui>}> : (vector<32xi32>) -> i32
    %196 = "index.and"(%41, %45) : (index, index) -> index
    %197 = "math.atan2"(%92, %112) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %198 = "memref.realloc"(%71) : (memref<28xf32>) -> memref<11xf32>
    %199 = "spirv.LogicalOr"(%12, %177) : (i1, i1) -> i1
    %200 = "vector.shuffle"(%190, %193) <{mask = [0, 1, 4, 7, 9, 11, 12, 13, 14, 17, 18, 19, 20, 23, 25, 28, 29, 32, 35, 36, 37, 39, 41, 42, 43, 44, 46, 48, 52, 56, 58, 59, 60]}> : (vector<32xi16>, vector<32xi16>) -> vector<33xi16>
    %201 = "tensor.empty"() : () -> tensor<28xf16>
    %202 = "spirv.BitwiseXor"(%94, %94) : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %203 = "math.powf"(%123, %123) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %204 = "tensor.insert"(%80, %50, %16) : (f32, tensor<32xf32>, index) -> tensor<32xf32>
    %205 = "affine.apply"(%119, %163, %21, %19) <{map = affine_map<(d0, d1, d2, d3) -> (-d1 - (d1 ceildiv 16 + 2))>}> : (index, index, index, index) -> index
    %206 = "affine.vector_load"(%64, %23, %20, %28) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> : (memref<?x28x11xi64>, index, index, index) -> vector<28xi64>
    %207 = "tensor.splat"(%138) : (f32) -> tensor<32xf32>
    %208 = "vector.broadcast"(%183) : (i16) -> vector<11x28xi16>
    %209 = "vector.broadcast"(%4) : (i16) -> vector<28xi16>
    %210:2 = "vector.scan"(%208, %209) <{inclusive = false, kind = #vector.kind<maxui>, reduction_dim = 0 : i64}> : (vector<11x28xi16>, vector<28xi16>) -> (vector<11x28xi16>, vector<28xi16>)
    %211 = "spirv.FOrdLessThan"(%80, %96) : (f32, f32) -> i1
    %212 = "math.exp"(%127) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %213 = "spirv.FUnordLessThan"(%80, %176) : (f32, f32) -> i1
    %214 = "spirv.CL.s_max"(%122, %11) : (i32, i32) -> i32
    %215 = "spirv.GL.InverseSqrt"(%92) : (f32) -> f32
    %216 = "vector.broadcast"(%196) : (index) -> vector<32xindex>
    "vector.scatter"(%153, %21, %25, %17, %216, %191, %191) : (memref<11x28x11xi1>, index, index, index, vector<32xindex>, vector<32xi1>, vector<32xi1>) -> ()
    "vector.compressstore"(%74, %16, %191, %191) : (memref<?xi1>, index, vector<32xi1>, vector<32xi1>) -> ()
    %217 = "arith.remf"(%176, %92) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %218 = "math.sqrt"(%148) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    "memref.store"(%122, %78, %21, %30, %24) <{nontemporal = false}> : (i32, memref<11x28x11xi32>, index, index, index) -> ()
    %219 = "arith.remf"(%96, %89) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    "vector.print"(%94) <{punctuation = #vector.punctuation<newline>}> : (vector<2xi32>) -> ()
    "vector.print"(%102) <{punctuation = #vector.punctuation<newline>}> : (vector<f16>) -> ()
    "vector.print"(%106) <{punctuation = #vector.punctuation<newline>}> : (vector<1xi32>) -> ()
    "vector.print"(%116) <{punctuation = #vector.punctuation<newline>}> : (vector<1xi32>) -> ()
    "vector.print"(%125) <{punctuation = #vector.punctuation<newline>}> : (vector<1xi1>) -> ()
    "vector.print"(%171) <{punctuation = #vector.punctuation<newline>}> : (vector<32xf32>) -> ()
    "vector.print"(%172) <{punctuation = #vector.punctuation<newline>}> : (vector<32xf32>) -> ()
    "vector.print"(%190) <{punctuation = #vector.punctuation<newline>}> : (vector<32xi16>) -> ()
    "vector.print"(%191) <{punctuation = #vector.punctuation<newline>}> : (vector<32xi1>) -> ()
    "vector.print"(%192) <{punctuation = #vector.punctuation<newline>}> : (vector<32xi32>) -> ()
    "vector.print"(%193) <{punctuation = #vector.punctuation<newline>}> : (vector<32xi16>) -> ()
    "vector.print"(%206) <{punctuation = #vector.punctuation<newline>}> : (vector<28xi64>) -> ()
    "vector.print"(%0) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%1) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%2) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%3) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%4) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%5) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%6) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%7) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%8) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%9) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%10) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%11) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%12) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%13) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%14) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%15) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%80) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%84) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%87) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%89) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%91) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%92) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%96) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%108) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%111) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%112) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%113) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%114) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%118) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%120) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%122) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%123) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%124) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%127) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%137) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%138) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%141) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%143) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%145) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%146) <{punctuation = #vector.punctuation<newline>}> : (i64) -> ()
    "vector.print"(%148) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%150) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%159) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%160) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%166) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%176) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%177) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%181) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%183) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%184) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%186) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%187) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%194) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%199) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%211) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%213) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%214) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%215) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "func.return"() : () -> ()
  }) : () -> ()
  "func.func"() <{function_type = (vector<11x28x11xf32>, index, memref<?xi64>) -> vector<11xf32>, sym_name = "func2", sym_visibility = "nested"}> ({
  ^bb0(%arg0: vector<11x28x11xf32>, %arg1: index, %arg2: memref<?xi64>):
    %0 = "arith.constant"() <{value = 0x4E0CD9CF : f32}> : () -> f32
    %1 = "arith.constant"() <{value = 1.09581914E+9 : f32}> : () -> f32
    %2 = "arith.constant"() <{value = false}> : () -> i1
    %3 = "arith.constant"() <{value = false}> : () -> i1
    %4 = "arith.constant"() <{value = 29053 : i16}> : () -> i16
    %5 = "arith.constant"() <{value = 635697735 : i32}> : () -> i32
    %6 = "arith.constant"() <{value = 0x4DD449FE : f32}> : () -> f32
    %7 = "arith.constant"() <{value = 1.92379226E+9 : f32}> : () -> f32
    %8 = "arith.constant"() <{value = -3272 : i16}> : () -> i16
    %9 = "arith.constant"() <{value = 1056808192 : i32}> : () -> i32
    %10 = "arith.constant"() <{value = 1616561925 : i32}> : () -> i32
    %11 = "arith.constant"() <{value = 1935052110 : i32}> : () -> i32
    %12 = "arith.constant"() <{value = false}> : () -> i1
    %13 = "arith.constant"() <{value = 806696163 : i32}> : () -> i32
    %14 = "arith.constant"() <{value = 4.668000e+03 : f16}> : () -> f16
    %15 = "arith.constant"() <{value = 1109543139 : i32}> : () -> i32
    %16 = "arith.constant"() <{value = 0 : index}> : () -> index
    %17 = "arith.constant"() <{value = 1 : index}> : () -> index
    %18 = "arith.constant"() <{value = 2 : index}> : () -> index
    %19 = "arith.constant"() <{value = 3 : index}> : () -> index
    %20 = "arith.constant"() <{value = 4 : index}> : () -> index
    %21 = "arith.constant"() <{value = 5 : index}> : () -> index
    %22 = "arith.constant"() <{value = 6 : index}> : () -> index
    %23 = "arith.constant"() <{value = 7 : index}> : () -> index
    %24 = "arith.constant"() <{value = 8 : index}> : () -> index
    %25 = "arith.constant"() <{value = 9 : index}> : () -> index
    %26 = "arith.constant"() <{value = 10 : index}> : () -> index
    %27 = "arith.constant"() <{value = 11 : index}> : () -> index
    %28 = "arith.constant"() <{value = 12 : index}> : () -> index
    %29 = "arith.constant"() <{value = 13 : index}> : () -> index
    %30 = "arith.constant"() <{value = 14 : index}> : () -> index
    %31 = "arith.constant"() <{value = 15 : index}> : () -> index
    %32 = "arith.constant"() <{value = 16 : index}> : () -> index
    %33 = "arith.constant"() <{value = 17 : index}> : () -> index
    %34 = "arith.constant"() <{value = 18 : index}> : () -> index
    %35 = "arith.constant"() <{value = 19 : index}> : () -> index
    %36 = "arith.constant"() <{value = 20 : index}> : () -> index
    %37 = "arith.constant"() <{value = 21 : index}> : () -> index
    %38 = "arith.constant"() <{value = 22 : index}> : () -> index
    %39 = "arith.constant"() <{value = 23 : index}> : () -> index
    %40 = "arith.constant"() <{value = 24 : index}> : () -> index
    %41 = "arith.constant"() <{value = 25 : index}> : () -> index
    %42 = "arith.constant"() <{value = 26 : index}> : () -> index
    %43 = "arith.constant"() <{value = 27 : index}> : () -> index
    %44 = "arith.constant"() <{value = 28 : index}> : () -> index
    %45 = "arith.constant"() <{value = 29 : index}> : () -> index
    %46 = "arith.constant"() <{value = 30 : index}> : () -> index
    %47 = "arith.constant"() <{value = 31 : index}> : () -> index
    %48 = "tensor.empty"(%29) : (index) -> tensor<?x28x11xf16>
    %49 = "tensor.empty"(%39) : (index) -> tensor<?xi32>
    %50 = "tensor.empty"() : () -> tensor<32xf32>
    %51 = "tensor.empty"() : () -> tensor<32xi64>
    %52 = "tensor.empty"(%34) : (index) -> tensor<?xi16>
    %53 = "tensor.empty"(%20) : (index) -> tensor<?x28x11xi1>
    %54 = "tensor.empty"(%18) : (index) -> tensor<?x28x11xi64>
    %55 = "tensor.empty"() : () -> tensor<11xf32>
    %56 = "tensor.empty"() : () -> tensor<11x28x11xi64>
    %57 = "tensor.empty"(%arg1) : (index) -> tensor<?xi1>
    %58 = "tensor.empty"() : () -> tensor<28xf32>
    %59 = "tensor.empty"() : () -> tensor<11xf32>
    %60 = "tensor.empty"() : () -> tensor<11xi16>
    %61 = "tensor.empty"(%19) : (index) -> tensor<?xf16>
    %62 = "tensor.empty"() : () -> tensor<28xi64>
    %63 = "tensor.empty"() : () -> tensor<11xi16>
    %64 = "memref.alloc"(%38) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?x28x11xi64>
    %65 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<28xf16>
    %66 = "memref.alloc"(%30) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xf32>
    %67 = "memref.alloc"(%26) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xf32>
    %68 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11x28x11xi1>
    %69 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11xf16>
    %70 = "memref.alloc"(%40) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?x28x11xf32>
    %71 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<28xf32>
    %72 = "memref.alloc"(%20) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xi32>
    %73 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11x28x11xi32>
    %74 = "memref.alloc"(%43) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xi1>
    %75 = "memref.alloc"(%42) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xf16>
    %76 = "memref.alloc"(%20, %35, %44) <{operandSegmentSizes = array<i32: 3, 0>}> : (index, index, index) -> memref<?x?x?xi32>
    %77 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11x28x11xi16>
    %78 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11x28x11xi32>
    %79 = "memref.alloc"(%46) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xi16>
    %80 = "vector.broadcast"(%3) : (i1) -> vector<11x28x11xi1>
    %81 = "vector.broadcast"(%1) : (f32) -> vector<32xf32>
    %82 = "vector.fma"(%81, %81, %81) : (vector<32xf32>, vector<32xf32>, vector<32xf32>) -> vector<32xf32>
    %83 = "vector.insertelement"(%6, %82, %22) : (f32, vector<32xf32>, index) -> vector<32xf32>
    %84 = "spirv.GL.SAbs"(%8) : (i16) -> i16
    %85 = "vector.shuffle"(%81, %82) <{mask = [1, 7, 8, 9, 14, 16, 20, 21, 24, 26, 30, 32, 34, 35, 36, 39, 42, 43, 44, 45, 46, 49, 50, 51, 55, 59, 60, 61, 62, 63]}> : (vector<32xf32>, vector<32xf32>) -> vector<30xf32>
    %86 = "spirv.CL.s_max"(%5, %5) : (i32, i32) -> i32
    %87 = "math.powf"(%59, %55) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>, tensor<11xf32>) -> tensor<11xf32>
    %88 = "spirv.GL.Tanh"(%0) : (f32) -> f32
    %89 = "spirv.IEqual"(%13, %86) : (i32, i32) -> i1
    %90 = "affine.vector_load"(%78, %47, %18, %44) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> : (memref<11x28x11xi32>, index, index, index) -> vector<28xi32>
    %91 = "spirv.UGreaterThanEqual"(%8, %8) : (i16, i16) -> i1
    %92 = "arith.minsi"(%86, %13) : (i32, i32) -> i32
    %93 = "spirv.GL.Fma"(%1, %1, %1) : (f32, f32, f32) -> f32
    %94 = "math.fma"(%7, %0, %0) <{fastmath = #arith.fastmath<none>}> : (f32, f32, f32) -> f32
    %95 = "index.sub"(%37, %22) : (index, index) -> index
    %96 = "spirv.GL.Tanh"(%7) : (f32) -> f32
    %97 = "arith.divui"(%86, %11) : (i32, i32) -> i32
    "scf.index_switch"(%arg1) <{cases = array<i64: 1, 2, 3, 4>}> ({
      %224 = "math.round"(%59) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>) -> tensor<11xf32>
      %225 = "arith.remf"(%0, %96) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      %226 = "vector.extract_strided_slice"(%90) <{offsets = [3], sizes = [17], strides = [1]}> : (vector<28xi32>) -> vector<17xi32>
      %227 = "math.tanh"(%50) <{fastmath = #arith.fastmath<none>}> : (tensor<32xf32>) -> tensor<32xf32>
      %228 = "arith.addi"(%2, %12) : (i1, i1) -> i1
      "memref.assume_alignment"(%arg2) <{alignment = 4 : i32}> : (memref<?xi64>) -> ()
      %229 = "math.fpowi"(%7, %9) <{fastmath = #arith.fastmath<none>}> : (f32, i32) -> f32
      %230 = "math.floor"(%48) <{fastmath = #arith.fastmath<none>}> : (tensor<?x28x11xf16>) -> tensor<?x28x11xf16>
      %231 = "vector.broadcast"(%33) : (index) -> vector<28xindex>
      %232 = "vector.broadcast"(%91) : (i1) -> vector<28xi1>
      %233 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %234 = "vector.broadcast"(%233) : (i64) -> vector<28xi64>
      "vector.scatter"(%64, %16, %25, %26, %231, %232, %234) : (memref<?x28x11xi64>, index, index, index, vector<28xindex>, vector<28xi1>, vector<28xi64>) -> ()
      %235 = "math.tanh"(%58) <{fastmath = #arith.fastmath<none>}> : (tensor<28xf32>) -> tensor<28xf32>
      %236 = "math.round"(%55) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>) -> tensor<11xf32>
      "memref.store"(%14, %75, %16) <{nontemporal = false}> : (f16, memref<?xf16>, index) -> ()
      %237 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<12x28xi64>
      %238 = "tensor.empty"() : () -> tensor<28x12xi64>
      %239 = "tensor.empty"() : () -> tensor<12x12xi64>
      %240 = "linalg.matmul"(%237, %238, %239) <{operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg3: i64, %arg4: i64, %arg5: i64):
        %244 = "arith.muli"(%arg3, %arg4) : (i64, i64) -> i64
        %245 = "arith.addi"(%arg5, %244) : (i64, i64) -> i64
        "linalg.yield"(%245) : (i64) -> ()
      }) {linalg.memoized_indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>]} : (memref<12x28xi64>, tensor<28x12xi64>, tensor<12x12xi64>) -> tensor<12x12xi64>
      %241 = "index.sub"(%42, %31) : (index, index) -> index
      %242 = "vector.transpose"(%90) <{transp = [0]}> : (vector<28xi32>) -> vector<28xi32>
      %243 = "math.log"(%58) <{fastmath = #arith.fastmath<none>}> : (tensor<28xf32>) -> tensor<28xf32>
      "scf.yield"() : () -> ()
    }, {
      %224 = "arith.floordivsi"(%13, %5) : (i32, i32) -> i32
      %225 = "vector.broadcast"(%96) : (f32) -> vector<11x28x11xf32>
      %226 = "vector.broadcast"(%11) : (i32) -> vector<11x28x11xi32>
      %227 = "vector.gather"(%50, %44, %226, %80, %225) : (tensor<32xf32>, index, vector<11x28x11xi32>, vector<11x28x11xi1>, vector<11x28x11xf32>) -> vector<11x28x11xf32>
      %228 = "arith.addi"(%12, %12) : (i1, i1) -> i1
      %229 = "vector.reduction"(%81) <{fastmath = #arith.fastmath<none>, kind = #vector.kind<mul>}> : (vector<32xf32>) -> f32
      "vector.print"(%226) <{punctuation = #vector.punctuation<newline>}> : (vector<11x28x11xi32>) -> ()
      %230 = "memref.alloca"(%21) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xi16>
      %231 = "vector.extract_strided_slice"(%225) <{offsets = [4], sizes = [7], strides = [1]}> : (vector<11x28x11xf32>) -> vector<7x28x11xf32>
      %232 = "math.fma"(%96, %6, %7) <{fastmath = #arith.fastmath<none>}> : (f32, f32, f32) -> f32
      "scf.parallel"(%29, %24, %17, %39, %19, %35) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
      ^bb0(%arg3: index, %arg4: index):
        %239 = "vector.broadcast"(%93) : (f32) -> vector<f32>
        %240 = "vector.transfer_write"(%239, %50, %37) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, permutation_map = affine_map<(d0) -> ()>}> : (vector<f32>, tensor<32xf32>, index) -> tensor<32xf32>
        %241 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11x12xi16>
        %242 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<12x11xi16>
        %243 = "tensor.empty"() : () -> tensor<11x11xi16>
        %244 = "linalg.matmul"(%241, %242, %243) <{operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg5: i16, %arg6: i16, %arg7: i16):
          %261 = "arith.muli"(%arg5, %arg6) : (i16, i16) -> i16
          %262 = "arith.addi"(%arg7, %261) : (i16, i16) -> i16
          "linalg.yield"(%262) : (i16) -> ()
        }) {linalg.memoized_indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>]} : (memref<11x12xi16>, memref<12x11xi16>, tensor<11x11xi16>) -> tensor<11x11xi16>
        %245 = "index.and"(%42, %42) : (index, index) -> index
        %246 = "arith.addi"(%2, %2) : (i1, i1) -> i1
        %247 = "index.or"(%19, %45) : (index, index) -> index
        "memref.copy"(%70, %70) : (memref<?x28x11xf32>, memref<?x28x11xf32>) -> ()
        %248 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xf32>
        %249 = "vector.gather"(%248, %23, %226, %80, %227) : (memref<32xf32>, index, vector<11x28x11xi32>, vector<11x28x11xi1>, vector<11x28x11xf32>) -> vector<11x28x11xf32>
        %250 = "tensor.empty"() : () -> tensor<28xf16>
        %251 = "tensor.empty"() : () -> tensor<f16>
        %252 = "linalg.dot"(%65, %250, %251) <{operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg5: f16, %arg6: f16, %arg7: f16):
          %261 = "arith.mulf"(%arg5, %arg6) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
          %262 = "arith.addf"(%arg7, %261) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
          "linalg.yield"(%262) : (f16) -> ()
        }) {linalg.memoized_indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>]} : (memref<28xf16>, tensor<28xf16>, tensor<f16>) -> tensor<f16>
        %253 = "memref.atomic_rmw"(%1, %70, %16, %22, %23) <{kind = 0 : i64}> : (f32, memref<?x28x11xf32>, index, index, index) -> f32
        %254 = "linalg.transpose"(%49, %49) <{permutation = array<i64: 0>}> ({
        ^bb0(%arg5: i32, %arg6: i32):
          "linalg.yield"(%arg5) : (i32) -> ()
        }) : (tensor<?xi32>, tensor<?xi32>) -> tensor<?xi32>
        %255 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11x28x11xf32>
        %256 = "index.shl"(%247, %39) : (index, index) -> index
        %257 = "math.copysign"(%6, %88) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %258 = "bufferization.clone"(%73) : (memref<11x28x11xi32>) -> memref<11x28x11xi32>
        %259 = "math.copysign"(%50, %50) <{fastmath = #arith.fastmath<none>}> : (tensor<32xf32>, tensor<32xf32>) -> tensor<32xf32>
        %260 = "arith.addf"(%6, %0) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        "scf.yield"() : () -> ()
      }) : (index, index, index, index, index, index) -> ()
      %233 = "vector.reduction"(%82) <{fastmath = #arith.fastmath<none>, kind = #vector.kind<minf>}> : (vector<32xf32>) -> f32
      %234 = "arith.divui"(%3, %91) : (i1, i1) -> i1
      %235 = "math.exp"(%55) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>) -> tensor<11xf32>
      %236 = "affine.max"(%19, %24, %38) <{map = affine_map<(d0, d1)[s0] -> ((d0 - d1) floordiv 2)>}> : (index, index, index) -> index
      %237 = "arith.addf"(%1, %96) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      %238 = "math.rsqrt"(%59) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>) -> tensor<11xf32>
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<1xi32>, lowerBoundsMap = affine_map<() -> (0)>, reductions = [], steps = [1], upperBoundsGroups = dense<1> : tensor<1xi32>, upperBoundsMap = affine_map<() -> (32)>}> ({
      ^bb0(%arg3: index):
        %239 = "math.powf"(%93, %88) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        "affine.yield"() : () -> ()
      }) : () -> ()
      "scf.yield"() : () -> ()
    }, {
      %224 = "arith.xori"(%86, %11) : (i32, i32) -> i32
      %225 = "math.log10"(%14) <{fastmath = #arith.fastmath<none>}> : (f16) -> f16
      %226 = "index.sizeof"() : () -> index
      %227 = "linalg.copy"(%58, %58) <{operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg3: f32, %arg4: f32):
        "linalg.yield"(%arg3) : (f32) -> ()
      }) : (tensor<28xf32>, tensor<28xf32>) -> tensor<28xf32>
      %228 = "affine.for"(%79) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (126)>}> ({
      ^bb0(%arg3: index, %arg4: memref<?xi16>):
        %242 = "memref.alloc"(%34) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xi16>
        "affine.yield"(%242) : (memref<?xi16>) -> ()
      }) : (memref<?xi16>) -> memref<?xi16>
      %229 = "tensor.empty"() : () -> tensor<11xi32>
      %230 = "math.fpowi"(%55, %229) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>, tensor<11xi32>) -> tensor<11xf32>
      %231 = "linalg.copy"(%54, %54) <{operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg3: i64, %arg4: i64):
        "linalg.yield"(%arg3) : (i64) -> ()
      }) : (tensor<?x28x11xi64>, tensor<?x28x11xi64>) -> tensor<?x28x11xi64>
      %232 = "vector.broadcast"(%14) : (f16) -> vector<f16>
      "vector.transfer_write"(%232, %65, %22) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, permutation_map = affine_map<(d0) -> ()>}> : (vector<f16>, memref<28xf16>, index) -> ()
      %233 = "tensor.extract"(%51, %24) : (tensor<32xi64>, index) -> i64
      %234 = "bufferization.to_tensor"(%75) : (memref<?xf16>) -> tensor<?xf16>
      %235 = "vector.insertelement"(%96, %81, %25) : (f32, vector<32xf32>, index) -> vector<32xf32>
      %236 = "arith.mulf"(%7, %96) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      %237 = "tensor.empty"(%226) : (index) -> tensor<?x11xi32>
      %238 = "linalg.broadcast"(%49, %237) <{dimensions = array<i64: 1>}> ({
      ^bb0(%arg3: i32, %arg4: i32):
        "linalg.yield"(%arg3) : (i32) -> ()
      }) : (tensor<?xi32>, tensor<?x11xi32>) -> tensor<?x11xi32>
      %239 = "math.exp2"(%6) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      %240 = "math.expm1"(%58) <{fastmath = #arith.fastmath<none>}> : (tensor<28xf32>) -> tensor<28xf32>
      %241 = "math.round"(%1) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      "scf.yield"() : () -> ()
    }, {
      %224 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<28xi64>
      "memref.tensor_store"(%62, %224) : (tensor<28xi64>, memref<28xi64>) -> ()
      %225 = "arith.remui"(%4, %8) : (i16, i16) -> i16
      %226 = "math.exp"(%7) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      %227 = "index.sizeof"() : () -> index
      %228 = "math.powf"(%55, %55) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>, tensor<11xf32>) -> tensor<11xf32>
      %229 = "math.powf"(%93, %93) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      %230 = "arith.remf"(%93, %88) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      %231 = "affine.min"(%24, %18, %18, %31) <{map = affine_map<(d0, d1, d2, d3) -> (-d1 - (d1 ceildiv 16 + 2))>}> : (index, index, index, index) -> index
      "memref.copy"(%66, %67) : (memref<?xf32>, memref<?xf32>) -> ()
      %232 = "index.shrs"(%37, %37) : (index, index) -> index
      %233 = "affine.vector_load"(%77, %26, %18, %28) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> : (memref<11x28x11xi16>, index, index, index) -> vector<12xi16>
      %234 = "vector.extract"(%81) <{static_position = array<i64: 5>}> : (vector<32xf32>) -> f32
      %235 = "affine.load"(%72, %35) <{map = affine_map<(d0) -> (d0)>}> : (memref<?xi32>, index) -> i32
      %236 = "vector.broadcast"(%3) : (i1) -> vector<32xi1>
      %237 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<28x32xi1>
      %238 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32x28xi1>
      %239 = "tensor.empty"() : () -> tensor<28x28xi1>
      %240 = "linalg.matmul"(%237, %238, %239) <{operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg3: i1, %arg4: i1, %arg5: i1):
        %242 = "arith.andi"(%arg3, %arg4) : (i1, i1) -> i1
        %243 = "arith.ori"(%arg5, %242) : (i1, i1) -> i1
        "linalg.yield"(%243) : (i1) -> ()
      }) {linalg.memoized_indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>]} : (memref<28x32xi1>, memref<32x28xi1>, tensor<28x28xi1>) -> tensor<28x28xi1>
      %241 = "memref.load"(%75, %16) <{nontemporal = false}> : (memref<?xf16>, index) -> f16
      "scf.yield"() : () -> ()
    }, {
      %224 = "arith.divui"(%8, %4) : (i16, i16) -> i16
      %225 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %226 = "tensor.from_elements"(%225, %225, %225, %225, %225, %225, %225, %225, %225, %225, %225) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> tensor<11xi64>
      "memref.store"(%13, %76, %16, %16, %16) <{nontemporal = false}> : (i32, memref<?x?x?xi32>, index, index, index) -> ()
      %227 = "math.round"(%59) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>) -> tensor<11xf32>
      %228 = "vector.contract"(%81, %81, %7) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = [#vector.iterator_type<reduction>], kind = #vector.kind<minf>}> : (vector<32xf32>, vector<32xf32>, f32) -> f32
      %229 = "tensor.rank"(%48) : (tensor<?x28x11xf16>) -> index
      %230 = "affine.load"(%66, %26) <{map = affine_map<(d0) -> (d0)>}> : (memref<?xf32>, index) -> f32
      %231 = "vector.flat_transpose"(%81) <{columns = 8 : i32, rows = 4 : i32}> : (vector<32xf32>) -> vector<32xf32>
      "scf.index_switch"(%20) <{cases = array<i64: 1, 2>}> ({
        %239 = "math.fpowi"(%230, %15) <{fastmath = #arith.fastmath<none>}> : (f32, i32) -> f32
        %240 = "math.copysign"(%59, %59) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>, tensor<11xf32>) -> tensor<11xf32>
        %241 = "arith.andi"(%5, %15) : (i32, i32) -> i32
        "affine.store"(%230, %67, %43) <{map = affine_map<(d0) -> (d0)>}> : (f32, memref<?xf32>, index) -> ()
        %242 = "arith.ceildivsi"(%2, %3) : (i1, i1) -> i1
        %243 = "math.copysign"(%96, %88) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %244 = "arith.ori"(%11, %10) : (i32, i32) -> i32
        %245 = "arith.divui"(%4, %8) : (i16, i16) -> i16
        %246 = "memref.atomic_rmw"(%11, %76, %16, %16, %16) <{kind = 7 : i64}> : (i32, memref<?x?x?xi32>, index, index, index) -> i32
        %247 = "affine.max"(%35, %35, %33, %38) <{map = affine_map<(d0, d1, d2, d3) -> ((d3 - 64) ceildiv 128)>}> : (index, index, index, index) -> index
        %248 = "index.shl"(%32, %40) : (index, index) -> index
        %249 = "arith.remf"(%0, %7) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %250 = "index.bool.constant"() <{value = true}> : () -> i1
        %251 = "arith.addf"(%96, %93) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %252 = "math.cttz"(%60) : (tensor<11xi16>) -> tensor<11xi16>
        %253 = "tensor.empty"() : () -> tensor<28x12xf16>
        %254 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<12x32xf16>
        %255 = "tensor.empty"() : () -> tensor<28x32xf16>
        %256 = "linalg.matmul"(%253, %254, %255) <{operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg3: f16, %arg4: f16, %arg5: f16):
          %257 = "arith.mulf"(%arg3, %arg4) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
          %258 = "arith.addf"(%arg5, %257) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
          "linalg.yield"(%258) : (f16) -> ()
        }) {linalg.memoized_indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>]} : (tensor<28x12xf16>, memref<12x32xf16>, tensor<28x32xf16>) -> tensor<28x32xf16>
        "scf.yield"() : () -> ()
      }, {
        %239 = "math.ceil"(%96) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %240 = "arith.addi"(%225, %225) : (i64, i64) -> i64
        %241 = "vector.transpose"(%231) <{transp = [0]}> : (vector<32xf32>) -> vector<32xf32>
        %242 = "arith.cmpi"(%4, %8) <{predicate = 9 : i64}> : (i16, i16) -> i1
        %243 = "tensor.splat"(%0) : (f32) -> tensor<32xf32>
        %244 = "vector.broadcast"(%89) : (i1) -> vector<32xi1>
        %245 = "vector.mask"(%244) ({
          %258 = "vector.multi_reduction"(%82, %82) <{kind = #vector.kind<mul>, reduction_dims = []}> : (vector<32xf32>, vector<32xf32>) -> vector<32xf32>
          "vector.yield"(%258) : (vector<32xf32>) -> ()
        }) : (vector<32xi1>) -> vector<32xf32>
        %246 = "tensor.cast"(%54) : (tensor<?x28x11xi64>) -> tensor<28x28x11xi64>
        %247 = "vector.broadcast"(%91) : (i1) -> vector<28x11xi1>
        %248:2 = "vector.scan"(%80, %247) <{inclusive = true, kind = #vector.kind<mul>, reduction_dim = 0 : i64}> : (vector<11x28x11xi1>, vector<28x11xi1>) -> (vector<11x28x11xi1>, vector<28x11xi1>)
        %249 = "index.casts"(%arg1) : (index) -> i32
        %250 = "math.copysign"(%50, %50) <{fastmath = #arith.fastmath<none>}> : (tensor<32xf32>, tensor<32xf32>) -> tensor<32xf32>
        %251 = "tensor.empty"() : () -> tensor<28xi32>
        %252 = "math.fpowi"(%58, %251) <{fastmath = #arith.fastmath<none>}> : (tensor<28xf32>, tensor<28xi32>) -> tensor<28xf32>
        %253 = "vector.bitcast"(%231) : (vector<32xf32>) -> vector<32xf32>
        %254 = "arith.shrui"(%84, %84) : (i16, i16) -> i16
        %255 = "memref.alloc"(%23) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xf32>
        %256 = "math.roundeven"(%0) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %257 = "vector.insertelement"(%88, %81, %24) : (f32, vector<32xf32>, index) -> vector<32xf32>
        "scf.yield"() : () -> ()
      }, {
        %239 = "math.ceil"(%48) <{fastmath = #arith.fastmath<none>}> : (tensor<?x28x11xf16>) -> tensor<?x28x11xf16>
        %240 = "math.ctlz"(%52) : (tensor<?xi16>) -> tensor<?xi16>
        %241 = "memref.alloc"(%44) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xi32>
        %242 = "vector.broadcast"(%0) : (f32) -> vector<32xf32>
        %243 = "vector.fma"(%242, %81, %242) : (vector<32xf32>, vector<32xf32>, vector<32xf32>) -> vector<32xf32>
        %244 = "arith.shrui"(%91, %89) : (i1, i1) -> i1
        %245 = "vector.extract"(%81) <{static_position = array<i64: 1>}> : (vector<32xf32>) -> f32
        %246 = "tensor.empty"() : () -> tensor<32xi32>
        %247 = "math.fpowi"(%50, %246) <{fastmath = #arith.fastmath<none>}> : (tensor<32xf32>, tensor<32xi32>) -> tensor<32xf32>
        %248 = "index.divu"(%40, %35) : (index, index) -> index
        %249 = "linalg.copy"(%57, %57) <{operandSegmentSizes = array<i32: 1, 1>}> ({
        ^bb0(%arg3: i1, %arg4: i1):
          "linalg.yield"(%arg3) : (i1) -> ()
        }) : (tensor<?xi1>, tensor<?xi1>) -> tensor<?xi1>
        "memref.store"(%14, %69, %20) <{nontemporal = false}> : (f16, memref<11xf16>, index) -> ()
        %250 = "math.copysign"(%59, %55) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>, tensor<11xf32>) -> tensor<11xf32>
        %251 = "math.atan"(%96) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %252 = "tensor.empty"() : () -> tensor<i64>
        %253 = "linalg.dot"(%62, %62, %252) <{operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg3: i64, %arg4: i64, %arg5: i64):
          %260 = "arith.muli"(%arg3, %arg4) : (i64, i64) -> i64
          %261 = "arith.addi"(%arg5, %260) : (i64, i64) -> i64
          "linalg.yield"(%261) : (i64) -> ()
        }) {linalg.memoized_indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>]} : (tensor<28xi64>, tensor<28xi64>, tensor<i64>) -> tensor<i64>
        %254 = "math.absi"(%52) : (tensor<?xi16>) -> tensor<?xi16>
        %255 = "tensor.empty"() : () -> tensor<11x28xf16>
        %256 = "tensor.empty"() : () -> tensor<28x12xf16>
        %257 = "tensor.empty"() : () -> tensor<11x12xf16>
        %258 = "linalg.matmul"(%255, %256, %257) <{operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg3: f16, %arg4: f16, %arg5: f16):
          %260 = "arith.mulf"(%arg3, %arg4) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
          %261 = "arith.addf"(%arg5, %260) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
          "linalg.yield"(%261) : (f16) -> ()
        }) {linalg.memoized_indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>]} : (tensor<11x28xf16>, tensor<28x12xf16>, tensor<11x12xf16>) -> tensor<11x12xf16>
        %259 = "math.rsqrt"(%0) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        "scf.yield"() : () -> ()
      }) : (index) -> ()
      %232 = "bufferization.clone"(%73) : (memref<11x28x11xi32>) -> memref<11x28x11xi32>
      %233 = "tensor.extract"(%60, %25) : (tensor<11xi16>, index) -> i16
      %234 = "vector.insertelement"(%96, %231, %36) : (f32, vector<32xf32>, index) -> vector<32xf32>
      %235 = "tensor.rank"(%52) : (tensor<?xi16>) -> index
      %236 = "math.powf"(%6, %0) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      %237 = "arith.minsi"(%91, %3) : (i1, i1) -> i1
      %238 = "math.cttz"(%53) : (tensor<?x28x11xi1>) -> tensor<?x28x11xi1>
      "scf.yield"() : () -> ()
    }) : (index) -> ()
    %98 = "spirv.GL.Round"(%93) : (f32) -> f32
    %99 = "arith.divsi"(%13, %5) : (i32, i32) -> i32
    %100 = "spirv.GL.Acos"(%1) : (f32) -> f32
    %101 = "spirv.SLessThanEqual"(%84, %8) : (i16, i16) -> i1
    %102 = "vector.broadcast"(%5) : (i32) -> vector<2xi32>
    %103 = "spirv.BitwiseAnd"(%102, %102) : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %104 = "spirv.LogicalEqual"(%91, %2) : (i1, i1) -> i1
    %105 = "spirv.CL.fmin"(%1, %100) : (f32, f32) -> f32
    %106 = "memref.alloc"(%41) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xi64>
    %107 = "vector.extract"(%80) <{static_position = array<i64: 1, 27>}> : (vector<11x28x11xi1>) -> vector<11xi1>
    %108 = "affine.if"(%20, %34, %25) ({
      %224 = "affine.max"(%32, %19, %28, %arg1) <{map = affine_map<(d0, d1, d2)[s0] -> ((d0 mod 16) * 64)>}> : (index, index, index, index) -> index
      %225 = "index.maxu"(%19, %46) : (index, index) -> index
      %226 = "index.shrs"(%16, %33) : (index, index) -> index
      %227 = "bufferization.clone"(%65) : (memref<28xf16>) -> memref<28xf16>
      %228 = "vector.splat"(%43) : (index) -> vector<32xindex>
      %229 = "arith.divui"(%13, %5) : (i32, i32) -> i32
      %230 = "vector.reduction"(%90) <{fastmath = #arith.fastmath<none>, kind = #vector.kind<or>}> : (vector<28xi32>) -> i32
      %231 = "affine.max"(%39, %95, %225) <{map = affine_map<(d0, d1, d2) -> (d0 - d1)>}> : (index, index, index) -> index
      "affine.yield"(%4) : (i16) -> ()
    }, {
      %224 = "affine.max"(%41, %19, %33, %41) <{map = affine_map<(d0, d1, d2)[s0] -> ((d0 mod 16) * 64)>}> : (index, index, index, index) -> index
      %225 = "affine.max"(%41, %19, %18) <{map = affine_map<(d0, d1, d2) -> (d2 - 80)>}> : (index, index, index) -> index
      %226 = "math.tanh"(%61) <{fastmath = #arith.fastmath<none>}> : (tensor<?xf16>) -> tensor<?xf16>
      %227 = "math.copysign"(%50, %50) <{fastmath = #arith.fastmath<none>}> : (tensor<32xf32>, tensor<32xf32>) -> tensor<32xf32>
      %228 = "vector.broadcast"(%88) : (f32) -> vector<32xf32>
      %229 = "vector.fma"(%228, %82, %81) : (vector<32xf32>, vector<32xf32>, vector<32xf32>) -> vector<32xf32>
      %230 = "vector.contract"(%82, %229, %1) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = [#vector.iterator_type<reduction>], kind = #vector.kind<mul>}> : (vector<32xf32>, vector<32xf32>, f32) -> f32
      %231 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %232 = "vector.broadcast"(%231) : (i64) -> vector<i64>
      %233 = "vector.transfer_write"(%232, %62, %38) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, permutation_map = affine_map<(d0) -> ()>}> : (vector<i64>, tensor<28xi64>, index) -> tensor<28xi64>
      %234 = "vector.extract_strided_slice"(%229) <{offsets = [13], sizes = [11], strides = [1]}> : (vector<32xf32>) -> vector<11xf32>
      "affine.yield"(%4) : (i16) -> ()
    }) {condition = affine_set<(d0, d1, d2) : (d0 + d2 + 32 >= 0, d0 - (d2 + 128) + d1 >= 0, -d1 >= 0)>} : (index, index, index) -> i16
    %109 = "arith.andi"(%13, %86) : (i32, i32) -> i32
    %110 = "memref.atomic_rmw"(%15, %78, %16, %20, %25) <{kind = 2 : i64}> : (i32, memref<11x28x11xi32>, index, index, index) -> i32
    %111 = "spirv.CL.pow"(%100, %7) : (f32, f32) -> f32
    %112 = "vector.extract_strided_slice"(%90) <{offsets = [19], sizes = [5], strides = [1]}> : (vector<28xi32>) -> vector<5xi32>
    %113 = "math.cttz"(%91) : (i1) -> i1
    %114 = "spirv.GL.Exp"(%7) : (f32) -> f32
    %115 = "memref.atomic_rmw"(%4, %77, %26, %30, %22) <{kind = 10 : i64}> : (i16, memref<11x28x11xi16>, index, index, index) -> i16
    %116 = "math.absi"(%86) : (i32) -> i32
    %117 = "spirv.INotEqual"(%102, %102) : (vector<2xi32>, vector<2xi32>) -> vector<2xi1>
    %118 = "arith.cmpf"(%88, %6) <{predicate = 0 : i64}> : (f32, f32) -> i1
    %119 = "vector.splat"(%38) : (index) -> vector<11xindex>
    %120 = "math.atan2"(%50, %50) <{fastmath = #arith.fastmath<none>}> : (tensor<32xf32>, tensor<32xf32>) -> tensor<32xf32>
    %121 = "scf.while"(%102) ({
    ^bb0(%arg3: vector<2xi32>):
      %224 = "arith.xori"(%9, %11) : (i32, i32) -> i32
      %225 = "bufferization.clone"(%77) : (memref<11x28x11xi16>) -> memref<11x28x11xi16>
      %226 = "vector.broadcast"(%88) : (f32) -> vector<11xf32>
      %227 = "vector.fma"(%226, %226, %226) : (vector<11xf32>, vector<11xf32>, vector<11xf32>) -> vector<11xf32>
      %228 = "math.absi"(%53) : (tensor<?x28x11xi1>) -> tensor<?x28x11xi1>
      %229 = "index.divs"(%22, %25) : (index, index) -> index
      %230 = "math.fma"(%111, %98, %1) <{fastmath = #arith.fastmath<none>}> : (f32, f32, f32) -> f32
      %231 = "arith.ceildivsi"(%91, %101) : (i1, i1) -> i1
      %232 = "memref.alloc"(%31) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xi1>
      "scf.condition"(%104, %102) : (i1, vector<2xi32>) -> ()
    }, {
    ^bb0(%arg3: vector<2xi32>):
      %224 = "vector.insertelement"(%105, %81, %44) : (f32, vector<32xf32>, index) -> vector<32xf32>
      %225 = "arith.remf"(%6, %93) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      %226 = "tensor.splat"(%1) : (f32) -> tensor<11x28x11xf32>
      %227 = "tensor.empty"() : () -> tensor<28xi64>
      %228 = "linalg.map"(%62, %227) ({
      ^bb0(%arg4: i64):
        %241 = "affine.min"(%40, %44) <{map = affine_map<(d0)[s0] -> (d0)>}> : (index, index) -> index
        %242 = "math.exp"(%58) <{fastmath = #arith.fastmath<none>}> : (tensor<28xf32>) -> tensor<28xf32>
        %243 = "vector.broadcast"(%101) : (i1) -> vector<28xi1>
        %244 = "vector.maskedload"(%72, %16, %243, %90) : (memref<?xi32>, index, vector<28xi1>, vector<28xi32>) -> vector<28xi32>
        %245 = "vector.matrix_multiply"(%102, %102) <{lhs_columns = 2 : i32, lhs_rows = 1 : i32, rhs_columns = 1 : i32}> : (vector<2xi32>, vector<2xi32>) -> vector<1xi32>
        %246 = "index.shrs"(%18, %95) : (index, index) -> index
        %247 = "index.shrs"(%43, %30) : (index, index) -> index
        %248 = "affine.load"(%68, %25, %21, %38) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> : (memref<11x28x11xi1>, index, index, index) -> i1
        %249 = "math.fma"(%55, %59, %59) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>, tensor<11xf32>, tensor<11xf32>) -> tensor<11xf32>
        %250 = "bufferization.clone"(%77) : (memref<11x28x11xi16>) -> memref<11x28x11xi16>
        %251 = "vector.multi_reduction"(%112, %5) <{kind = #vector.kind<minsi>, reduction_dims = [0]}> : (vector<5xi32>, i32) -> i32
        %252 = "vector.insertelement"(%93, %82, %32) : (f32, vector<32xf32>, index) -> vector<32xf32>
        %253 = "tensor.empty"() : () -> tensor<11xf16>
        %254 = "vector.broadcast"(%2) : (i1) -> vector<11x11xi1>
        %255 = "vector.mask"(%80) ({
          %278 = "vector.multi_reduction"(%80, %254) <{kind = #vector.kind<minui>, reduction_dims = [1]}> : (vector<11x28x11xi1>, vector<11x11xi1>) -> vector<11x11xi1>
          "vector.yield"(%278) : (vector<11x11xi1>) -> ()
        }) : (vector<11x28x11xi1>) -> vector<11x11xi1>
        %256 = "linalg.copy"(%53, %53) <{operandSegmentSizes = array<i32: 1, 1>}> ({
        ^bb0(%arg5: i1, %arg6: i1):
          "linalg.yield"(%arg5) : (i1) -> ()
        }) : (tensor<?x28x11xi1>, tensor<?x28x11xi1>) -> tensor<?x28x11xi1>
        %257 = "vector.broadcast"(%84) : (i16) -> vector<32xi16>
        %258 = "vector.broadcast"(%89) : (i1) -> vector<32xi1>
        %259 = "vector.maskedload"(%79, %16, %258, %257) : (memref<?xi16>, index, vector<32xi1>, vector<32xi16>) -> vector<32xi16>
        "memref.assume_alignment"(%68) <{alignment = 16 : i32}> : (memref<11x28x11xi1>) -> ()
        %260 = "index.ceildivu"(%26, %31) : (index, index) -> index
        %261 = "memref.alloc"(%35) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xi32>
        %262 = "arith.addi"(%5, %11) : (i32, i32) -> i32
        %263 = "memref.alloc"(%25) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<11x?x28xi64>
        "linalg.transpose"(%54, %263) <{permutation = array<i64: 2, 0, 1>}> ({
        ^bb0(%arg5: i64, %arg6: i64):
          "linalg.yield"(%arg5) : (i64) -> ()
        }) : (tensor<?x28x11xi64>, memref<11x?x28xi64>) -> ()
        %264 = "arith.remf"(%93, %1) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %265 = "index.and"(%17, %46) : (index, index) -> index
        %266 = "tensor.empty"() : () -> tensor<i64>
        %267 = "linalg.dot"(%227, %62, %266) <{operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg5: i64, %arg6: i64, %arg7: i64):
          %278 = "arith.muli"(%arg5, %arg6) : (i64, i64) -> i64
          %279 = "arith.addi"(%arg7, %278) : (i64, i64) -> i64
          "linalg.yield"(%279) : (i64) -> ()
        }) {linalg.memoized_indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>]} : (tensor<28xi64>, tensor<28xi64>, tensor<i64>) -> tensor<i64>
        %268 = "memref.load"(%75, %16) <{nontemporal = false}> : (memref<?xf16>, index) -> f16
        %269 = "arith.ceildivsi"(%15, %5) : (i32, i32) -> i32
        %270 = "index.and"(%19, %38) : (index, index) -> index
        %271 = "arith.divui"(%9, %15) : (i32, i32) -> i32
        %272 = "index.or"(%21, %31) : (index, index) -> index
        %273 = "arith.andi"(%13, %86) : (i32, i32) -> i32
        %274 = "vector.broadcast"(%100) : (f32) -> vector<28xf32>
        %275 = "vector.fma"(%274, %274, %274) : (vector<28xf32>, vector<28xf32>, vector<28xf32>) -> vector<28xf32>
        %276 = "bufferization.clone"(%73) : (memref<11x28x11xi32>) -> memref<11x28x11xi32>
        %277 = "arith.andi"(%3, %101) : (i1, i1) -> i1
        "linalg.yield"(%arg4) : (i64) -> ()
      }) : (tensor<28xi64>, tensor<28xi64>) -> tensor<28xi64>
      "vector.print"(%112) <{punctuation = #vector.punctuation<newline>}> : (vector<5xi32>) -> ()
      %229 = "vector.broadcast"(%33) : (index) -> vector<11xindex>
      %230 = "vector.broadcast"(%13) : (i32) -> vector<11xi32>
      "vector.scatter"(%73, %16, %19, %22, %229, %107, %230) : (memref<11x28x11xi32>, index, index, index, vector<11xindex>, vector<11xi1>, vector<11xi32>) -> ()
      %231 = "vector.extract"(%82) <{static_position = array<i64: 7>}> : (vector<32xf32>) -> f32
      %232 = "math.log10"(%93) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      %233 = "math.powf"(%58, %58) <{fastmath = #arith.fastmath<none>}> : (tensor<28xf32>, tensor<28xf32>) -> tensor<28xf32>
      %234 = "vector.extract_strided_slice"(%82) <{offsets = [22], sizes = [5], strides = [1]}> : (vector<32xf32>) -> vector<5xf32>
      %235 = "math.round"(%55) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>) -> tensor<11xf32>
      %236 = "math.expm1"(%98) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      %237 = "index.shl"(%27, %45) : (index, index) -> index
      %238 = "math.tanh"(%50) <{fastmath = #arith.fastmath<none>}> : (tensor<32xf32>) -> tensor<32xf32>
      %239 = "arith.remui"(%5, %9) : (i32, i32) -> i32
      %240 = "bufferization.clone"(%77) : (memref<11x28x11xi16>) -> memref<11x28x11xi16>
      "scf.yield"(%102) : (vector<2xi32>) -> ()
    }) : (vector<2xi32>) -> vector<2xi32>
    %122 = "arith.remf"(%100, %114) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %123 = "spirv.GL.UMax"(%84, %8) : (i16, i16) -> i16
    %124 = "spirv.GL.SClamp"(%13, %10, %9) : (i32, i32, i32) -> i32
    %125 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11x28x11xi64>
    "memref.tensor_store"(%56, %125) : (tensor<11x28x11xi64>, memref<11x28x11xi64>) -> ()
    %126 = "vector.flat_transpose"(%81) <{columns = 8 : i32, rows = 4 : i32}> : (vector<32xf32>) -> vector<32xf32>
    %127 = "spirv.CL.tanh"(%114) : (f32) -> f32
    %128 = "bufferization.clone"(%65) : (memref<28xf16>) -> memref<28xf16>
    %129 = "math.fma"(%6, %127, %7) <{fastmath = #arith.fastmath<none>}> : (f32, f32, f32) -> f32
    "memref.assume_alignment"(%66) <{alignment = 16 : i32}> : (memref<?xf32>) -> ()
    %130 = "spirv.CL.ceil"(%105) : (f32) -> f32
    %131 = "spirv.CL.floor"(%88) : (f32) -> f32
    %132 = "index.divs"(%arg1, %37) : (index, index) -> index
    %133 = "math.round"(%88) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %134 = "arith.remf"(%114, %96) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %135 = "spirv.CL.s_abs"(%123) : (i16) -> i16
    %136 = "bufferization.clone"(%73) : (memref<11x28x11xi32>) -> memref<11x28x11xi32>
    %137 = "spirv.SGreaterThan"(%102, %102) : (vector<2xi32>, vector<2xi32>) -> vector<2xi1>
    %138 = "vector.broadcast"(%100) : (f32) -> vector<32xf32>
    %139 = "vector.fma"(%138, %138, %82) : (vector<32xf32>, vector<32xf32>, vector<32xf32>) -> vector<32xf32>
    "vector.print"(%82) <{punctuation = #vector.punctuation<newline>}> : (vector<32xf32>) -> ()
    %140 = "math.exp"(%1) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    "affine.store"(%14, %75, %23) <{map = affine_map<(d0) -> (d0)>}> : (f16, memref<?xf16>, index) -> ()
    %141 = "arith.muli"(%135, %84) : (i16, i16) -> i16
    %142 = "spirv.GL.Fma"(%127, %0, %93) : (f32, f32, f32) -> f32
    %143 = "index.and"(%33, %16) : (index, index) -> index
    %144 = "spirv.GL.Cosh"(%114) : (f32) -> f32
    %145 = "linalg.copy"(%63, %60) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg3: i16, %arg4: i16):
      "linalg.yield"(%arg3) : (i16) -> ()
    }) : (tensor<11xi16>, tensor<11xi16>) -> tensor<11xi16>
    %146 = "spirv.ULessThan"(%15, %124) : (i32, i32) -> i1
    %147 = "arith.minsi"(%135, %135) : (i16, i16) -> i16
    %148 = "spirv.GL.UMax"(%9, %11) : (i32, i32) -> i32
    %149 = "vector.broadcast"(%1) : (f32) -> vector<11x28x11xf32>
    %150 = "vector.fma"(%149, %149, %149) : (vector<11x28x11xf32>, vector<11x28x11xf32>, vector<11x28x11xf32>) -> vector<11x28x11xf32>
    %151 = "vector.extract_strided_slice"(%126) <{offsets = [26], sizes = [5], strides = [1]}> : (vector<32xf32>) -> vector<5xf32>
    %152 = "math.roundeven"(%144) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %153 = "vector.multi_reduction"(%102, %10) <{kind = #vector.kind<minui>, reduction_dims = [0]}> : (vector<2xi32>, i32) -> i32
    %154 = "spirv.GL.Tanh"(%0) : (f32) -> f32
    %155 = "vector.load"(%76, %16, %16, %16) : (memref<?x?x?xi32>, index, index, index) -> vector<11xi32>
    %156 = "math.cttz"(%12) : (i1) -> i1
    %157 = "vector.broadcast"(%1) : (f32) -> vector<11xf32>
    %158 = "vector.mask"(%80) ({
      %224 = "vector.multi_reduction"(%149, %157) <{kind = #vector.kind<mul>, reduction_dims = [1, 2]}> : (vector<11x28x11xf32>, vector<11xf32>) -> vector<11xf32>
      "vector.yield"(%224) : (vector<11xf32>) -> ()
    }) : (vector<11x28x11xi1>) -> vector<11xf32>
    %159 = "tensor.extract"(%48, %16, %16, %19) : (tensor<?x28x11xf16>, index, index, index) -> f16
    %160 = "index.shru"(%30, %36) : (index, index) -> index
    %161 = "memref.alloca"(%27) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?x28x11xi16>
    %162 = "tensor.insert"(%159, %61, %16) : (f16, tensor<?xf16>, index) -> tensor<?xf16>
    %163 = "tensor.empty"() : () -> tensor<i16>
    %164 = "linalg.dot"(%60, %63, %163) <{operandSegmentSizes = array<i32: 2, 1>}> ({
    ^bb0(%arg3: i16, %arg4: i16, %arg5: i16):
      %224 = "arith.muli"(%arg3, %arg4) : (i16, i16) -> i16
      %225 = "arith.addi"(%arg5, %224) : (i16, i16) -> i16
      "linalg.yield"(%225) : (i16) -> ()
    }) {linalg.memoized_indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>]} : (tensor<11xi16>, tensor<11xi16>, tensor<i16>) -> tensor<i16>
    %165 = "math.ipowi"(%148, %13) : (i32, i32) -> i32
    %166 = "memref.alloca"(%41) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xi16>
    %167 = "spirv.FNegate"(%131) : (f32) -> f32
    %168 = "spirv.SLessThan"(%84, %135) : (i16, i16) -> i1
    %169 = "index.castu"(%45) : (index) -> i32
    %170 = "math.fma"(%55, %55, %55) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>, tensor<11xf32>, tensor<11xf32>) -> tensor<11xf32>
    %171 = "math.atan"(%6) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %172 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<12x11xf16>
    %173 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11x32xf16>
    %174 = "tensor.empty"() : () -> tensor<12x32xf16>
    %175 = "linalg.matmul"(%172, %173, %174) <{operandSegmentSizes = array<i32: 2, 1>}> ({
    ^bb0(%arg3: f16, %arg4: f16, %arg5: f16):
      %224 = "arith.mulf"(%arg3, %arg4) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
      %225 = "arith.addf"(%arg5, %224) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
      "linalg.yield"(%225) : (f16) -> ()
    }) {linalg.memoized_indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>]} : (memref<12x11xf16>, memref<11x32xf16>, tensor<12x32xf16>) -> tensor<12x32xf16>
    %176 = "spirv.GL.UMax"(%153, %124) : (i32, i32) -> i32
    %177 = "spirv.GL.UMax"(%123, %4) : (i16, i16) -> i16
    %178 = "vector.broadcast"(%38) : (index) -> vector<32xindex>
    %179 = "vector.broadcast"(%146) : (i1) -> vector<32xi1>
    %180 = "vector.broadcast"(%159) : (f16) -> vector<32xf16>
    "vector.scatter"(%75, %16, %178, %179, %180) : (memref<?xf16>, index, vector<32xindex>, vector<32xi1>, vector<32xf16>) -> ()
    %181 = "linalg.copy"(%49, %49) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg3: i32, %arg4: i32):
      "linalg.yield"(%arg3) : (i32) -> ()
    }) : (tensor<?xi32>, tensor<?xi32>) -> tensor<?xi32>
    %182 = "spirv.FOrdLessThanEqual"(%98, %105) : (f32, f32) -> i1
    %183 = "spirv.CL.erf"(%88) : (f32) -> f32
    %184 = "vector.broadcast"(%38) : (index) -> vector<12xindex>
    %185 = "vector.broadcast"(%146) : (i1) -> vector<12xi1>
    %186 = "vector.broadcast"(%14) : (f16) -> vector<12xf16>
    "vector.scatter"(%75, %16, %184, %185, %186) : (memref<?xf16>, index, vector<12xindex>, vector<12xi1>, vector<12xf16>) -> ()
    %187 = "affine.for"(%160) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (111)>}> ({
    ^bb0(%arg3: index, %arg4: index):
      "affine.yield"(%22) : (index) -> ()
    }) : (index) -> index
    %188 = "spirv.GL.Acos"(%6) : (f32) -> f32
    %189 = "spirv.CL.erf"(%183) : (f32) -> f32
    %190 = "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [11], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (12, 11, 28)>}> ({
    ^bb0(%arg3: index, %arg4: index, %arg5: index):
      %224 = "index.and"(%33, %45) : (index, index) -> index
      "affine.yield"(%8) : (i16) -> ()
    }) : () -> memref<12x11x28xi16>
    %191 = "vector.insertelement"(%188, %157, %19) : (f32, vector<11xf32>, index) -> vector<11xf32>
    %192 = "vector.broadcast"(%189) : (f32) -> vector<11x28xf32>
    %193:2 = "vector.scan"(%149, %192) <{inclusive = false, kind = #vector.kind<add>, reduction_dim = 2 : i64}> : (vector<11x28x11xf32>, vector<11x28xf32>) -> (vector<11x28x11xf32>, vector<11x28xf32>)
    %194 = "arith.remf"(%189, %144) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %195 = "math.ipowi"(%5, %176) : (i32, i32) -> i32
    %196 = "bufferization.clone"(%77) : (memref<11x28x11xi16>) -> memref<11x28x11xi16>
    %197 = "vector.broadcast"(%15) : (i32) -> vector<i32>
    "vector.transfer_write"(%197, %73, %37, %41, %38) <{operandSegmentSizes = array<i32: 1, 1, 3, 0>, permutation_map = affine_map<(d0, d1, d2) -> ()>}> : (vector<i32>, memref<11x28x11xi32>, index, index, index) -> ()
    %198 = "affine.load"(%79, %26) <{map = affine_map<(d0) -> (d0)>}> : (memref<?xi16>, index) -> i16
    %199 = "spirv.BitFieldSExtract"(%102, %84, %148) : (vector<2xi32>, i16, i32) -> vector<2xi32>
    %200 = "memref.cast"(%68) : (memref<11x28x11xi1>) -> memref<11x?x11xi1>
    %201 = "linalg.copy"(%51, %51) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg3: i64, %arg4: i64):
      "linalg.yield"(%arg3) : (i64) -> ()
    }) : (tensor<32xi64>, tensor<32xi64>) -> tensor<32xi64>
    %202 = "arith.minsi"(%12, %146) : (i1, i1) -> i1
    %203 = "tensor.extract"(%181, %16) : (tensor<?xi32>, index) -> i32
    %204 = "spirv.FUnordLessThan"(%98, %7) : (f32, f32) -> i1
    %205 = "scf.index_switch"(%95) <{cases = array<i64: 1>}> ({
      %224 = "affine.max"(%39, %27, %39, %45) <{map = affine_map<(d0, d1, d2)[s0] -> (d0 * 2)>}> : (index, index, index, index) -> index
      %225 = "arith.cmpf"(%88, %100) <{predicate = 5 : i64}> : (f32, f32) -> i1
      %226 = "linalg.copy"(%48, %48) <{operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg3: f16, %arg4: f16):
        "linalg.yield"(%arg3) : (f16) -> ()
      }) : (tensor<?x28x11xf16>, tensor<?x28x11xf16>) -> tensor<?x28x11xf16>
      %227 = "memref.realloc"(%74) : (memref<?xi1>) -> memref<32xi1>
      %228 = "math.expm1"(%55) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>) -> tensor<11xf32>
      %229 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xf32>
      %230 = "memref.realloc"(%128) : (memref<28xf16>) -> memref<32xf16>
      %231 = "scf.while"(%61) ({
      ^bb0(%arg3: tensor<?xf16>):
        %241 = "math.exp2"(%6) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %242 = "math.ctlz"(%86) : (i32) -> i32
        %243 = "arith.constant"() <{value = 1 : i64}> : () -> i64
        %244 = "vector.transfer_read"(%arg2, %36, %243) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, permutation_map = affine_map<(d0) -> ()>}> : (memref<?xi64>, index, i64) -> vector<i64>
        %245 = "bufferization.clone"(%71) : (memref<28xf32>) -> memref<28xf32>
        %246 = "tensor.cast"(%226) : (tensor<?x28x11xf16>) -> tensor<32x28x11xf16>
        %247 = "math.tan"(%144) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %248 = "arith.addf"(%1, %167) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %249 = "arith.addf"(%1, %96) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %250 = "tensor.empty"(%19) : (index) -> tensor<?xf16>
        "scf.condition"(%104, %250) : (i1, tensor<?xf16>) -> ()
      }, {
      ^bb0(%arg3: tensor<?xf16>):
        %241 = "vector.contract"(%138, %138, %1) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = [#vector.iterator_type<reduction>], kind = #vector.kind<add>}> : (vector<32xf32>, vector<32xf32>, f32) -> f32
        %242 = "math.round"(%96) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %243 = "arith.ceildivsi"(%153, %9) : (i32, i32) -> i32
        %244 = "math.exp2"(%14) <{fastmath = #arith.fastmath<none>}> : (f16) -> f16
        %245 = "memref.load"(%67, %16) <{nontemporal = false}> : (memref<?xf32>, index) -> f32
        %246 = "vector.broadcast"(%15) : (i32) -> vector<32xi32>
        %247 = "math.roundeven"(%142) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %248 = "math.tan"(%6) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %249 = "vector.broadcast"(%6) : (f32) -> vector<11x28xf32>
        %250:2 = "vector.scan"(%150, %249) <{inclusive = false, kind = #vector.kind<minf>, reduction_dim = 2 : i64}> : (vector<11x28x11xf32>, vector<11x28xf32>) -> (vector<11x28x11xf32>, vector<11x28xf32>)
        %251 = "arith.divui"(%15, %11) : (i32, i32) -> i32
        %252 = "affine.load"(%68, %24, %28, %20) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> : (memref<11x28x11xi1>, index, index, index) -> i1
        %253 = "index.sub"(%224, %36) : (index, index) -> index
        %254 = "vector.broadcast"(%89) : (i1) -> vector<32xi1>
        %255 = "vector.gather"(%58, %46, %246, %254, %138) : (tensor<28xf32>, index, vector<32xi32>, vector<32xi1>, vector<32xf32>) -> vector<32xf32>
        %256 = "arith.addi"(%91, %168) : (i1, i1) -> i1
        %257 = "math.expm1"(%50) <{fastmath = #arith.fastmath<none>}> : (tensor<32xf32>) -> tensor<32xf32>
        %258 = "index.maxs"(%23, %224) : (index, index) -> index
        %259 = "tensor.empty"(%32) : (index) -> tensor<?xf16>
        "scf.yield"(%259) : (tensor<?xf16>) -> ()
      }) : (tensor<?xf16>) -> tensor<?xf16>
      %232 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xf32>
      "linalg.transpose"(%50, %232) <{permutation = array<i64: 0>}> ({
      ^bb0(%arg3: f32, %arg4: f32):
        "linalg.yield"(%arg3) : (f32) -> ()
      }) : (tensor<32xf32>, memref<32xf32>) -> ()
      %233 = "math.fma"(%59, %59, %55) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>, tensor<11xf32>, tensor<11xf32>) -> tensor<11xf32>
      %234 = "vector.broadcast"(%114) : (f32) -> vector<11xf32>
      %235 = "vector.fma"(%234, %157, %234) : (vector<11xf32>, vector<11xf32>, vector<11xf32>) -> vector<11xf32>
      %236 = "arith.remf"(%183, %130) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
      %237 = "vector.transfer_read"(%50, %23, %1) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, permutation_map = affine_map<(d0) -> ()>}> : (tensor<32xf32>, index, f32) -> vector<f32>
      %238 = "memref.atomic_rmw"(%14, %75, %16) <{kind = 9 : i64}> : (f16, memref<?xf16>, index) -> f16
      %239 = "math.ceil"(%98) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      %240 = "math.expm1"(%167) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      "scf.yield"(%50) : (tensor<32xf32>) -> ()
    }, {
      "memref.assume_alignment"(%64) <{alignment = 16 : i32}> : (memref<?x28x11xi64>) -> ()
      %224 = "math.cttz"(%89) : (i1) -> i1
      %225 = "vector.extract"(%138) <{static_position = array<i64: 10>}> : (vector<32xf32>) -> f32
      %226 = "arith.remf"(%159, %159) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
      %227 = "math.exp2"(%131) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      %228 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %229 = "tensor.from_elements"(%228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228, %228) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> tensor<11x28x11xi64>
      %230 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<28xi64>
      %231 = "tensor.empty"() : () -> tensor<i64>
      %232 = "linalg.dot"(%62, %230, %231) <{operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg3: i64, %arg4: i64, %arg5: i64):
        %244 = "arith.muli"(%arg3, %arg4) : (i64, i64) -> i64
        %245 = "arith.addi"(%arg5, %244) : (i64, i64) -> i64
        "linalg.yield"(%245) : (i64) -> ()
      }) : (tensor<28xi64>, memref<28xi64>, tensor<i64>) -> tensor<i64>
      %233 = "arith.addi"(%204, %12) : (i1, i1) -> i1
      %234 = "vector.transpose"(%80) <{transp = [0, 2, 1]}> : (vector<11x28x11xi1>) -> vector<11x11x28xi1>
      %235 = "math.round"(%114) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      %236 = "vector.broadcast"(%228) : (i64) -> vector<i64>
      %237 = "vector.transfer_write"(%236, %62, %36) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, permutation_map = affine_map<(d0) -> ()>}> : (vector<i64>, tensor<28xi64>, index) -> tensor<28xi64>
      %238 = "affine.load"(%71, %31) <{map = affine_map<(d0) -> (d0)>}> : (memref<28xf32>, index) -> f32
      %239 = "math.ctlz"(%146) : (i1) -> i1
      %240 = "scf.execute_region"() ({
        %244 = "math.roundeven"(%105) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %245 = "vector.broadcast"(%39) : (index) -> vector<32xindex>
        %246 = "vector.broadcast"(%2) : (i1) -> vector<32xi1>
        %247 = "vector.broadcast"(%10) : (i32) -> vector<32xi32>
        "vector.scatter"(%73, %16, %21, %24, %245, %246, %247) : (memref<11x28x11xi32>, index, index, index, vector<32xindex>, vector<32xi1>, vector<32xi32>) -> ()
        %248 = "math.ctlz"(%91) : (i1) -> i1
        %249 = "vector.broadcast"(%154) : (f32) -> vector<11x28x11xf32>
        %250 = "vector.fma"(%249, %149, %150) : (vector<11x28x11xf32>, vector<11x28x11xf32>, vector<11x28x11xf32>) -> vector<11x28x11xf32>
        %251 = "affine.apply"(%160, %95, %43, %37) <{map = affine_map<(d0, d1, d2, d3) -> ((d3 - 64) ceildiv 128)>}> : (index, index, index, index) -> index
        %252 = "arith.muli"(%182, %168) : (i1, i1) -> i1
        %253 = "index.casts"(%35) : (index) -> i32
        %254 = "index.or"(%36, %32) : (index, index) -> index
        %255 = "bufferization.to_memref"(%57) : (tensor<?xi1>) -> memref<?xi1>
        %256 = "arith.andi"(%135, %84) : (i16, i16) -> i16
        %257 = "math.round"(%14) <{fastmath = #arith.fastmath<none>}> : (f16) -> f16
        %258 = "index.sizeof"() : () -> index
        %259 = "affine.load"(%230, %38) <{map = affine_map<(d0) -> (d0)>}> : (memref<28xi64>, index) -> i64
        %260 = "arith.muli"(%124, %15) : (i32, i32) -> i32
        %261 = "math.ipowi"(%231, %232) : (tensor<i64>, tensor<i64>) -> tensor<i64>
        %262 = "arith.addi"(%146, %2) : (i1, i1) -> i1
        %263 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11xi16>
        "scf.yield"(%263) : (memref<11xi16>) -> ()
      }) : () -> memref<11xi16>
      %241 = "index.and"(%24, %35) : (index, index) -> index
      %242 = "tensor.empty"(%30) : (index) -> tensor<?xi32>
      %243 = "linalg.map"(%181, %72, %242) ({
      ^bb0(%arg3: i32, %arg4: i32):
        %244 = "tensor.empty"() : () -> tensor<32x28xf16>
        %245 = "tensor.empty"() : () -> tensor<12x28xf16>
        %246 = "linalg.matmul"(%174, %244, %245) <{operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg5: f16, %arg6: f16, %arg7: f16):
          %280 = "arith.mulf"(%arg5, %arg6) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
          %281 = "arith.addf"(%arg7, %280) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
          "linalg.yield"(%281) : (f16) -> ()
        }) : (tensor<12x32xf16>, tensor<32x28xf16>, tensor<12x28xf16>) -> tensor<12x28xf16>
        %247 = "memref.load"(%128, %26) <{nontemporal = false}> : (memref<28xf16>, index) -> f16
        %248 = "vector.mask"(%107) ({
          %280 = "vector.multi_reduction"(%157, %157) <{kind = #vector.kind<minf>, reduction_dims = []}> : (vector<11xf32>, vector<11xf32>) -> vector<11xf32>
          "vector.yield"(%280) : (vector<11xf32>) -> ()
        }) : (vector<11xi1>) -> vector<11xf32>
        %249 = "tensor.dim"(%50, %16) : (tensor<32xf32>, index) -> index
        %250 = "memref.alloca"(%39) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?x28x11xi1>
        %251 = "index.divu"(%36, %26) : (index, index) -> index
        %252 = "math.ceil"(%6) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %253 = "tensor.cast"(%61) : (tensor<?xf16>) -> tensor<11xf16>
        %254 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11x28x11xf16>
        %255 = "math.cos"(%55) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>) -> tensor<11xf32>
        %256 = "arith.remui"(%168, %89) : (i1, i1) -> i1
        %257 = "arith.subi"(%153, %9) : (i32, i32) -> i32
        %258 = "vector.extract_strided_slice"(%149) <{offsets = [5], sizes = [3], strides = [1]}> : (vector<11x28x11xf32>) -> vector<3x28x11xf32>
        %259 = "vector.splat"(%34) : (index) -> vector<28xindex>
        %260 = "arith.remui"(%89, %104) : (i1, i1) -> i1
        "memref.assume_alignment"(%136) <{alignment = 1 : i32}> : (memref<11x28x11xi32>) -> ()
        %261 = "arith.addi"(%104, %3) : (i1, i1) -> i1
        %262 = "index.sizeof"() : () -> index
        %263 = "tensor.empty"() : () -> tensor<11x28x11xi1>
        %264 = "vector.broadcast"(%104) : (i1) -> vector<28xi1>
        %265 = "vector.gather"(%263, %18, %23, %41, %90, %264, %264) : (tensor<11x28x11xi1>, index, index, index, vector<28xi32>, vector<28xi1>, vector<28xi1>) -> vector<28xi1>
        %266 = "arith.remf"(%105, %88) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %267 = "arith.remui"(%182, %2) : (i1, i1) -> i1
        %268 = "vector.broadcast"(%3) : (i1) -> vector<32xi1>
        %269 = "vector.mask"(%268) ({
          %280 = "vector.multi_reduction"(%81, %139) <{kind = #vector.kind<maxf>, reduction_dims = []}> : (vector<32xf32>, vector<32xf32>) -> vector<32xf32>
          "vector.yield"(%280) : (vector<32xf32>) -> ()
        }) : (vector<32xi1>) -> vector<32xf32>
        %270 = "arith.subi"(%153, %10) : (i32, i32) -> i32
        %271 = "arith.shrui"(%5, %arg3) : (i32, i32) -> i32
        %272 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32x28xf16>
        %273 = "linalg.matmul"(%174, %272, %245) <{operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg5: f16, %arg6: f16, %arg7: f16):
          %280 = "arith.mulf"(%arg5, %arg6) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
          %281 = "arith.addf"(%arg7, %280) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
          "linalg.yield"(%281) : (f16) -> ()
        }) : (tensor<12x32xf16>, memref<32x28xf16>, tensor<12x28xf16>) -> tensor<12x28xf16>
        %274 = "arith.remui"(%135, %198) : (i16, i16) -> i16
        %275 = "arith.shrui"(%153, %203) : (i32, i32) -> i32
        %276 = "vector.transpose"(%157) <{transp = [0]}> : (vector<11xf32>) -> vector<11xf32>
        %277 = "vector.transfer_read"(%70, %19, %23, %43, %96) <{operandSegmentSizes = array<i32: 1, 3, 1, 0>, permutation_map = affine_map<(d0, d1, d2) -> (d1, d2)>}> : (memref<?x28x11xf32>, index, index, index, f32) -> vector<32x32xf32>
        %278 = "vector.mask"(%268) ({
          %280 = "vector.multi_reduction"(%126, %126) <{kind = #vector.kind<mul>, reduction_dims = []}> : (vector<32xf32>, vector<32xf32>) -> vector<32xf32>
          "vector.yield"(%280) : (vector<32xf32>) -> ()
        }) : (vector<32xi1>) -> vector<32xf32>
        %279 = "vector.mask"(%264) ({
          %280 = "vector.multi_reduction"(%264, %265) <{kind = #vector.kind<or>, reduction_dims = []}> : (vector<28xi1>, vector<28xi1>) -> vector<28xi1>
          "vector.yield"(%280) : (vector<28xi1>) -> ()
        }) : (vector<28xi1>) -> vector<28xi1>
        "affine.store"(%123, %240, %28) <{map = affine_map<(d0) -> (d0)>}> : (i16, memref<11xi16>, index) -> ()
        "linalg.yield"(%13) : (i32) -> ()
      }) : (tensor<?xi32>, memref<?xi32>, tensor<?xi32>) -> tensor<?xi32>
      "scf.yield"(%50) : (tensor<32xf32>) -> ()
    }) : (index) -> tensor<32xf32>
    %206 = "spirv.GL.Acos"(%111) : (f32) -> f32
    "scf.index_switch"(%42) <{cases = array<i64: 1>}> ({
      %224 = "math.exp2"(%100) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      "memref.copy"(%72, %72) : (memref<?xi32>, memref<?xi32>) -> ()
      %225 = "bufferization.to_tensor"(%66) : (memref<?xf32>) -> tensor<?xf32>
      "linalg.transpose"(%76, %76) <{permutation = array<i64: 2, 0, 1>}> ({
      ^bb0(%arg3: i32, %arg4: i32):
        "linalg.yield"(%arg3) : (i32) -> ()
      }) : (memref<?x?x?xi32>, memref<?x?x?xi32>) -> ()
      %226 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11xi64>
      %227 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %228 = "vector.broadcast"(%227) : (i64) -> vector<11xi64>
      %229 = "vector.gather"(%226, %39, %155, %107, %228) : (memref<11xi64>, index, vector<11xi32>, vector<11xi1>, vector<11xi64>) -> vector<11xi64>
      %230 = "arith.muli"(%146, %3) : (i1, i1) -> i1
      %231 = "math.tanh"(%88) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      %232 = "math.exp"(%142) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      %233 = "index.sub"(%31, %36) : (index, index) -> index
      %234 = "bufferization.clone"(%73) : (memref<11x28x11xi32>) -> memref<11x28x11xi32>
      %235 = "linalg.copy"(%48, %48) <{operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg3: f16, %arg4: f16):
        "linalg.yield"(%arg3) : (f16) -> ()
      }) : (tensor<?x28x11xf16>, tensor<?x28x11xf16>) -> tensor<?x28x11xf16>
      %236 = "math.fpowi"(%114, %13) <{fastmath = #arith.fastmath<none>}> : (f32, i32) -> f32
      %237 = "math.powf"(%55, %59) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>, tensor<11xf32>) -> tensor<11xf32>
      %238 = "bufferization.clone"(%78) : (memref<11x28x11xi32>) -> memref<11x28x11xi32>
      %239 = "vector.broadcast"(%188) : (f32) -> vector<11x28x11xf32>
      %240 = "vector.fma"(%239, %150, %239) : (vector<11x28x11xf32>, vector<11x28x11xf32>, vector<11x28x11xf32>) -> vector<11x28x11xf32>
      %241 = "arith.divui"(%2, %168) : (i1, i1) -> i1
      "scf.yield"() : () -> ()
    }, {
      %224 = "index.and"(%47, %38) : (index, index) -> index
      %225 = "tensor.splat"(%15) : (i32) -> tensor<11xi32>
      %226 = "vector.multi_reduction"(%80, %107) <{kind = #vector.kind<and>, reduction_dims = [0, 1]}> : (vector<11x28x11xi1>, vector<11xi1>) -> vector<11xi1>
      %227 = "memref.alloc"(%34, %41, %38) <{operandSegmentSizes = array<i32: 3, 0>}> : (index, index, index) -> memref<?x?x?xi1>
      %228 = "index.sub"(%30, %40) : (index, index) -> index
      %229 = "vector.splat"(%2) : (i1) -> vector<11xi1>
      %230 = "tensor.splat"(%167) : (f32) -> tensor<11xf32>
      %231 = "arith.andi"(%204, %204) : (i1, i1) -> i1
      %232 = "index.shrs"(%arg1, %132) : (index, index) -> index
      %233 = "math.fma"(%55, %59, %59) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>, tensor<11xf32>, tensor<11xf32>) -> tensor<11xf32>
      "vector.print"(%197) <{punctuation = #vector.punctuation<newline>}> : (vector<i32>) -> ()
      %234 = "math.roundeven"(%0) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      %235 = "affine.vector_load"(%78, %31, %21, %43) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> : (memref<11x28x11xi32>, index, index, index) -> vector<32xi32>
      %236 = "bufferization.to_memref"(%55) : (tensor<11xf32>) -> memref<11xf32>
      %237 = "vector.transpose"(%149) <{transp = [1, 2, 0]}> : (vector<11x28x11xf32>) -> vector<28x11x11xf32>
      %238 = "vector.broadcast"(%146) : (i1) -> vector<5xi1>
      "vector.compressstore"(%73, %20, %36, %26, %238, %112) : (memref<11x28x11xi32>, index, index, index, vector<5xi1>, vector<5xi32>) -> ()
      "scf.yield"() : () -> ()
    }) : (index) -> ()
    %207 = "spirv.GL.SClamp"(%123, %198, %4) : (i16, i16, i16) -> i16
    %208 = "vector.broadcast"(%182) : (i1) -> vector<28x11xi1>
    %209:2 = "vector.scan"(%80, %208) <{inclusive = true, kind = #vector.kind<maxui>, reduction_dim = 0 : i64}> : (vector<11x28x11xi1>, vector<28x11xi1>) -> (vector<11x28x11xi1>, vector<28x11xi1>)
    %210 = "index.castu"(%17) : (index) -> i32
    %211 = "vector.broadcast"(%33) : (index) -> vector<32xindex>
    %212 = "vector.broadcast"(%3) : (i1) -> vector<32xi1>
    %213 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %214 = "vector.broadcast"(%213) : (i64) -> vector<32xi64>
    "vector.scatter"(%64, %16, %34, %16, %211, %212, %214) : (memref<?x28x11xi64>, index, index, index, vector<32xindex>, vector<32xi1>, vector<32xi64>) -> ()
    %215 = "spirv.LogicalAnd"(%182, %168) : (i1, i1) -> i1
    %216 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32xf32>
    "memref.tensor_store"(%50, %216) : (tensor<32xf32>, memref<32xf32>) -> ()
    %217 = "vector.broadcast"(%0) : (f32) -> vector<f32>
    %218 = "vector.transfer_write"(%217, %50, %47) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, permutation_map = affine_map<(d0) -> ()>}> : (vector<f32>, tensor<32xf32>, index) -> tensor<32xf32>
    %219 = "tensor.empty"() : () -> tensor<11xi16>
    %220 = "linalg.map"(%145, %63, %219) ({
    ^bb0(%arg3: i16, %arg4: i16):
      %224 = "arith.cmpf"(%98, %111) <{predicate = 0 : i64}> : (f32, f32) -> i1
      %225 = "vector.broadcast"(%98) : (f32) -> vector<32xf32>
      %226 = "vector.fma"(%225, %82, %138) : (vector<32xf32>, vector<32xf32>, vector<32xf32>) -> vector<32xf32>
      %227 = "affine.load"(%77, %30, %44, %35) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> : (memref<11x28x11xi16>, index, index, index) -> i16
      "memref.alloca_scope"() ({
        %256 = "affine.load"(%76, %21, %26, %32) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> : (memref<?x?x?xi32>, index, index, index) -> i32
        %257 = "affine.vector_load"(%69, %40) <{map = affine_map<(d0) -> (d0)>}> : (memref<11xf16>, index) -> vector<32xf16>
        %258 = "arith.remf"(%142, %130) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %259 = "math.exp"(%167) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %260 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<11x32xf32>
        "linalg.broadcast"(%55, %260) <{dimensions = array<i64: 1>}> ({
        ^bb0(%arg5: f32, %arg6: f32):
          "linalg.yield"(%arg5) : (f32) -> ()
        }) : (tensor<11xf32>, memref<11x32xf32>) -> ()
        %261 = "arith.constant"() <{value = 1 : i64}> : () -> i64
        %262 = "vector.broadcast"(%261) : (i64) -> vector<32xi64>
        %263 = "vector.transfer_write"(%262, %56, %143, %22, %17) <{operandSegmentSizes = array<i32: 1, 1, 3, 0>, permutation_map = affine_map<(d0, d1, d2) -> (d0)>}> : (vector<32xi64>, tensor<11x28x11xi64>, index, index, index) -> tensor<11x28x11xi64>
        %264 = "vector.transfer_read"(%60, %42, %123) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, permutation_map = affine_map<(d0) -> ()>}> : (tensor<11xi16>, index, i16) -> vector<i16>
        %265 = "math.fpowi"(%93, %15) <{fastmath = #arith.fastmath<none>}> : (f32, i32) -> f32
        %266 = "vector.reduction"(%82) <{fastmath = #arith.fastmath<none>, kind = #vector.kind<minf>}> : (vector<32xf32>) -> f32
        %267 = "affine.load"(%64, %31, %30, %47) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> : (memref<?x28x11xi64>, index, index, index) -> i64
        %268 = "math.round"(%100) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %269 = "math.rsqrt"(%58) <{fastmath = #arith.fastmath<none>}> : (tensor<28xf32>) -> tensor<28xf32>
        %270 = "index.or"(%41, %26) : (index, index) -> index
        %271 = "math.cttz"(%181) : (tensor<?xi32>) -> tensor<?xi32>
        "memref.store"(%142, %260, %19, %43) <{nontemporal = false}> : (f32, memref<11x32xf32>, index, index) -> ()
        "memref.copy"(%77, %77) : (memref<11x28x11xi16>, memref<11x28x11xi16>) -> ()
        %272 = "memref.load"(%190, %19, %22, %40) <{nontemporal = false}> : (memref<12x11x28xi16>, index, index, index) -> i16
        %273 = "tensor.unpack"(%164, %164) <{inner_dims_pos = array<i64>, static_inner_tiles = array<i64>}> : (tensor<i16>, tensor<i16>) -> tensor<i16>
        %274 = "arith.negf"(%0) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %275 = "bufferization.clone"(%196) : (memref<11x28x11xi16>) -> memref<11x28x11xi16>
        %276 = "vector.broadcast"(%104) : (i1) -> vector<32xi1>
        %277 = "vector.mask"(%276) ({
          %288 = "vector.multi_reduction"(%138, %82) <{kind = #vector.kind<maxf>, reduction_dims = []}> : (vector<32xf32>, vector<32xf32>) -> vector<32xf32>
          "vector.yield"(%288) : (vector<32xf32>) -> ()
        }) : (vector<32xi1>) -> vector<32xf32>
        %278 = "math.roundeven"(%48) <{fastmath = #arith.fastmath<none>}> : (tensor<?x28x11xf16>) -> tensor<?x28x11xf16>
        %279 = "tensor.from_elements"(%159, %159, %159, %159, %159, %159, %14, %159, %14, %159, %14, %14, %14, %159, %159, %14, %159, %159, %159, %14, %14, %159, %159, %14, %14, %159, %14, %14) : (f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16, f16) -> tensor<28xf16>
        %280 = "arith.xori"(%2, %101) : (i1, i1) -> i1
        %281 = "arith.remsi"(%215, %182) : (i1, i1) -> i1
        %282 = "arith.remf"(%114, %130) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %283 = "vector.reduction"(%262) <{fastmath = #arith.fastmath<none>, kind = #vector.kind<minsi>}> : (vector<32xi64>) -> i64
        "affine.store"(%177, %77, %31, %31, %42) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> : (i16, memref<11x28x11xi16>, index, index, index) -> ()
        %284 = "math.atan2"(%14, %14) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
        %285 = "arith.divui"(%153, %256) : (i32, i32) -> i32
        %286 = "math.round"(%0) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %287 = "bufferization.clone"(%77) : (memref<11x28x11xi16>) -> memref<11x28x11xi16>
        "memref.alloca_scope.return"() : () -> ()
      }) : () -> ()
      "memref.assume_alignment"(%66) <{alignment = 1 : i32}> : (memref<?xf32>) -> ()
      %228 = "affine.if"(%21, %47, %29) ({
        %256 = "bufferization.clone"(%196) : (memref<11x28x11xi16>) -> memref<11x28x11xi16>
        %257 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<28xi64>
        "memref.tensor_store"(%62, %257) : (tensor<28xi64>, memref<28xi64>) -> ()
        %258 = "tensor.empty"() : () -> tensor<28xi1>
        %259 = "vector.broadcast"(%15) : (i32) -> vector<11x28x11xi32>
        %260 = "vector.gather"(%258, %35, %259, %80, %80) : (tensor<28xi1>, index, vector<11x28x11xi32>, vector<11x28x11xi1>, vector<11x28x11xi1>) -> vector<11x28x11xi1>
        %261 = "math.roundeven"(%6) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %262 = "index.shru"(%27, %36) : (index, index) -> index
        %263 = "arith.mulf"(%127, %7) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %264 = "vector.insertelement"(%144, %225, %27) : (f32, vector<32xf32>, index) -> vector<32xf32>
        %265 = "arith.remsi"(%168, %89) : (i1, i1) -> i1
        %266 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<28xi64>
        "affine.yield"(%266) : (memref<28xi64>) -> ()
      }, {
        %256 = "math.exp"(%127) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %257 = "math.copysign"(%55, %59) <{fastmath = #arith.fastmath<none>}> : (tensor<11xf32>, tensor<11xf32>) -> tensor<11xf32>
        %258 = "index.maxs"(%45, %41) : (index, index) -> index
        %259 = "math.ceil"(%1) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %260 = "arith.remf"(%154, %100) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %261 = "arith.muli"(%207, %8) : (i16, i16) -> i16
        %262 = "index.and"(%160, %44) : (index, index) -> index
        %263 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
        %264 = "vector.transfer_read"(%55, %27, %263) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, permutation_map = affine_map<(d0) -> ()>}> : (tensor<11xf32>, index, f32) -> vector<f32>
        %265 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<28xi64>
        "affine.yield"(%265) : (memref<28xi64>) -> ()
      }) {condition = affine_set<(d0, d1, d2) : (d0 + d2 + 32 >= 0, d0 - (d2 + 128) + d1 >= 0, -d1 >= 0)>} : (index, index, index) -> memref<28xi64>
      %229 = "vector.broadcast"(%144) : (f32) -> vector<11x28x11xf32>
      %230 = "math.log1p"(%88) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      "scf.execute_region"() ({
        %256 = "affine.load"(%64, %28, %28, %17) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> : (memref<?x28x11xi64>, index, index, index) -> i64
        %257 = "arith.addi"(%2, %12) : (i1, i1) -> i1
        %258 = "affine.load"(%76, %33, %28, %47) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> : (memref<?x?x?xi32>, index, index, index) -> i32
        %259 = "arith.divui"(%84, %4) : (i16, i16) -> i16
        %260 = "vector.transpose"(%151) <{transp = [0]}> : (vector<5xf32>) -> vector<5xf32>
        %261 = "vector.flat_transpose"(%81) <{columns = 8 : i32, rows = 4 : i32}> : (vector<32xf32>) -> vector<32xf32>
        %262 = "math.atan2"(%98, %0) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %263 = "vector.broadcast"(%17) : (index) -> vector<28xindex>
        %264 = "vector.broadcast"(%182) : (i1) -> vector<28xi1>
        %265 = "vector.broadcast"(%177) : (i16) -> vector<28xi16>
        "vector.scatter"(%77, %26, %38, %16, %263, %264, %265) : (memref<11x28x11xi16>, index, index, index, vector<28xindex>, vector<28xi1>, vector<28xi16>) -> ()
        %266 = "vector.broadcast"(%6) : (f32) -> vector<32xf32>
        %267 = "vector.fma"(%266, %225, %226) : (vector<32xf32>, vector<32xf32>, vector<32xf32>) -> vector<32xf32>
        %268 = "arith.muli"(%182, %104) : (i1, i1) -> i1
        "memref.store"(%13, %73, %21, %24, %16) <{nontemporal = false}> : (i32, memref<11x28x11xi32>, index, index, index) -> ()
        %269 = "index.shrs"(%47, %24) : (index, index) -> index
        %270 = "index.sub"(%132, %160) : (index, index) -> index
        %271 = "tensor.from_elements"(%256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256, %256) : (i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> tensor<11x28x11xi64>
        %272 = "math.fma"(%96, %6, %131) <{fastmath = #arith.fastmath<none>}> : (f32, f32, f32) -> f32
        %273 = "vector.broadcast"(%14) : (f16) -> vector<11xf16>
        %274 = "vector.transfer_write"(%273, %48, %38, %27, %41) <{operandSegmentSizes = array<i32: 1, 1, 3, 0>, permutation_map = affine_map<(d0, d1, d2) -> (d0)>}> : (vector<11xf16>, tensor<?x28x11xf16>, index, index, index) -> tensor<?x28x11xf16>
        "scf.yield"() : () -> ()
      }) : () -> ()
      %231 = "index.shrs"(%23, %29) : (index, index) -> index
      %232 = "arith.divui"(%146, %89) : (i1, i1) -> i1
      %233 = "index.and"(%143, %18) : (index, index) -> index
      %234 = "bufferization.to_memref"(%51) : (tensor<32xi64>) -> memref<32xi64>
      %235 = "math.ctlz"(%2) : (i1) -> i1
      %236 = "tensor.splat"(%89) : (i1) -> tensor<28xi1>
      %237 = "index.sizeof"() : () -> index
      %238 = "index.xor"(%231, %24) : (index, index) -> index
      %239 = "bufferization.clone"(%196) : (memref<11x28x11xi16>) -> memref<11x28x11xi16>
      %240 = "affine.if"(%28, %27, %32) ({
        %256 = "math.exp"(%58) <{fastmath = #arith.fastmath<none>}> : (tensor<28xf32>) -> tensor<28xf32>
        %257 = "tensor.empty"() : () -> tensor<i64>
        %258 = "linalg.dot"(%51, %51, %257) <{operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg5: i64, %arg6: i64, %arg7: i64):
          %265 = "arith.muli"(%arg5, %arg6) : (i64, i64) -> i64
          %266 = "arith.addi"(%arg7, %265) : (i64, i64) -> i64
          "linalg.yield"(%266) : (i64) -> ()
        }) : (tensor<32xi64>, tensor<32xi64>, tensor<i64>) -> tensor<i64>
        %259 = "index.sub"(%143, %143) : (index, index) -> index
        %260 = "math.exp"(%6) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %261 = "linalg.copy"(%61, %61) <{operandSegmentSizes = array<i32: 1, 1>}> ({
        ^bb0(%arg5: f16, %arg6: f16):
          "linalg.yield"(%arg5) : (f16) -> ()
        }) : (tensor<?xf16>, tensor<?xf16>) -> tensor<?xf16>
        %262 = "math.roundeven"(%50) <{fastmath = #arith.fastmath<none>}> : (tensor<32xf32>) -> tensor<32xf32>
        %263 = "math.ctlz"(%91) : (i1) -> i1
        %264 = "tensor.empty"() : () -> tensor<28xi1>
        "affine.yield"(%130) : (f32) -> ()
      }, {
        %256 = "index.divu"(%238, %31) : (index, index) -> index
        %257 = "vector.flat_transpose"(%139) <{columns = 8 : i32, rows = 4 : i32}> : (vector<32xf32>) -> vector<32xf32>
        %258 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32x28xf16>
        %259 = "tensor.empty"() : () -> tensor<12x28xf16>
        %260 = "linalg.matmul"(%174, %258, %259) <{operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg5: f16, %arg6: f16, %arg7: f16):
          %267 = "arith.mulf"(%arg5, %arg6) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
          %268 = "arith.addf"(%arg7, %267) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
          "linalg.yield"(%268) : (f16) -> ()
        }) : (tensor<12x32xf16>, memref<32x28xf16>, tensor<12x28xf16>) -> tensor<12x28xf16>
        %261 = "vector.broadcast"(%168) : (i1) -> vector<32xi1>
        "vector.compressstore"(%66, %16, %261, %138) : (memref<?xf32>, index, vector<32xi1>, vector<32xf32>) -> ()
        %262 = "vector.multi_reduction"(%157, %142) <{kind = #vector.kind<maxf>, reduction_dims = [0]}> : (vector<11xf32>, f32) -> f32
        %263 = "vector.broadcast"(%111) : (f32) -> vector<28xf32>
        %264 = "vector.fma"(%263, %263, %263) : (vector<28xf32>, vector<28xf32>, vector<28xf32>) -> vector<28xf32>
        %265 = "vector.mask"(%107) ({
          %267 = "vector.multi_reduction"(%157, %157) <{kind = #vector.kind<maxf>, reduction_dims = []}> : (vector<11xf32>, vector<11xf32>) -> vector<11xf32>
          "vector.yield"(%267) : (vector<11xf32>) -> ()
        }) : (vector<11xi1>) -> vector<11xf32>
        %266 = "arith.subi"(%101, %89) : (i1, i1) -> i1
        "affine.yield"(%130) : (f32) -> ()
      }) {condition = affine_set<(d0, d1, d2) : (d2 mod 4 + d1 == 0, -(d0 floordiv 16) == 0, d2 mod 4 >= 0, d2 mod 4 + 16 == 0)>} : (index, index, index) -> f32
      %241 = "vector.splat"(%131) : (f32) -> vector<32xf32>
      %242 = "tensor.empty"(%46) : (index) -> tensor<?xi1>
      %243 = "linalg.map"(%74, %74, %57, %242) ({
      ^bb0(%arg5: i1, %arg6: i1, %arg7: i1):
        %256 = "math.ctlz"(%101) : (i1) -> i1
        %257 = "vector.broadcast"(%182) : (i1) -> vector<28x11xi1>
        %258 = "vector.contract"(%80, %107, %257) <{indexing_maps = [affine_map<(d0, d1, d2) -> (d2, d0, d1)>, affine_map<(d0, d1, d2) -> (d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = [#vector.iterator_type<parallel>, #vector.iterator_type<parallel>, #vector.iterator_type<reduction>], kind = #vector.kind<mul>}> : (vector<11x28x11xi1>, vector<11xi1>, vector<28x11xi1>) -> vector<28x11xi1>
        %259 = "memref.cast"(%128) : (memref<28xf16>) -> memref<28xf16>
        %260 = "index.and"(%29, %39) : (index, index) -> index
        "memref.store"(%86, %73, %17, %28, %21) <{nontemporal = false}> : (i32, memref<11x28x11xi32>, index, index, index) -> ()
        %261 = "bufferization.to_memref"(%48) : (tensor<?x28x11xf16>) -> memref<?x28x11xf16>
        %262 = "bufferization.clone"(%73) : (memref<11x28x11xi32>) -> memref<11x28x11xi32>
        %263 = "arith.divf"(%0, %96) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %264 = "tensor.cast"(%56) : (tensor<11x28x11xi64>) -> tensor<?x?x?xi64>
        %265 = "arith.andi"(%146, %arg5) : (i1, i1) -> i1
        %266 = "index.and"(%30, %237) : (index, index) -> index
        %267 = "vector.broadcast"(%98) : (f32) -> vector<28x11xf32>
        %268:2 = "vector.scan"(%149, %267) <{inclusive = false, kind = #vector.kind<maxf>, reduction_dim = 0 : i64}> : (vector<11x28x11xf32>, vector<28x11xf32>) -> (vector<11x28x11xf32>, vector<28x11xf32>)
        %269 = "memref.alloca"(%95, %233) <{operandSegmentSizes = array<i32: 2, 0>}> : (index, index) -> memref<?x?x11xi64>
        %270 = "math.ctpop"(%203) : (i32) -> i32
        %271 = "vector.broadcast"(%6) : (f32) -> vector<28xf32>
        %272 = "vector.fma"(%271, %271, %271) : (vector<28xf32>, vector<28xf32>, vector<28xf32>) -> vector<28xf32>
        %273 = "math.floor"(%206) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %274 = "arith.andi"(%124, %148) : (i32, i32) -> i32
        %275 = "math.copysign"(%189, %114) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %276 = "arith.cmpf"(%114, %7) <{predicate = 2 : i64}> : (f32, f32) -> i1
        %277 = "math.ipowi"(%62, %62) : (tensor<28xi64>, tensor<28xi64>) -> tensor<28xi64>
        %278 = "vector.splat"(%13) : (i32) -> vector<32xi32>
        %279 = "vector.broadcast"(%177) : (i16) -> vector<i16>
        %280 = "vector.transfer_write"(%279, %63, %43) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, permutation_map = affine_map<(d0) -> ()>}> : (vector<i16>, tensor<11xi16>, index) -> tensor<11xi16>
        %281 = "math.rsqrt"(%88) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %282 = "index.or"(%36, %21) : (index, index) -> index
        %283 = "linalg.copy"(%49, %181) <{operandSegmentSizes = array<i32: 1, 1>}> ({
        ^bb0(%arg8: i32, %arg9: i32):
          "linalg.yield"(%arg8) : (i32) -> ()
        }) : (tensor<?xi32>, tensor<?xi32>) -> tensor<?xi32>
        %284 = "math.log1p"(%142) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
        %285 = "vector.broadcast"(%23) : (index) -> vector<32xindex>
        %286 = "vector.broadcast"(%89) : (i1) -> vector<32xi1>
        %287 = "vector.broadcast"(%124) : (i32) -> vector<32xi32>
        "vector.scatter"(%76, %16, %16, %16, %285, %286, %287) : (memref<?x?x?xi32>, index, index, index, vector<32xindex>, vector<32xi1>, vector<32xi32>) -> ()
        %288 = "math.ctpop"(%219) : (tensor<11xi16>) -> tensor<11xi16>
        %289 = "linalg.copy"(%163, %163) <{operandSegmentSizes = array<i32: 1, 1>}> ({
        ^bb0(%arg8: i16, %arg9: i16):
          "linalg.yield"(%arg8) : (i16) -> ()
        }) : (tensor<i16>, tensor<i16>) -> tensor<i16>
        %290 = "arith.constant"() <{value = 1 : i64}> : () -> i64
        %291 = "vector.broadcast"(%290) : (i64) -> vector<i64>
        %292 = "vector.transfer_write"(%291, %51, %33) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, permutation_map = affine_map<(d0) -> ()>}> : (vector<i64>, tensor<32xi64>, index) -> tensor<32xi64>
        %293 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<28xi1>
        %294 = "memref.load"(%76, %16, %16, %16) <{nontemporal = false}> : (memref<?x?x?xi32>, index, index, index) -> i32
        "linalg.yield"(%91) : (i1) -> ()
      }) : (memref<?xi1>, memref<?xi1>, tensor<?xi1>, tensor<?xi1>) -> tensor<?xi1>
      %244 = "vector.bitcast"(%107) : (vector<11xi1>) -> vector<11xi1>
      %245 = "arith.remsi"(%13, %176) : (i32, i32) -> i32
      %246 = "memref.alloc"(%237) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xf16>
      %247 = "math.exp2"(%154) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
      %248 = "memref.cast"(%69) : (memref<11xf16>) -> memref<?xf16>
      %249 = "linalg.copy"(%59, %59) <{operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg5: f32, %arg6: f32):
        "linalg.yield"(%arg5) : (f32) -> ()
      }) : (tensor<11xf32>, tensor<11xf32>) -> tensor<11xf32>
      %250 = "arith.divui"(%8, %arg3) : (i16, i16) -> i16
      %251 = "tensor.extract"(%63, %16) : (tensor<11xi16>, index) -> i16
      %252 = "vector.broadcast"(%41) : (index) -> vector<11xindex>
      %253 = "vector.broadcast"(%159) : (f16) -> vector<11xf16>
      "vector.scatter"(%128, %33, %252, %244, %253) : (memref<28xf16>, index, vector<11xindex>, vector<11xi1>, vector<11xf16>) -> ()
      %254 = "arith.andi"(%arg4, %arg3) : (i16, i16) -> i16
      %255 = "index.shrs"(%42, %35) : (index, index) -> index
      "linalg.yield"(%198) : (i16) -> ()
    }) : (tensor<11xi16>, tensor<11xi16>, tensor<11xi16>) -> tensor<11xi16>
    %221 = "spirv.CL.floor"(%183) : (f32) -> f32
    %222 = "spirv.GL.FMix"(%6, %6, %142) : (f32, f32, f32) -> f32
    %223 = "spirv.FUnordNotEqual"(%98, %7) : (f32, f32) -> i1
    "vector.print"(%80) <{punctuation = #vector.punctuation<newline>}> : (vector<11x28x11xi1>) -> ()
    "vector.print"(%81) <{punctuation = #vector.punctuation<newline>}> : (vector<32xf32>) -> ()
    "vector.print"(%82) <{punctuation = #vector.punctuation<newline>}> : (vector<32xf32>) -> ()
    "vector.print"(%90) <{punctuation = #vector.punctuation<newline>}> : (vector<28xi32>) -> ()
    "vector.print"(%102) <{punctuation = #vector.punctuation<newline>}> : (vector<2xi32>) -> ()
    "vector.print"(%107) <{punctuation = #vector.punctuation<newline>}> : (vector<11xi1>) -> ()
    "vector.print"(%112) <{punctuation = #vector.punctuation<newline>}> : (vector<5xi32>) -> ()
    "vector.print"(%126) <{punctuation = #vector.punctuation<newline>}> : (vector<32xf32>) -> ()
    "vector.print"(%138) <{punctuation = #vector.punctuation<newline>}> : (vector<32xf32>) -> ()
    "vector.print"(%139) <{punctuation = #vector.punctuation<newline>}> : (vector<32xf32>) -> ()
    "vector.print"(%149) <{punctuation = #vector.punctuation<newline>}> : (vector<11x28x11xf32>) -> ()
    "vector.print"(%150) <{punctuation = #vector.punctuation<newline>}> : (vector<11x28x11xf32>) -> ()
    "vector.print"(%151) <{punctuation = #vector.punctuation<newline>}> : (vector<5xf32>) -> ()
    "vector.print"(%155) <{punctuation = #vector.punctuation<newline>}> : (vector<11xi32>) -> ()
    "vector.print"(%157) <{punctuation = #vector.punctuation<newline>}> : (vector<11xf32>) -> ()
    "vector.print"(%197) <{punctuation = #vector.punctuation<newline>}> : (vector<i32>) -> ()
    "vector.print"(%217) <{punctuation = #vector.punctuation<newline>}> : (vector<f32>) -> ()
    "vector.print"(%0) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%1) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%2) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%3) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%4) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%5) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%6) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%7) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%8) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%9) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%10) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%11) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%12) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%13) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%14) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%15) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%84) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%86) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%88) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%89) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%91) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%93) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%96) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%98) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%100) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%101) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%104) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%105) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%111) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%114) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%123) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%124) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%127) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%130) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%131) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%135) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%142) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%144) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%146) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%148) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%153) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%154) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%159) <{punctuation = #vector.punctuation<newline>}> : (f16) -> ()
    "vector.print"(%167) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%168) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%176) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%177) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%182) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%183) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%188) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%189) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%198) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%203) <{punctuation = #vector.punctuation<newline>}> : (i32) -> ()
    "vector.print"(%204) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%206) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%207) <{punctuation = #vector.punctuation<newline>}> : (i16) -> ()
    "vector.print"(%215) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "vector.print"(%221) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%222) <{punctuation = #vector.punctuation<newline>}> : (f32) -> ()
    "vector.print"(%223) <{punctuation = #vector.punctuation<newline>}> : (i1) -> ()
    "func.return"(%157) : (vector<11xf32>) -> ()
  }) : () -> ()
}) : () -> ()
