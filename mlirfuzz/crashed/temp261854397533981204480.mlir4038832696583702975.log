generated/temp261854397533981204480.mlir:206:16: error: All operations with attached regions need to implement the RegionBranchOpInterface.
        %157 = linalg.copy ins(%10 : tensor<?x?xi32>) outs(%10 : tensor<?x?xi32>) -> tensor<?x?xi32>
               ^
generated/temp261854397533981204480.mlir:206:16: note: see current operation: 
%237 = "linalg.copy"(%59, %59) <{operandSegmentSizes = array<i32: 1, 1>}> ({
^bb0(%arg4: i32, %arg5: i32):
  "linalg.yield"(%arg4) : (i32) -> ()
}) : (tensor<?x?xi32>, tensor<?x?xi32>) -> tensor<?x?xi32>
generated/temp261854397533981204480.mlir:215:18: error: All operations with attached regions need to implement the RegionBranchOpInterface.
          %182 = vector.mask %181 { vector.multi_reduction <minui>, %53, %35 [] : vector<2xi32> to vector<2xi32> } : vector<2xi1> -> vector<2xi32>
                 ^
generated/temp261854397533981204480.mlir:215:18: note: see current operation: 
%265 = "vector.mask"(%264) ({
  "vector.yield"(%102) : (vector<2xi32>) -> ()
}) : (vector<2xi1>) -> vector<2xi32>
generated/temp261854397533981204480.mlir:225:27: error: All operations with attached regions need to implement the RegionBranchOpInterface.
        %broadcasted_29 = linalg.broadcast ins(%alloc_17 : memref<?x5xi1>) outs(%163 : tensor<?x5x17xi1>) dimensions = [2] 
                          ^
generated/temp261854397533981204480.mlir:225:27: note: see current operation: 
%244 = "linalg.broadcast"(%79, %243) <{dimensions = array<i64: 2>}> ({
^bb0(%arg4: i1, %arg5: i1):
  "linalg.yield"(%arg4) : (i1) -> ()
}) : (memref<?x5xi1>, tensor<?x5x17xi1>) -> tensor<?x5x17xi1>
generated/temp261854397533981204480.mlir:240:16: error: All operations with attached regions need to implement the RegionBranchOpInterface.
        %174 = vector.mask %173 { vector.multi_reduction <xor>, %53, %53 [] : vector<2xi32> to vector<2xi32> } : vector<2xi1> -> vector<2xi32>
               ^
generated/temp261854397533981204480.mlir:240:16: note: see current operation: 
%256 = "vector.mask"(%255) ({
  "vector.yield"(%102) : (vector<2xi32>) -> ()
}) : (vector<2xi1>) -> vector<2xi32>
generated/temp261854397533981204480.mlir:157:15: error: All operations with attached regions need to implement the RegionBranchOpInterface.
    %mapped = linalg.map ins(%11, %alloc_19 : tensor<?xi16>, memref<?xi16>) outs(%58 : tensor<?xi16>)
              ^
generated/temp261854397533981204480.mlir:157:15: note: see current operation: 
%129 = "linalg.map"(%61, %81, %128) ({
^bb0(%arg2: i16, %arg3: i16):
  %216 = "index.castu"(%arg2) : (i16) -> index
  %217 = "tensor.empty"() : () -> tensor<32x17xi64>
  %218 = "vector.broadcast"(%113) : (i64) -> vector<5xi64>
  %219 = "arith.constant"() <{value = dense<true> : vector<5xi1>}> : () -> vector<5xi1>
  %220 = "arith.constant"() <{value = dense<468005861> : vector<5xi32>}> : () -> vector<5xi32>
  %221 = "vector.gather"(%217, %100, %33, %220, %219, %218) : (tensor<32x17xi64>, index, index, vector<5xi32>, vector<5xi1>, vector<5xi64>) -> vector<5xi64>
  %222 = "vector.splat"(%100) : (index) -> vector<5xindex>
  %223 = "index.and"(%100, %100) : (index, index) -> index
  %224 = "arith.remui"(%6, %9) : (i1, i1) -> i1
  %225 = "memref.cast"(%67) : (memref<?x?xf16>) -> memref<17x?xf16>
  %226 = "vector.contract"(%102, %102, %7) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = [#vector.iterator_type<reduction>], kind = #vector.kind<add>}> : (vector<2xi32>, vector<2xi32>, i32) -> i32
  %227 = "affine.if"() ({
    %258 = "arith.constant"() <{value = dense<1.76038246E+9> : vector<17xf32>}> : () -> vector<17xf32>
    %259 = "bufferization.to_memref"(%65) : (tensor<?x?xi16>) -> memref<?x?xi16>
    %260 = "arith.addi"(%84, %84) : (i1, i1) -> i1
    %261 = "vector.load"(%70, %19) : (memref<5xi1>, index) -> vector<17xi1>
    %262 = "math.absf"(%111) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %263 = "affine.load"(%73, %23) <{map = affine_map<(d0) -> (d0)>}> : (memref<?xi16>, index) -> i16
    "memref.dealloc"(%116) : (memref<?xf32>) -> ()
    "vector.compressstore"(%69, %19, %219, %221) : (memref<17xi64>, index, vector<5xi1>, vector<5xi64>) -> ()
    %264 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<5x5xi32>
    "affine.yield"(%264) : (memref<5x5xi32>) -> ()
  }, {
    %258 = "arith.cmpi"(%112, %9) <{predicate = 7 : i64}> : (i1, i1) -> i1
    %259 = "vector.broadcast"(%86) : (f32) -> vector<5x5x17xf32>
    %260 = "vector.broadcast"(%107) : (f32) -> vector<5x5xf32>
    %261:2 = "vector.scan"(%259, %260) <{inclusive = true, kind = #vector.kind<mul>, reduction_dim = 2 : i64}> : (vector<5x5x17xf32>, vector<5x5xf32>) -> (vector<5x5x17xf32>, vector<5x5xf32>)
    %262 = "index.ceildivu"(%29, %100) : (index, index) -> index
    %263 = "math.log"(%111) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %264 = "arith.cmpi"(%113, %1) <{predicate = 3 : i64}> : (i64, i64) -> i1
    %265 = "arith.mulf"(%101, %3) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %266 = "vector.broadcast"(%arg1) : (f16) -> vector<32xf16>
    %267 = "vector.broadcast"(%106) : (i1) -> vector<32xi1>
    %268 = "vector.maskedload"(%67, %16, %16, %267, %266) : (memref<?x?xf16>, index, index, vector<32xi1>, vector<32xf16>) -> vector<32xf16>
    %269 = "index.shl"(%27, %25) : (index, index) -> index
    %270 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<5x5xi32>
    "affine.yield"(%270) : (memref<5x5xi32>) -> ()
  }) {condition = affine_set<() : (-24 >= 0, 6 == 0, 1 == 0)>} : () -> memref<5x5xi32>
  %228 = "affine.load"(%72, %16) <{map = affine_map<(d0) -> (d0)>}> : (memref<5xi64>, index) -> i64
  %229 = "arith.remf"(%101, %5) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
  %230 = "index.maxu"(%22, %35) : (index, index) -> index
  %231 = "vector.broadcast"(%113) : (i64) -> vector<32x17xi64>
  %232 = "vector.broadcast"(%84) : (i1) -> vector<32x17xi1>
  %233 = "arith.constant"() <{value = dense<905606255> : vector<32x17xi32>}> : () -> vector<32x17xi32>
  %234 = "vector.gather"(%69, %19, %233, %232, %231) : (memref<17xi64>, index, vector<32x17xi32>, vector<32x17xi1>, vector<32x17xi64>) -> vector<32x17xi64>
  %235 = "tensor.expand_shape"(%52) <{reassociation = [[0], [1, 2]]}> : (tensor<?x5xi1>) -> tensor<?x5x1xi1>
  %236 = "index.ceildivu"(%24, %21) : (index, index) -> index
  %237 = "linalg.copy"(%59, %59) <{operandSegmentSizes = array<i32: 1, 1>}> ({
  ^bb0(%arg4: i32, %arg5: i32):
    "linalg.yield"(%arg4) : (i32) -> ()
  }) : (tensor<?x?xi32>, tensor<?x?xi32>) -> tensor<?x?xi32>
  "vector.warp_execute_on_lane_0"(%16) <{warp_size = 32 : i64}> ({
    %258 = "math.fpowi"(%107, %10) <{fastmath = #arith.fastmath<none>}> : (f32, i32) -> f32
    %259 = "arith.divf"(%122, %arg1) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
    %260 = "tensor.empty"() : () -> tensor<544xi32>
    %261 = "tensor.unpack"(%48, %260, %33) <{inner_dims_pos = array<i64: 0>, outer_dims_perm = array<i64: 0>, static_inner_tiles = array<i64: -9223372036854775808>}> : (tensor<32x17xi32>, tensor<544xi32>, index) -> tensor<544xi32>
    %262 = "index.floordivs"(%35, %32) : (index, index) -> index
    %263 = "math.ceil"(%arg1) <{fastmath = #arith.fastmath<none>}> : (f16) -> f16
    %264 = "arith.constant"() <{value = dense<true> : vector<2xi1>}> : () -> vector<2xi1>
    %265 = "vector.mask"(%264) ({
      "vector.yield"(%102) : (vector<2xi32>) -> ()
    }) : (vector<2xi1>) -> vector<2xi32>
    %266 = "tensor.expand_shape"(%260) <{reassociation = [[0, 1]]}> : (tensor<544xi32>) -> tensor<544x1xi32>
    %267 = "memref.load"(%77, %16) : (memref<?xi1>, index) -> i1
    "vector.yield"() : () -> ()
  }) : (index) -> ()
  %238 = "index.shru"(%41, %39) : (index, index) -> index
  %239 = "math.atan2"(%126, %3) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
  %240 = "index.maxu"(%34, %18) : (index, index) -> index
  %241 = "vector.broadcast"(%86) : (f32) -> vector<32x17xf32>
  %242 = "vector.fma"(%241, %241, %241) : (vector<32x17xf32>, vector<32x17xf32>, vector<32x17xf32>) -> vector<32x17xf32>
  %243 = "tensor.empty"(%31) : (index) -> tensor<?x5x17xi1>
  %244 = "linalg.broadcast"(%79, %243) <{dimensions = array<i64: 2>}> ({
  ^bb0(%arg4: i1, %arg5: i1):
    "linalg.yield"(%arg4) : (i1) -> ()
  }) : (memref<?x5xi1>, tensor<?x5x17xi1>) -> tensor<?x5x17xi1>
  %245 = "vector.broadcast"(%230) : (index) -> vector<32xindex>
  %246 = "vector.broadcast"(%84) : (i1) -> vector<32xi1>
  %247 = "vector.broadcast"(%arg1) : (f16) -> vector<32xf16>
  "vector.scatter"(%68, %16, %245, %246, %247) : (memref<?xf16>, index, vector<32xindex>, vector<32xi1>, vector<32xf16>) -> ()
  %248 = "vector.reduction"(%102) <{fastmath = #arith.fastmath<none>, kind = #vector.kind<xor>}> : (vector<2xi32>) -> i32
  %249 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
  %250 = "vector.transfer_read"(%115, %38, %249) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, permutation_map = affine_map<(d0) -> ()>}> : (tensor<?xf32>, index, f32) -> vector<f32>
  "memref.store"(%122, %68, %16) : (f16, memref<?xf16>, index) -> ()
  %251 = "arith.constant"() <{value = 0 : i16}> : () -> i16
  %252 = "vector.transfer_read"(%237, %18, %92, %7) <{operandSegmentSizes = array<i32: 1, 2, 1, 0>, permutation_map = affine_map<(d0, d1) -> ()>}> : (tensor<?x?xi32>, index, index, i32) -> vector<i32>
  %253 = "index.ceildivu"(%223, %27) : (index, index) -> index
  "affine.vector_store"(%218, %88, %36, %34, %25) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> : (vector<5xi64>, memref<?x5x1xi64>, index, index, index) -> ()
  %254 = "arith.remf"(%122, %arg1) <{fastmath = #arith.fastmath<none>}> : (f16, f16) -> f16
  %255 = "arith.constant"() <{value = dense<false> : vector<2xi1>}> : () -> vector<2xi1>
  %256 = "vector.mask"(%255) ({
    "vector.yield"(%102) : (vector<2xi32>) -> ()
  }) : (vector<2xi1>) -> vector<2xi32>
  %257 = "vector.transfer_read"(%61, %110, %2) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, permutation_map = affine_map<(d0) -> ()>}> : (tensor<?xi16>, index, i16) -> vector<i16>
  "linalg.yield"(%12) : (i16) -> ()
}) : (tensor<?xi16>, memref<?xi16>, tensor<?xi16>) -> tensor<?xi16>
generated/temp261854397533981204480.mlir:245:11: error: All operations with attached regions need to implement the RegionBranchOpInterface.
    %60 = linalg.copy ins(%0 : tensor<32x17xi32>) outs(%0 : tensor<32x17xi32>) -> tensor<32x17xi32>
          ^
generated/temp261854397533981204480.mlir:245:11: note: see current operation: 
%131 = "linalg.copy"(%48, %48) <{operandSegmentSizes = array<i32: 1, 1>}> ({
^bb0(%arg2: i32, %arg3: i32):
  "linalg.yield"(%arg2) : (i32) -> ()
}) : (tensor<32x17xi32>, tensor<32x17xi32>) -> tensor<32x17xi32>
generated/temp261854397533981204480.mlir:258:14: error: All operations with attached regions need to implement the RegionBranchOpInterface.
      %142 = linalg.matmul ins(%4, %140 : tensor<?x5xi1>, tensor<5x17xi1>) outs(%141 : tensor<?x17xi1>) -> tensor<?x17xi1>
             ^
generated/temp261854397533981204480.mlir:258:14: note: see current operation: 
%220 = "linalg.matmul"(%52, %218, %219) <{operandSegmentSizes = array<i32: 2, 1>}> ({
^bb0(%arg3: i1, %arg4: i1, %arg5: i1):
  %231 = "arith.andi"(%arg3, %arg4) : (i1, i1) -> i1
  %232 = "arith.ori"(%arg5, %231) : (i1, i1) -> i1
  "linalg.yield"(%232) : (i1) -> ()
}) {linalg.memoized_indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>]} : (tensor<?x5xi1>, tensor<5x17xi1>, tensor<?x17xi1>) -> tensor<?x17xi1>
generated/temp261854397533981204480.mlir:310:16: error: All operations with attached regions need to implement the RegionBranchOpInterface.
        %140 = linalg.dot ins(%41, %41 : tensor<544xi1>, tensor<544xi1>) outs(%139 : tensor<i1>) -> tensor<i1>
               ^
generated/temp261854397533981204480.mlir:310:16: note: see current operation: 
%219 = "linalg.dot"(%108, %108, %218) <{operandSegmentSizes = array<i32: 2, 1>}> ({
^bb0(%arg4: i1, %arg5: i1, %arg6: i1):
  %248 = "arith.andi"(%arg4, %arg5) : (i1, i1) -> i1
  %249 = "arith.ori"(%arg6, %248) : (i1, i1) -> i1
  "linalg.yield"(%249) : (i1) -> ()
}) {linalg.memoized_indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>]} : (tensor<544xi1>, tensor<544xi1>, tensor<i1>) -> tensor<i1>
generated/temp261854397533981204480.mlir:324:18: error: All operations with attached regions need to implement the RegionBranchOpInterface.
          %171 = vector.mask %23 { vector.multi_reduction <add>, %78, %70 [] : vector<1xi1> to vector<1xi1> } : vector<1xi1> -> vector<1xi1>
                 ^
generated/temp261854397533981204480.mlir:324:18: note: see current operation: 
%254 = "vector.mask"(%90) ({
  "vector.yield"(%89) : (vector<1xi1>) -> ()
}) : (vector<1xi1>) -> vector<1xi1>
generated/temp261854397533981204480.mlir:328:18: error: All operations with attached regions need to implement the RegionBranchOpInterface.
          %166 = linalg.copy ins(%31 : tensor<5xi1>) outs(%31 : tensor<5xi1>) -> tensor<5xi1>
                 ^
generated/temp261854397533981204480.mlir:328:18: note: see current operation: 
%249 = "linalg.copy"(%98, %98) <{operandSegmentSizes = array<i32: 1, 1>}> ({
^bb0(%arg4: i1, %arg5: i1):
  "linalg.yield"(%arg4) : (i1) -> ()
}) : (tensor<5xi1>, tensor<5xi1>) -> tensor<5xi1>
generated/temp261854397533981204480.mlir:330:18: error: All operations with attached regions need to implement the RegionBranchOpInterface.
          %168 = linalg.copy ins(%11 : tensor<?xi16>) outs(%58 : tensor<?xi16>) -> tensor<?xi16>
                 ^
generated/temp261854397533981204480.mlir:330:18: note: see current operation: 
%251 = "linalg.copy"(%61, %128) <{operandSegmentSizes = array<i32: 1, 1>}> ({
^bb0(%arg4: i16, %arg5: i16):
  "linalg.yield"(%arg4) : (i16) -> ()
}) : (tensor<?xi16>, tensor<?xi16>) -> tensor<?xi16>
generated/temp261854397533981204480.mlir:304:18: error: All operations with attached regions need to implement the RegionBranchOpInterface.
    %mapped_24 = linalg.map ins(%alloc_7, %alloc_7 : memref<17xi64>, memref<17xi64>) outs(%79 : tensor<17xi64>)
                 ^
generated/temp261854397533981204480.mlir:304:18: note: see current operation: 
%153 = "linalg.map"(%69, %69, %152) ({
^bb0(%arg2: i64, %arg3: i64):
  %216 = "tensor.cast"(%49) : (tensor<?x5xi64>) -> tensor<1x5xi64>
  "vector.print"(%102) <{punctuation = #vector.punctuation<newline>}> : (vector<2xi32>) -> ()
  %217 = "memref.load"(%80, %16) : (memref<?xf32>, index) -> f32
  %218 = "tensor.empty"() : () -> tensor<i1>
  %219 = "linalg.dot"(%108, %108, %218) <{operandSegmentSizes = array<i32: 2, 1>}> ({
  ^bb0(%arg4: i1, %arg5: i1, %arg6: i1):
    %248 = "arith.andi"(%arg4, %arg5) : (i1, i1) -> i1
    %249 = "arith.ori"(%arg6, %248) : (i1, i1) -> i1
    "linalg.yield"(%249) : (i1) -> ()
  }) {linalg.memoized_indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>]} : (tensor<544xi1>, tensor<544xi1>, tensor<i1>) -> tensor<i1>
  %220 = "arith.muli"(%113, %arg2) : (i64, i64) -> i64
  %221 = "memref.alloca"(%82) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?xi32>
  %222 = "arith.constant"() <{value = 30.7132416 : f32}> : () -> f32
  "memref.assume_alignment"(%78) <{alignment = 16 : i32}> : (memref<?x?xf32>) -> ()
  %223 = "affine.if"() ({
    %248 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<544xi1>
    "linalg.transpose"(%108, %248) <{permutation = array<i64: 0>}> ({
    ^bb0(%arg4: i1, %arg5: i1):
      "linalg.yield"(%arg4) : (i1) -> ()
    }) : (tensor<544xi1>, memref<544xi1>) -> ()
    %249 = "index.shru"(%47, %16) : (index, index) -> index
    %250 = "index.casts"(%147) : (i1) -> index
    %251 = "index.shru"(%135, %32) : (index, index) -> index
    %252 = "arith.shrsi"(%1, %113) : (i64, i64) -> i64
    %253 = "arith.constant"() <{value = 22 : index}> : () -> index
    %254 = "vector.mask"(%90) ({
      "vector.yield"(%89) : (vector<1xi1>) -> ()
    }) : (vector<1xi1>) -> vector<1xi1>
    "affine.yield"(%85) : (f32) -> ()
  }, {
    %248 = "arith.remsi"(%123, %4) : (i32, i32) -> i32
    %249 = "linalg.copy"(%98, %98) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg4: i1, %arg5: i1):
      "linalg.yield"(%arg4) : (i1) -> ()
    }) : (tensor<5xi1>, tensor<5xi1>) -> tensor<5xi1>
    %250 = "index.or"(%46, %23) : (index, index) -> index
    %251 = "linalg.copy"(%61, %128) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg4: i16, %arg5: i16):
      "linalg.yield"(%arg4) : (i16) -> ()
    }) : (tensor<?xi16>, tensor<?xi16>) -> tensor<?xi16>
    %252 = "arith.shrsi"(%9, %138) : (i1, i1) -> i1
    %253 = "tensor.collapse_shape"(%57) <{reassociation = [[0, 1]]}> : (tensor<?x?xi1>) -> tensor<?xi1>
    "memref.copy"(%88, %88) : (memref<?x5x1xi64>, memref<?x5x1xi64>) -> ()
    %254 = "arith.constant"() <{value = 1.000000e+00 : f32}> : () -> f32
    "affine.yield"(%136) : (f32) -> ()
  }) {condition = affine_set<() : (1 == 0, 22 == 0, 1 == 0, -24 == 0)>} : () -> f32
  %224 = "math.absi"(%98) : (tensor<5xi1>) -> tensor<5xi1>
  %225 = "arith.constant"() <{value = 1506594351 : i32}> : () -> i32
  %226 = "arith.minui"(%arg3, %1) : (i64, i64) -> i64
  %227 = "vector.transfer_read"(%64, %104, %44, %5) <{operandSegmentSizes = array<i32: 1, 2, 1, 0>, permutation_map = affine_map<(d0, d1) -> ()>}> : (tensor<?x?xf32>, index, index, f32) -> vector<f32>
  %228 = "math.fma"(%86, %111, %101) <{fastmath = #arith.fastmath<none>}> : (f32, f32, f32) -> f32
  %229 = "memref.alloc"(%24) <{operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<?x1xi1>
  "linalg.broadcast"(%117, %229) <{dimensions = array<i64: 1>}> ({
  ^bb0(%arg4: i1, %arg5: i1):
    "linalg.yield"(%arg4) : (i1) -> ()
  }) : (tensor<?xi1>, memref<?x1xi1>) -> ()
  %230 = "arith.constant"() <{value = 0 : i16}> : () -> i16
  %231 = "tensor.insert"(%12, %61, %16) : (i16, tensor<?xi16>, index) -> tensor<?xi16>
  %232 = "scf.execute_region"() ({
    %248 = "index.castu"(%30) : (index) -> i32
    %249 = "arith.andi"(%arg2, %arg3) : (i64, i64) -> i64
    %250 = "arith.constant"() <{value = 25914 : i16}> : () -> i16
    %251 = "arith.constant"() <{value = dense<1917401609> : vector<17xi32>}> : () -> vector<17xi32>
    %252 = "arith.addi"(%106, %118) : (i1, i1) -> i1
    %253 = "arith.constant"() <{value = -16 : index}> : () -> index
    "memref.assume_alignment"(%75) <{alignment = 8 : i32}> : (memref<5xf32>) -> ()
    %254 = "index.mul"(%100, %31) : (index, index) -> index
    %255 = "vector.flat_transpose"(%102) <{columns = 1 : i32, rows = 2 : i32}> : (vector<2xi32>) -> vector<2xi32>
    %256 = "memref.realloc"(%71) : (memref<?xi64>) -> memref<17xi64>
    %257 = "arith.divf"(%136, %11) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %258 = "arith.divf"(%97, %3) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %259 = "arith.constant"() <{value = 0 : i16}> : () -> i16
    %260 = "arith.shli"(%123, %123) : (i32, i32) -> i32
    %261 = "arith.constant"() <{value = dense<1117825710> : vector<1x32xi32>}> : () -> vector<1x32xi32>
    %262 = "arith.constant"() <{value = dense<468005861> : vector<32xi32>}> : () -> vector<32xi32>
    %263:2 = "vector.scan"(%261, %262) <{inclusive = true, kind = #vector.kind<minui>, reduction_dim = 0 : i64}> : (vector<1x32xi32>, vector<32xi32>) -> (vector<1x32xi32>, vector<32xi32>)
    %264 = "affine.min"(%42, %145, %21, %35, %35) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d1 - 2)>}> : (index, index, index, index, index) -> index
    "scf.yield"(%148) : (f16) -> ()
  }) : () -> f16
  %233 = "index.and"(%41, %30) : (index, index) -> index
  %234 = "index.casts"(%145) : (index) -> i32
  %235 = "arith.constant"() <{value = dense<2.04775539E+9> : vector<17xf32>}> : () -> vector<17xf32>
  %236 = "vector.fma"(%235, %235, %235) : (vector<17xf32>, vector<17xf32>, vector<17xf32>) -> vector<17xf32>
  "memref.dealloc"(%60) : (memref<?x?xi32>) -> ()
  %237 = "arith.constant"() <{value = 212219455 : i32}> : () -> i32
  %238 = "vector.broadcast"(%147) : (i1) -> vector<32xi1>
  %239 = "vector.maskedload"(%66, %16, %16, %238, %238) : (memref<?x?xi1>, index, index, vector<32xi1>, vector<32xi1>) -> vector<32xi1>
  %240 = "arith.constant"() <{value = 8 : i16}> : () -> i16
  %241 = "arith.constant"() <{value = 1 : i32}> : () -> i32
  %242 = "vector.broadcast"(%127) : (i1) -> vector<1x1xi1>
  %243 = "vector.outerproduct"(%89, %89, %242) <{kind = #vector.kind<maxui>}> : (vector<1xi1>, vector<1xi1>, vector<1x1xi1>) -> vector<1x1xi1>
  %244 = "bufferization.clone"(%74) : (memref<5x5xf16>) -> memref<5x5xf16>
  %245 = "arith.remsi"(%113, %1) : (i64, i64) -> i64
  "vector.print"(%90) <{punctuation = #vector.punctuation<newline>}> : (vector<1xi1>) -> ()
  %246 = "index.shru"(%18, %22) : (index, index) -> index
  %247 = "arith.remf"(%101, %96) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
  "linalg.yield"(%113) : (i64) -> ()
}) : (memref<17xi64>, memref<17xi64>, tensor<17xi64>) -> tensor<17xi64>
generated/temp261854397533981204480.mlir:427:20: error: All operations with attached regions need to implement the RegionBranchOpInterface.
    %broadcasted = linalg.broadcast ins(%alloc_7 : memref<17xi64>) outs(%116 : tensor<17x32xi64>) dimensions = [1] 
                   ^
generated/temp261854397533981204480.mlir:427:20: note: see current operation: 
%192 = "linalg.broadcast"(%69, %191) <{dimensions = array<i64: 1>}> ({
^bb0(%arg2: i64, %arg3: i64):
  "linalg.yield"(%arg2) : (i64) -> ()
}) : (memref<17xi64>, tensor<17x32xi64>) -> tensor<17x32xi64>
generated/temp261854397533981204480.mlir:446:12: error: All operations with attached regions need to implement the RegionBranchOpInterface.
    %134 = vector.mask %133 { vector.multi_reduction <add>, %53, %35 [] : vector<2xi32> to vector<2xi32> } : vector<2xi1> -> vector<2xi32>
           ^
generated/temp261854397533981204480.mlir:446:12: note: see current operation: 
%211 = "vector.mask"(%210) ({
  "vector.yield"(%102) : (vector<2xi32>) -> ()
}) : (vector<2xi1>) -> vector<2xi32>
generated/temp261854397533981204480.mlir:612:18: error: All operations with attached regions need to implement the RegionBranchOpInterface.
    %generated = tensor.generate %c22 {
                 ^
generated/temp261854397533981204480.mlir:612:18: note: see current operation: 
%83 = "tensor.generate"(%38) ({
^bb0(%arg1: index):
  %216 = "arith.constant"() <{value = dense<-3517> : vector<1xi16>}> : () -> vector<1xi16>
  %217 = "math.ctpop"(%60) : (tensor<?x17xi16>) -> tensor<?x17xi16>
  %218 = "arith.constant"() <{value = 808913625 : i32}> : () -> i32
  "tensor.yield"(%82) : (i32) -> ()
}) : (index) -> tensor<?xi32>
generated/temp261854397533981204480.mlir:628:11: error: All operations with attached regions need to implement the RegionBranchOpInterface.
    %25 = linalg.dot ins(%alloc_10, %23 : memref<5xi64>, tensor<5xi64>) outs(%24 : tensor<i64>) -> tensor<i64>
          ^
generated/temp261854397533981204480.mlir:628:11: note: see current operation: 
%91 = "linalg.dot"(%70, %89, %90) <{operandSegmentSizes = array<i32: 2, 1>}> ({
^bb0(%arg1: i64, %arg2: i64, %arg3: i64):
  %216 = "arith.muli"(%arg1, %arg2) : (i64, i64) -> i64
  %217 = "arith.addi"(%arg3, %216) : (i64, i64) -> i64
  "linalg.yield"(%217) : (i64) -> ()
}) {linalg.memoized_indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>]} : (memref<5xi64>, tensor<5xi64>, tensor<i64>) -> tensor<i64>
generated/temp261854397533981204480.mlir:655:11: error: All operations with attached regions need to implement the RegionBranchOpInterface.
    %45 = linalg.dot ins(%alloc_8, %alloc_8 : memref<5xi1>, memref<5xi1>) outs(%44 : tensor<i1>) -> tensor<i1>
          ^
generated/temp261854397533981204480.mlir:655:11: note: see current operation: 
%116 = "linalg.dot"(%68, %68, %115) <{operandSegmentSizes = array<i32: 2, 1>}> ({
^bb0(%arg1: i1, %arg2: i1, %arg3: i1):
  %216 = "arith.andi"(%arg1, %arg2) : (i1, i1) -> i1
  %217 = "arith.ori"(%arg3, %216) : (i1, i1) -> i1
  "linalg.yield"(%217) : (i1) -> ()
}) {linalg.memoized_indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>]} : (memref<5xi1>, memref<5xi1>, tensor<i1>) -> tensor<i1>
generated/temp261854397533981204480.mlir:716:14: error: All operations with attached regions need to implement the RegionBranchOpInterface.
      %137 = vector.mask %33 { vector.multi_reduction <or>, %32, %33 [] : vector<1xi1> to vector<1xi1> } : vector<1xi1> -> vector<1xi1>
             ^
generated/temp261854397533981204480.mlir:716:14: note: see current operation: 
%217 = "vector.mask"(%102) ({
  "vector.yield"(%101) : (vector<1xi1>) -> ()
}) : (vector<1xi1>) -> vector<1xi1>
generated/temp261854397533981204480.mlir:742:11: error: All operations with attached regions need to implement the RegionBranchOpInterface.
    %94 = linalg.dot ins(%6, %alloc_23 : tensor<5xi32>, memref<5xi32>) outs(%93 : tensor<i32>) -> tensor<i32>
          ^
generated/temp261854397533981204480.mlir:742:11: note: see current operation: 
%166 = "linalg.dot"(%54, %164, %165) <{operandSegmentSizes = array<i32: 2, 1>}> ({
^bb0(%arg1: i32, %arg2: i32, %arg3: i32):
  %216 = "arith.muli"(%arg1, %arg2) : (i32, i32) -> i32
  %217 = "arith.addi"(%arg3, %216) : (i32, i32) -> i32
  "linalg.yield"(%217) : (i32) -> ()
}) {linalg.memoized_indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>]} : (tensor<5xi32>, memref<5xi32>, tensor<i32>) -> tensor<i32>
generated/temp261854397533981204480.mlir:781:18: error: All operations with attached regions need to implement the RegionBranchOpInterface.
          %168 = linalg.copy ins(%93 : tensor<i32>) outs(%94 : tensor<i32>) -> tensor<i32>
                 ^
generated/temp261854397533981204480.mlir:781:18: note: see current operation: 
%255 = "linalg.copy"(%165, %166) <{operandSegmentSizes = array<i32: 1, 1>}> ({
^bb0(%arg4: i32, %arg5: i32):
  "linalg.yield"(%arg4) : (i32) -> ()
}) : (tensor<i32>, tensor<i32>) -> tensor<i32>
generated/temp261854397533981204480.mlir:745:20: error: All operations with attached regions need to implement the RegionBranchOpInterface.
      %mapped_28 = linalg.map ins(%12, %12 : tensor<?x17xi16>, tensor<?x17xi16>) outs(%136 : tensor<?x17xi16>)
                   ^
generated/temp261854397533981204480.mlir:745:20: note: see current operation: 
%217 = "linalg.map"(%60, %60, %216) ({
^bb0(%arg2: i16, %arg3: i16):
  %224 = "arith.xori"(%9, %13) : (i1, i1) -> i1
  %225 = "tensor.collapse_shape"(%49) <{reassociation = [[0, 1]]}> : (tensor<?x5xi64>) -> tensor<?xi64>
  %226 = "arith.shrui"(%151, %151) : (i16, i16) -> i16
  %227 = "arith.remf"(%114, %154) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
  %228 = "index.casts"(%46) : (index) -> i32
  %229 = "tensor.empty"() : () -> tensor<17xf32>
  %230 = "arith.remf"(%104, %144) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
  %231 = "math.exp2"(%119) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
  %232 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<5xi64>
  %233 = "arith.constant"() <{value = dense<1.76038246E+9> : vector<17xf32>}> : () -> vector<17xf32>
  %234 = "vector.broadcast"(%156) : (i1) -> vector<17xi1>
  "vector.compressstore"(%73, %17, %234, %233) : (memref<5xf32>, index, vector<17xi1>, vector<17xf32>) -> ()
  %235 = "arith.constant"() <{value = 28 : index}> : () -> index
  %236 = "vector.contract"(%127, %127, %12) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = [#vector.iterator_type<reduction>], kind = #vector.kind<mul>}> : (vector<5xi16>, vector<5xi16>, i16) -> i16
  %237 = "vector.extract_strided_slice"(%112) <{offsets = [0], sizes = [1], strides = [1]}> : (vector<2xi32>) -> vector<1xi32>
  %238 = "memref.alloc"(%42, %37) <{operandSegmentSizes = array<i32: 2, 0>}> : (index, index) -> memref<?x?xi16>
  "memref.tensor_store"(%63, %238) : (tensor<?x?xi16>, memref<?x?xi16>) -> ()
  %239 = "arith.constant"() <{value = 1 : index}> : () -> index
  %240 = "bufferization.clone"(%67) : (memref<17xi64>) -> memref<17xi64>
  %241 = "vector.broadcast"(%120) : (f32) -> vector<5x5xf32>
  %242 = "vector.fma"(%241, %241, %241) : (vector<5x5xf32>, vector<5x5xf32>, vector<5x5xf32>) -> vector<5x5xf32>
  %243 = "math.rsqrt"(%118) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
  %244 = "index.shru"(%38, %42) : (index, index) -> index
  %245 = "arith.shrui"(%153, %4) : (i32, i32) -> i32
  %246 = "index.floordivs"(%37, %19) : (index, index) -> index
  %247 = "tensor.collapse_shape"(%58) <{reassociation = [[0, 1]]}> : (tensor<?x?xi32>) -> tensor<?xi32>
  %248 = "vector.insertelement"(%161, %87, %149) : (i1, vector<5xi1>, index) -> vector<5xi1>
  %249 = "math.sqrt"(%229) <{fastmath = #arith.fastmath<none>}> : (tensor<17xf32>) -> tensor<17xf32>
  %250 = "tensor.dim"(%216, %16) : (tensor<?x17xi16>, index) -> index
  %251 = "arith.constant"() <{value = false}> : () -> i1
  %252 = "arith.constant"() <{value = dense<324534398> : vector<5x5xi64>}> : () -> vector<5x5xi64>
  %253 = "vector.outerproduct"(%88, %123, %252) <{kind = #vector.kind<xor>}> : (vector<5xi64>, vector<5xi64>, vector<5x5xi64>) -> vector<5x5xi64>
  %254 = "arith.constant"() <{value = 468005861 : i32}> : () -> i32
  %255 = "linalg.copy"(%165, %166) <{operandSegmentSizes = array<i32: 1, 1>}> ({
  ^bb0(%arg4: i32, %arg5: i32):
    "linalg.yield"(%arg4) : (i32) -> ()
  }) : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %256 = "arith.minui"(%arg3, %2) : (i16, i16) -> i16
  %257 = "arith.cmpi"(%142, %4) <{predicate = 5 : i64}> : (i32, i32) -> i1
  "linalg.yield"(%arg2) : (i16) -> ()
}) : (tensor<?x17xi16>, tensor<?x17xi16>, tensor<?x17xi16>) -> tensor<?x17xi16>
generated/temp261854397533981204480.mlir:808:18: error: All operations with attached regions need to implement the RegionBranchOpInterface.
          %155 = linalg.matmul ins(%alloc_17, %alloc_33 : memref<?x5xi1>, memref<5x17xi1>) outs(%154 : tensor<?x17xi1>) -> tensor<?x17xi1>
                 ^
generated/temp261854397533981204480.mlir:808:18: note: see current operation: 
%242 = "linalg.matmul"(%77, %240, %241) <{operandSegmentSizes = array<i32: 2, 1>}> ({
^bb0(%arg3: i1, %arg4: i1, %arg5: i1):
  %273 = "arith.andi"(%arg3, %arg4) : (i1, i1) -> i1
  %274 = "arith.ori"(%arg5, %273) : (i1, i1) -> i1
  "linalg.yield"(%274) : (i1) -> ()
}) {linalg.memoized_indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>]} : (memref<?x5xi1>, memref<5x17xi1>, tensor<?x17xi1>) -> tensor<?x17xi1>
generated/temp261854397533981204480.mlir:821:18: error: All operations with attached regions need to implement the RegionBranchOpInterface.
          %166 = vector.mask %21 { vector.multi_reduction <xor>, %21, %21 [] : vector<5xi1> to vector<5xi1> } : vector<5xi1> -> vector<5xi1>
                 ^
generated/temp261854397533981204480.mlir:821:18: note: see current operation: 
%254 = "vector.mask"(%87) ({
  "vector.yield"(%87) : (vector<5xi1>) -> ()
}) : (vector<5xi1>) -> vector<5xi1>
generated/temp261854397533981204480.mlir:827:26: error: All operations with attached regions need to implement the RegionBranchOpInterface.
          %broadcasted = linalg.broadcast ins(%10 : tensor<?x?xi32>) outs(%170 : tensor<?x?x1xi32>) dimensions = [2] 
                         ^
generated/temp261854397533981204480.mlir:827:26: note: see current operation: 
%260 = "linalg.broadcast"(%58, %259) <{dimensions = array<i64: 2>}> ({
^bb0(%arg3: i32, %arg4: i32):
  "linalg.yield"(%arg3) : (i32) -> ()
}) : (tensor<?x?xi32>, tensor<?x?x1xi32>) -> tensor<?x?x1xi32>
generated/temp261854397533981204480.mlir:831:18: error: All operations with attached regions need to implement the RegionBranchOpInterface.
          %174 = linalg.dot ins(%arg1, %173 : memref<17xi64>, tensor<17xi64>) outs(%24 : tensor<i64>) -> tensor<i64>
                 ^
generated/temp261854397533981204480.mlir:831:18: note: see current operation: 
%264 = "linalg.dot"(%arg1, %263, %90) <{operandSegmentSizes = array<i32: 2, 1>}> ({
^bb0(%arg3: i64, %arg4: i64, %arg5: i64):
  %273 = "arith.muli"(%arg3, %arg4) : (i64, i64) -> i64
  %274 = "arith.addi"(%arg5, %273) : (i64, i64) -> i64
  "linalg.yield"(%274) : (i64) -> ()
}) {linalg.memoized_indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>]} : (memref<17xi64>, tensor<17xi64>, tensor<i64>) -> tensor<i64>
generated/temp261854397533981204480.mlir:797:20: error: All operations with attached regions need to implement the RegionBranchOpInterface.
      %mapped_28 = linalg.map ins(%10 : tensor<?x?xi32>) outs(%136 : tensor<?x?xi32>)
                   ^
generated/temp261854397533981204480.mlir:797:20: note: see current operation: 
%217 = "linalg.map"(%58, %216) ({
^bb0(%arg2: i32):
  %233 = "arith.constant"() <{value = true}> : () -> i1
  %234 = "math.fpowi"(%144, %142) <{fastmath = #arith.fastmath<none>}> : (f32, i32) -> f32
  %235 = "arith.constant"() <{value = dense<1.000000e+00> : vector<17x1x5xf16>}> : () -> vector<17x1x5xf16>
  %236 = "vector.broadcast"(%162) : (f16) -> vector<1x5xf16>
  %237:2 = "vector.scan"(%235, %236) <{inclusive = false, kind = #vector.kind<mul>, reduction_dim = 0 : i64}> : (vector<17x1x5xf16>, vector<1x5xf16>) -> (vector<17x1x5xf16>, vector<1x5xf16>)
  %238 = "vector.broadcast"(%133) : (i32) -> vector<i32>
  %239 = "vector.transfer_write"(%238, %54, %92) <{operandSegmentSizes = array<i32: 1, 1, 1, 0>, permutation_map = affine_map<(d0) -> ()>}> : (vector<i32>, tensor<5xi32>, index) -> tensor<5xi32>
  %240 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<5x17xi1>
  %241 = "tensor.empty"(%34) : (index) -> tensor<?x17xi1>
  %242 = "linalg.matmul"(%77, %240, %241) <{operandSegmentSizes = array<i32: 2, 1>}> ({
  ^bb0(%arg3: i1, %arg4: i1, %arg5: i1):
    %273 = "arith.andi"(%arg3, %arg4) : (i1, i1) -> i1
    %274 = "arith.ori"(%arg5, %273) : (i1, i1) -> i1
    "linalg.yield"(%274) : (i1) -> ()
  }) {linalg.memoized_indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>]} : (memref<?x5xi1>, memref<5x17xi1>, tensor<?x17xi1>) -> tensor<?x17xi1>
  %243 = "index.or"(%159, %86) : (index, index) -> index
  %244 = "tensor.insert"(%1, %57, %16) : (i64, tensor<?xi64>, index) -> tensor<?xi64>
  %245 = "arith.constant"() <{value = 0.451181591 : f32}> : () -> f32
  %246 = "math.fpowi"(%5, %7) <{fastmath = #arith.fastmath<none>}> : (f32, i32) -> f32
  %247 = "arith.constant"() <{value = dense<2> : vector<17xindex>}> : () -> vector<17xindex>
  "vector.compressstore"(%75, %16, %102, %102) : (memref<?xi1>, index, vector<1xi1>, vector<1xi1>) -> ()
  %248 = "arith.constant"() <{value = dense<468005861> : vector<1xi32>}> : () -> vector<1xi32>
  %249 = "vector.maskedload"(%164, %18, %102, %248) : (memref<5xi32>, index, vector<1xi1>, vector<1xi32>) -> vector<1xi32>
  %250 = "vector.insert"(%1, %140) <{static_position = array<i64: 0>}> : (i64, vector<2xi64>) -> vector<2xi64>
  %251 = "vector.flat_transpose"(%128) <{columns = 5 : i32, rows = 1 : i32}> : (vector<5xi32>) -> vector<5xi32>
  %252 = "arith.constant"() <{value = dense<2.04775539E+9> : vector<32x17xf32>}> : () -> vector<32x17xf32>
  %253 = "vector.fma"(%252, %252, %252) : (vector<32x17xf32>, vector<32x17xf32>, vector<32x17xf32>) -> vector<32x17xf32>
  %254 = "vector.mask"(%87) ({
    "vector.yield"(%87) : (vector<5xi1>) -> ()
  }) : (vector<5xi1>) -> vector<5xi1>
  %255 = "arith.andi"(%161, %134) : (i1, i1) -> i1
  %256 = "arith.divf"(%131, %3) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
  %257 = "vector.broadcast"(%104) : (f32) -> vector<32xf32>
  %258:2 = "vector.scan"(%253, %257) <{inclusive = true, kind = #vector.kind<maxf>, reduction_dim = 1 : i64}> : (vector<32x17xf32>, vector<32xf32>) -> (vector<32x17xf32>, vector<32xf32>)
  %259 = "tensor.empty"(%159, %37) : (index, index) -> tensor<?x?x1xi32>
  %260 = "linalg.broadcast"(%58, %259) <{dimensions = array<i64: 2>}> ({
  ^bb0(%arg3: i32, %arg4: i32):
    "linalg.yield"(%arg3) : (i32) -> ()
  }) : (tensor<?x?xi32>, tensor<?x?x1xi32>) -> tensor<?x?x1xi32>
  %261 = "index.castu"(%134) : (i1) -> index
  %262 = "math.fma"(%131, %84, %103) <{fastmath = #arith.fastmath<none>}> : (f32, f32, f32) -> f32
  %263 = "tensor.empty"() : () -> tensor<17xi64>
  %264 = "linalg.dot"(%arg1, %263, %90) <{operandSegmentSizes = array<i32: 2, 1>}> ({
  ^bb0(%arg3: i64, %arg4: i64, %arg5: i64):
    %273 = "arith.muli"(%arg3, %arg4) : (i64, i64) -> i64
    %274 = "arith.addi"(%arg5, %273) : (i64, i64) -> i64
    "linalg.yield"(%274) : (i64) -> ()
  }) {linalg.memoized_indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>]} : (memref<17xi64>, tensor<17xi64>, tensor<i64>) -> tensor<i64>
  %265 = "tensor.empty"() : () -> tensor<5xi1>
  %266 = "arith.constant"() <{value = 0x4DB83349 : f32}> : () -> f32
  %267 = "index.shru"(%28, %17) : (index, index) -> index
  "vector.print"(%112) <{punctuation = #vector.punctuation<newline>}> : (vector<2xi32>) -> ()
  %268 = "index.maxu"(%267, %40) : (index, index) -> index
  %269 = "index.castu"(%149) : (index) -> i32
  %270 = "math.roundeven"(%114) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
  %271 = "arith.ceildivsi"(%arg2, %arg2) : (i32, i32) -> i32
  %272 = "index.shl"(%92, %32) : (index, index) -> index
  "linalg.yield"(%142) : (i32) -> ()
}) : (tensor<?x?xi32>, tensor<?x?xi32>) -> tensor<?x?xi32>
generated/temp261854397533981204480.mlir:849:14: error: All operations with attached regions need to implement the RegionBranchOpInterface.
      %142 = vector.mask %32 { vector.multi_reduction <mul>, %32, %33 [] : vector<1xi1> to vector<1xi1> } : vector<1xi1> -> vector<1xi1>
             ^
generated/temp261854397533981204480.mlir:849:14: note: see current operation: 
%224 = "vector.mask"(%101) ({
  "vector.yield"(%101) : (vector<1xi1>) -> ()
}) : (vector<1xi1>) -> vector<1xi1>
generated/temp261854397533981204480.mlir:950:20: error: All operations with attached regions need to implement the RegionBranchOpInterface.
            %186 = linalg.copy ins(%24 : tensor<i64>) outs(%25 : tensor<i64>) -> tensor<i64>
                   ^
generated/temp261854397533981204480.mlir:950:20: note: see current operation: 
%276 = "linalg.copy"(%90, %91) <{operandSegmentSizes = array<i32: 1, 1>}> ({
^bb0(%arg6: i64, %arg7: i64):
  "linalg.yield"(%arg6) : (i64) -> ()
}) : (tensor<i64>, tensor<i64>) -> tensor<i64>
generated/temp261854397533981204480.mlir:915:22: error: All operations with attached regions need to implement the RegionBranchOpInterface.
        %mapped_29 = linalg.map ins(%15, %15, %15 : tensor<?x?xi16>, tensor<?x?xi16>, tensor<?x?xi16>) outs(%144 : tensor<?x?xi16>)
                     ^
generated/temp261854397533981204480.mlir:915:22: note: see current operation: 
%225 = "linalg.map"(%63, %63, %63, %224) ({
^bb0(%arg3: i16, %arg4: i16, %arg5: i16):
  %246 = "vector.insert"(%arg3, %127) <{static_position = array<i64: 3>}> : (i16, vector<5xi16>) -> vector<5xi16>
  %247 = "math.absi"(%58) : (tensor<?x?xi32>) -> tensor<?x?xi32>
  %248 = "arith.xori"(%205, %161) : (i1, i1) -> i1
  "memref.dealloc"(%99) : (memref<?xi1>) -> ()
  %249 = "tensor.collapse_shape"(%49) <{reassociation = [[0, 1]]}> : (tensor<?x5xi64>) -> tensor<?xi64>
  %250 = "math.tanh"(%50) <{fastmath = #arith.fastmath<none>}> : (tensor<?x?xf16>) -> tensor<?x?xf16>
  %251 = "arith.constant"() <{value = dense<-3517> : vector<17xi16>}> : () -> vector<17xi16>
  %252 = "index.casts"(%153) : (i32) -> index
  "affine.store"(%152, %73, %19) <{map = affine_map<(d0) -> (d0)>}> : (f32, memref<5xf32>, index) -> ()
  %253 = "vector.flat_transpose"(%220) <{columns = 8 : i32, rows = 4 : i32}> : (vector<32xi1>) -> vector<32xi1>
  %254 = "arith.addi"(%arg5, %12) : (i16, i16) -> i16
  %255 = "arith.addf"(%157, %14) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
  %256 = "arith.constant"() <{value = dense<324534398> : vector<32x17xi64>}> : () -> vector<32x17xi64>
  %257 = "math.absf"(%62) <{fastmath = #arith.fastmath<none>}> : (tensor<?x?xf32>) -> tensor<?x?xf32>
  %258 = "tensor.dim"(%63, %17) : (tensor<?x?xi16>, index) -> index
  %259 = "arith.xori"(%172, %122) : (i1, i1) -> i1
  %260 = "vector.reduction"(%219) <{fastmath = #arith.fastmath<none>, kind = #vector.kind<minsi>}> : (vector<32xi1>) -> i1
  %261 = "index.divs"(%39, %159) : (index, index) -> index
  %262 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<5x5xi64>
  %263 = "arith.constant"() <{value = dense<324534398> : vector<5x5xi64>}> : () -> vector<5x5xi64>
  %264 = "vector.broadcast"(%176) : (i1) -> vector<5x5xi1>
  %265 = "vector.broadcast"(%153) : (i32) -> vector<5x5xi32>
  %266 = "vector.gather"(%262, %261, %38, %265, %264, %263) : (memref<5x5xi64>, index, index, vector<5x5xi32>, vector<5x5xi1>, vector<5x5xi64>) -> vector<5x5xi64>
  %267 = "arith.minsi"(%122, %168) : (i1, i1) -> i1
  %268 = "arith.constant"() <{value = true}> : () -> i1
  %269 = "arith.addi"(%153, %81) : (i32, i32) -> i32
  %270 = "vector.broadcast"(%131) : (f32) -> vector<5xf32>
  %271 = "arith.constant"() <{value = -1.42591296E+9 : f32}> : () -> f32
  %272 = "index.bool.constant"() <{value = false}> : () -> i1
  "memref.copy"(%68, %68) : (memref<5xi1>, memref<5xi1>) -> ()
  %273 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32x17xf32>
  %274 = "arith.constant"() <{value = false}> : () -> i1
  %275 = "math.log2"(%131) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
  %276 = "linalg.copy"(%90, %91) <{operandSegmentSizes = array<i32: 1, 1>}> ({
  ^bb0(%arg6: i64, %arg7: i64):
    "linalg.yield"(%arg6) : (i64) -> ()
  }) : (tensor<i64>, tensor<i64>) -> tensor<i64>
  %277 = "math.fpowi"(%arg0, %133) <{fastmath = #arith.fastmath<none>}> : (f32, i32) -> f32
  %278 = "arith.constant"() <{value = dense<1.18634624E+9> : vector<5x5xf32>}> : () -> vector<5x5xf32>
  %279 = "vector.fma"(%278, %278, %278) : (vector<5x5xf32>, vector<5x5xf32>, vector<5x5xf32>) -> vector<5x5xf32>
  "linalg.yield"(%arg5) : (i16) -> ()
}) : (tensor<?x?xi16>, tensor<?x?xi16>, tensor<?x?xi16>, tensor<?x?xi16>) -> tensor<?x?xi16>
generated/temp261854397533981204480.mlir:995:27: error: All operations with attached regions need to implement the RegionBranchOpInterface.
            %transposed = linalg.transpose ins(%0 : tensor<32x17xi32>) outs(%176 : tensor<17x32xi32>) permutation = [1, 0] 
                          ^
generated/temp261854397533981204480.mlir:995:27: note: see current operation: 
%267 = "linalg.transpose"(%48, %266) <{permutation = array<i64: 1, 0>}> ({
^bb0(%arg4: i32, %arg5: i32):
  "linalg.yield"(%arg4) : (i32) -> ()
}) : (tensor<32x17xi32>, tensor<17x32xi32>) -> tensor<17x32xi32>
generated/temp261854397533981204480.mlir:970:22: error: All operations with attached regions need to implement the RegionBranchOpInterface.
        %mapped_30 = linalg.map ins(%alloc_19 : memref<?xi16>) outs(%155 : tensor<?xi16>)
                     ^
generated/temp261854397533981204480.mlir:970:22: note: see current operation: 
%237 = "linalg.map"(%79, %236) ({
^bb0(%arg3: i16):
  %246 = "math.absf"(%179) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
  %247 = "arith.constant"() <{value = false}> : () -> i1
  %248 = "vector.transfer_read"(%77, %93, %38, %247) <{operandSegmentSizes = array<i32: 1, 2, 1, 0>, permutation_map = affine_map<(d0, d1) -> (d0)>}> : (memref<?x5xi1>, index, index, i1) -> vector<1xi1>
  %249 = "arith.shrui"(%1, %1) : (i64, i64) -> i64
  %250 = "memref.load"(%74, %16, %16) : (memref<?x?xf32>, index, index) -> f32
  %251 = "arith.remsi"(%arg1, %4) : (i32, i32) -> i32
  %252 = "math.log1p"(%162) <{fastmath = #arith.fastmath<none>}> : (f16) -> f16
  %253 = "math.fpowi"(%114, %81) <{fastmath = #arith.fastmath<none>}> : (f32, i32) -> f32
  %254 = "tensor.collapse_shape"(%62) <{reassociation = [[0, 1]]}> : (tensor<?x?xf32>) -> tensor<?xf32>
  %255 = "arith.remf"(%180, %103) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
  %256 = "memref.alloc"(%40, %46) <{operandSegmentSizes = array<i32: 2, 0>}> : (index, index) -> memref<?x?xi16>
  "memref.tensor_store"(%63, %256) : (tensor<?x?xi16>, memref<?x?xi16>) -> ()
  %257 = "bufferization.clone"(%72) : (memref<5x5xf16>) -> memref<5x5xf16>
  %258 = "arith.constant"() <{value = -2690 : i16}> : () -> i16
  %259 = "index.shru"(%46, %145) : (index, index) -> index
  %260 = "vector.broadcast"(%155) : (f32) -> vector<1xf32>
  "vector.compressstore"(%73, %19, %101, %260) : (memref<5xf32>, index, vector<1xi1>, vector<1xf32>) -> ()
  %261 = "tensor.collapse_shape"(%53) <{reassociation = [[0, 1]]}> : (tensor<?x5xi64>) -> tensor<?xi64>
  %262:2 = "vector.scan"(%189, %220) <{inclusive = false, kind = #vector.kind<add>, reduction_dim = 1 : i64}> : (vector<32x17xi1>, vector<32xi1>) -> (vector<32x17xi1>, vector<32xi1>)
  %263 = "vector.broadcast"(%103) : (f32) -> vector<5x5xf32>
  %264 = "math.copysign"(%3, %170) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
  %265 = "arith.constant"() <{value = 17 : index}> : () -> index
  %266 = "tensor.empty"() : () -> tensor<17x32xi32>
  %267 = "linalg.transpose"(%48, %266) <{permutation = array<i64: 1, 0>}> ({
  ^bb0(%arg4: i32, %arg5: i32):
    "linalg.yield"(%arg4) : (i32) -> ()
  }) : (tensor<32x17xi32>, tensor<17x32xi32>) -> tensor<17x32xi32>
  %268 = "index.bool.constant"() <{value = true}> : () -> i1
  %269 = "index.ceildivs"(%19, %34) : (index, index) -> index
  %270 = "arith.constant"() <{value = 2 : index}> : () -> index
  %271 = "affine.load"(%76, %24, %30) <{map = affine_map<(d0, d1) -> (d0, d1)>}> : (memref<?x?xf32>, index, index) -> f32
  %272:2 = "vector.scan"(%189, %223) <{inclusive = true, kind = #vector.kind<minsi>, reduction_dim = 0 : i64}> : (vector<32x17xi1>, vector<17xi1>) -> (vector<32x17xi1>, vector<17xi1>)
  %273 = "arith.mulf"(%114, %11) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
  %274 = "tensor.collapse_shape"(%61) <{reassociation = [[0, 1]]}> : (tensor<32x17xi1>) -> tensor<544xi1>
  "memref.assume_alignment"(%75) <{alignment = 4 : i32}> : (memref<?xi1>) -> ()
  %275 = "index.bool.constant"() <{value = true}> : () -> i1
  %276 = "arith.divf"(%163, %107) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
  %277 = "memref.load"(%76, %16, %16) : (memref<?x?xf32>, index, index) -> f32
  %278 = "memref.alloc"(%30, %269) <{operandSegmentSizes = array<i32: 2, 0>}> : (index, index) -> memref<?x?xi16>
  "memref.tensor_store"(%63, %278) : (tensor<?x?xi16>, memref<?x?xi16>) -> ()
  "linalg.yield"(%151) : (i16) -> ()
}) : (memref<?xi16>, tensor<?xi16>) -> tensor<?xi16>
generated/temp261854397533981204480.mlir:1011:25: error: All operations with attached regions need to implement the RegionBranchOpInterface.
        %generated_31 = tensor.generate %c31, %c27 {
                        ^
generated/temp261854397533981204480.mlir:1011:25: note: see current operation: 
%238 = "tensor.generate"(%47, %43) ({
^bb0(%arg3: index, %arg4: index):
  %246 = "vector.flat_transpose"(%223) <{columns = 17 : i32, rows = 1 : i32}> : (vector<17xi1>) -> vector<17xi1>
  %247 = "vector.shuffle"(%222, %123) <{mask = [0, 1, 2, 4, 6]}> : (vector<5xi64>, vector<5xi64>) -> vector<5xi64>
  %248 = "index.sizeof"() : () -> index
  %249 = "arith.divui"(%168, %176) : (i1, i1) -> i1
  "tensor.yield"(%9) : (i1) -> ()
}) : (index, index) -> tensor<?x?xi1>
generated/temp261854397533981204480.mlir:1047:16: error: All operations with attached regions need to implement the RegionBranchOpInterface.
        %157 = vector.mask %33 { vector.multi_reduction <minui>, %32, %32 [] : vector<1xi1> to vector<1xi1> } : vector<1xi1> -> vector<1xi1>
               ^
generated/temp261854397533981204480.mlir:1047:16: note: see current operation: 
%241 = "vector.mask"(%102) ({
  "vector.yield"(%101) : (vector<1xi1>) -> ()
}) : (vector<1xi1>) -> vector<1xi1>
generated/temp261854397533981204480.mlir:1084:20: error: All operations with attached regions need to implement the RegionBranchOpInterface.
            %189 = linalg.dot ins(%alloc_10, %alloc_10 : memref<5xi64>, memref<5xi64>) outs(%24 : tensor<i64>) -> tensor<i64>
                   ^
generated/temp261854397533981204480.mlir:1084:20: note: see current operation: 
%275 = "linalg.dot"(%70, %70, %90) <{operandSegmentSizes = array<i32: 2, 1>}> ({
^bb0(%arg4: i64, %arg5: i64, %arg6: i64):
  %276 = "arith.muli"(%arg4, %arg5) : (i64, i64) -> i64
  %277 = "arith.addi"(%arg6, %276) : (i64, i64) -> i64
  "linalg.yield"(%277) : (i64) -> ()
}) {linalg.memoized_indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>]} : (memref<5xi64>, memref<5xi64>, tensor<i64>) -> tensor<i64>
generated/temp261854397533981204480.mlir:1050:22: error: All operations with attached regions need to implement the RegionBranchOpInterface.
        %mapped_32 = linalg.map ins(%alloc_12 : memref<5x5xf16>) outs(%159 : tensor<5x5xf16>)
                     ^
generated/temp261854397533981204480.mlir:1050:22: note: see current operation: 
%244 = "linalg.map"(%72, %243) ({
^bb0(%arg3: f16):
  %246 = "memref.realloc"(%73) : (memref<5xf32>) -> memref<17xf32>
  %247 = "arith.constant"() <{value = false}> : () -> i1
  %248 = "vector.multi_reduction"(%234, %151) <{kind = #vector.kind<and>, reduction_dims = [0]}> : (vector<5xi16>, i16) -> i16
  %249:2 = "vector.scan"(%189, %223) <{inclusive = true, kind = #vector.kind<or>, reduction_dim = 0 : i64}> : (vector<32x17xi1>, vector<17xi1>) -> (vector<32x17xi1>, vector<17xi1>)
  %250 = "math.exp"(%103) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
  %251 = "tensor.collapse_shape"(%58) <{reassociation = [[0, 1]]}> : (tensor<?x?xi32>) -> tensor<?xi32>
  %252 = "vector.insert"(%168, %87) <{static_position = array<i64: 1>}> : (i1, vector<5xi1>) -> vector<5xi1>
  %253 = "arith.xori"(%173, %205) : (i1, i1) -> i1
  %254 = "arith.cmpf"(%180, %5) <{predicate = 12 : i64}> : (f32, f32) -> i1
  %255 = "index.floordivs"(%80, %44) : (index, index) -> index
  %256 = "arith.floordivsi"(%arg1, %153) : (i32, i32) -> i32
  "vector.print"(%203) <{punctuation = #vector.punctuation<newline>}> : (vector<1xi16>) -> ()
  %257 = "vector.extract"(%234) <{static_position = array<i64: 1>}> : (vector<5xi16>) -> i16
  %258 = "arith.remf"(%174, %152) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
  %259 = "vector.broadcast"(%207) : (f32) -> vector<17xf32>
  %260 = "vector.maskedload"(%74, %16, %16, %223, %259) : (memref<?x?xf32>, index, index, vector<17xi1>, vector<17xf32>) -> vector<17xf32>
  "linalg.transpose"(%238, %64) <{permutation = array<i64: 1, 0>}> ({
  ^bb0(%arg4: i1, %arg5: i1):
    "linalg.yield"(%arg4) : (i1) -> ()
  }) : (tensor<?x?xi1>, memref<?x?xi1>) -> ()
  %261 = "index.shru"(%31, %200) : (index, index) -> index
  %262 = "affine.load"(%164, %22) <{map = affine_map<(d0) -> (d0)>}> : (memref<5xi32>, index) -> i32
  %263 = "index.divu"(%194, %44) : (index, index) -> index
  %264 = "arith.addf"(%131, %169) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
  %265 = "arith.ceildivsi"(%13, %134) : (i1, i1) -> i1
  %266 = "affine.apply"(%44, %149, %42) <{map = affine_map<(d0, d1, d2) -> (d1 - d2 floordiv 128)>}> : (index, index, index) -> index
  %267 = "arith.shli"(%161, %161) : (i1, i1) -> i1
  %268 = "arith.constant"() <{value = false}> : () -> i1
  %269 = "arith.constant"() <{value = true}> : () -> i1
  %270 = "arith.muli"(%248, %151) : (i16, i16) -> i16
  %271 = "math.ctpop"(%133) : (i32) -> i32
  %272 = "arith.addi"(%177, %262) : (i32, i32) -> i32
  %273 = "tensor.expand_shape"(%190) <{reassociation = [[0], [1], [2, 3]]}> : (tensor<32x17x1xi16>) -> tensor<32x17x1x1xi16>
  %274 = "vector.insert"(%1, %217) <{static_position = array<i64: 4>}> : (i64, vector<5xi64>) -> vector<5xi64>
  %275 = "linalg.dot"(%70, %70, %90) <{operandSegmentSizes = array<i32: 2, 1>}> ({
  ^bb0(%arg4: i64, %arg5: i64, %arg6: i64):
    %276 = "arith.muli"(%arg4, %arg5) : (i64, i64) -> i64
    %277 = "arith.addi"(%arg6, %276) : (i64, i64) -> i64
    "linalg.yield"(%277) : (i64) -> ()
  }) {linalg.memoized_indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>]} : (memref<5xi64>, memref<5xi64>, tensor<i64>) -> tensor<i64>
  "linalg.yield"(%94) : (f16) -> ()
}) : (memref<5x5xf16>, tensor<5x5xf16>) -> tensor<5x5xf16>
generated/temp261854397533981204480.mlir:904:15: error: All operations with attached regions need to implement the RegionBranchOpInterface.
    %mapped = linalg.map ins(%alloc_21, %alloc_21 : memref<?x32xi32>, memref<?x32xi32>) outs(%131 : tensor<?x32xi32>)
              ^
generated/temp261854397533981204480.mlir:904:15: note: see current operation: 
%210 = "linalg.map"(%109, %109, %209) ({
^bb0(%arg1: i32, %arg2: i32):
  %216 = "math.fma"(%103, %131, %arg0) <{fastmath = #arith.fastmath<none>}> : (f32, f32, f32) -> f32
  %217 = "affine.vector_load"(%67, %39) <{map = affine_map<(d0) -> (d0)>}> : (memref<17xi64>, index) -> vector<5xi64>
  %218 = "math.absi"(%205) : (i1) -> i1
  %219 = "arith.constant"() <{value = dense<false> : vector<32xi1>}> : () -> vector<32xi1>
  %220 = "vector.maskedload"(%68, %18, %219, %219) : (memref<5xi1>, index, vector<32xi1>, vector<32xi1>) -> vector<32xi1>
  %221 = "index.maxu"(%200, %43) : (index, index) -> index
  %222 = "vector.flat_transpose"(%88) <{columns = 5 : i32, rows = 1 : i32}> : (vector<5xi64>) -> vector<5xi64>
  %223 = "affine.vector_load"(%75, %26) <{map = affine_map<(d0) -> (d0)>}> : (memref<?xi1>, index) -> vector<17xi1>
  %224 = "tensor.empty"(%44, %38) : (index, index) -> tensor<?x?xi16>
  %225 = "linalg.map"(%63, %63, %63, %224) ({
  ^bb0(%arg3: i16, %arg4: i16, %arg5: i16):
    %246 = "vector.insert"(%arg3, %127) <{static_position = array<i64: 3>}> : (i16, vector<5xi16>) -> vector<5xi16>
    %247 = "math.absi"(%58) : (tensor<?x?xi32>) -> tensor<?x?xi32>
    %248 = "arith.xori"(%205, %161) : (i1, i1) -> i1
    "memref.dealloc"(%99) : (memref<?xi1>) -> ()
    %249 = "tensor.collapse_shape"(%49) <{reassociation = [[0, 1]]}> : (tensor<?x5xi64>) -> tensor<?xi64>
    %250 = "math.tanh"(%50) <{fastmath = #arith.fastmath<none>}> : (tensor<?x?xf16>) -> tensor<?x?xf16>
    %251 = "arith.constant"() <{value = dense<-3517> : vector<17xi16>}> : () -> vector<17xi16>
    %252 = "index.casts"(%153) : (i32) -> index
    "affine.store"(%152, %73, %19) <{map = affine_map<(d0) -> (d0)>}> : (f32, memref<5xf32>, index) -> ()
    %253 = "vector.flat_transpose"(%220) <{columns = 8 : i32, rows = 4 : i32}> : (vector<32xi1>) -> vector<32xi1>
    %254 = "arith.addi"(%arg5, %12) : (i16, i16) -> i16
    %255 = "arith.addf"(%157, %14) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %256 = "arith.constant"() <{value = dense<324534398> : vector<32x17xi64>}> : () -> vector<32x17xi64>
    %257 = "math.absf"(%62) <{fastmath = #arith.fastmath<none>}> : (tensor<?x?xf32>) -> tensor<?x?xf32>
    %258 = "tensor.dim"(%63, %17) : (tensor<?x?xi16>, index) -> index
    %259 = "arith.xori"(%172, %122) : (i1, i1) -> i1
    %260 = "vector.reduction"(%219) <{fastmath = #arith.fastmath<none>, kind = #vector.kind<minsi>}> : (vector<32xi1>) -> i1
    %261 = "index.divs"(%39, %159) : (index, index) -> index
    %262 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<5x5xi64>
    %263 = "arith.constant"() <{value = dense<324534398> : vector<5x5xi64>}> : () -> vector<5x5xi64>
    %264 = "vector.broadcast"(%176) : (i1) -> vector<5x5xi1>
    %265 = "vector.broadcast"(%153) : (i32) -> vector<5x5xi32>
    %266 = "vector.gather"(%262, %261, %38, %265, %264, %263) : (memref<5x5xi64>, index, index, vector<5x5xi32>, vector<5x5xi1>, vector<5x5xi64>) -> vector<5x5xi64>
    %267 = "arith.minsi"(%122, %168) : (i1, i1) -> i1
    %268 = "arith.constant"() <{value = true}> : () -> i1
    %269 = "arith.addi"(%153, %81) : (i32, i32) -> i32
    %270 = "vector.broadcast"(%131) : (f32) -> vector<5xf32>
    %271 = "arith.constant"() <{value = -1.42591296E+9 : f32}> : () -> f32
    %272 = "index.bool.constant"() <{value = false}> : () -> i1
    "memref.copy"(%68, %68) : (memref<5xi1>, memref<5xi1>) -> ()
    %273 = "memref.alloc"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<32x17xf32>
    %274 = "arith.constant"() <{value = false}> : () -> i1
    %275 = "math.log2"(%131) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %276 = "linalg.copy"(%90, %91) <{operandSegmentSizes = array<i32: 1, 1>}> ({
    ^bb0(%arg6: i64, %arg7: i64):
      "linalg.yield"(%arg6) : (i64) -> ()
    }) : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %277 = "math.fpowi"(%arg0, %133) <{fastmath = #arith.fastmath<none>}> : (f32, i32) -> f32
    %278 = "arith.constant"() <{value = dense<1.18634624E+9> : vector<5x5xf32>}> : () -> vector<5x5xf32>
    %279 = "vector.fma"(%278, %278, %278) : (vector<5x5xf32>, vector<5x5xf32>, vector<5x5xf32>) -> vector<5x5xf32>
    "linalg.yield"(%arg5) : (i16) -> ()
  }) : (tensor<?x?xi16>, tensor<?x?xi16>, tensor<?x?xi16>, tensor<?x?xi16>) -> tensor<?x?xi16>
  %226 = "arith.cmpi"(%133, %15) <{predicate = 6 : i64}> : (i32, i32) -> i1
  %227 = "math.fpowi"(%118, %4) <{fastmath = #arith.fastmath<none>}> : (f32, i32) -> f32
  "memref.copy"(%78, %78) : (memref<?xf32>, memref<?xf32>) -> ()
  %228 = "arith.constant"() <{value = dense<1.18634624E+9> : vector<32xf32>}> : () -> vector<32xf32>
  "vector.compressstore"(%74, %16, %16, %219, %228) : (memref<?x?xf32>, index, index, vector<32xi1>, vector<32xf32>) -> ()
  "vector.compressstore"(%79, %16, %87, %206) : (memref<?xi16>, index, vector<5xi1>, vector<5xi16>) -> ()
  %229 = "arith.divsi"(%134, %176) : (i1, i1) -> i1
  %230 = "index.shru"(%194, %27) : (index, index) -> index
  %231 = "vector.contract"(%128, %128, %arg2) <{indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = [#vector.iterator_type<reduction>], kind = #vector.kind<or>}> : (vector<5xi32>, vector<5xi32>, i32) -> i32
  %232 = "arith.divui"(%176, %134) : (i1, i1) -> i1
  %233 = "arith.cmpf"(%154, %152) <{predicate = 0 : i64}> : (f32, f32) -> i1
  %234 = "vector.flat_transpose"(%206) <{columns = 5 : i32, rows = 1 : i32}> : (vector<5xi16>) -> vector<5xi16>
  %235 = "arith.constant"() <{value = dense<true> : vector<5xi1>}> : () -> vector<5xi1>
  %236 = "tensor.empty"(%29) : (index) -> tensor<?xi16>
  %237 = "linalg.map"(%79, %236) ({
  ^bb0(%arg3: i16):
    %246 = "math.absf"(%179) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %247 = "arith.constant"() <{value = false}> : () -> i1
    %248 = "vector.transfer_read"(%77, %93, %38, %247) <{operandSegmentSizes = array<i32: 1, 2, 1, 0>, permutation_map = affine_map<(d0, d1) -> (d0)>}> : (memref<?x5xi1>, index, index, i1) -> vector<1xi1>
    %249 = "arith.shrui"(%1, %1) : (i64, i64) -> i64
    %250 = "memref.load"(%74, %16, %16) : (memref<?x?xf32>, index, index) -> f32
    %251 = "arith.remsi"(%arg1, %4) : (i32, i32) -> i32
    %252 = "math.log1p"(%162) <{fastmath = #arith.fastmath<none>}> : (f16) -> f16
    %253 = "math.fpowi"(%114, %81) <{fastmath = #arith.fastmath<none>}> : (f32, i32) -> f32
    %254 = "tensor.collapse_shape"(%62) <{reassociation = [[0, 1]]}> : (tensor<?x?xf32>) -> tensor<?xf32>
    %255 = "arith.remf"(%180, %103) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %256 = "memref.alloc"(%40, %46) <{operandSegmentSizes = array<i32: 2, 0>}> : (index, index) -> memref<?x?xi16>
    "memref.tensor_store"(%63, %256) : (tensor<?x?xi16>, memref<?x?xi16>) -> ()
    %257 = "bufferization.clone"(%72) : (memref<5x5xf16>) -> memref<5x5xf16>
    %258 = "arith.constant"() <{value = -2690 : i16}> : () -> i16
    %259 = "index.shru"(%46, %145) : (index, index) -> index
    %260 = "vector.broadcast"(%155) : (f32) -> vector<1xf32>
    "vector.compressstore"(%73, %19, %101, %260) : (memref<5xf32>, index, vector<1xi1>, vector<1xf32>) -> ()
    %261 = "tensor.collapse_shape"(%53) <{reassociation = [[0, 1]]}> : (tensor<?x5xi64>) -> tensor<?xi64>
    %262:2 = "vector.scan"(%189, %220) <{inclusive = false, kind = #vector.kind<add>, reduction_dim = 1 : i64}> : (vector<32x17xi1>, vector<32xi1>) -> (vector<32x17xi1>, vector<32xi1>)
    %263 = "vector.broadcast"(%103) : (f32) -> vector<5x5xf32>
    %264 = "math.copysign"(%3, %170) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %265 = "arith.constant"() <{value = 17 : index}> : () -> index
    %266 = "tensor.empty"() : () -> tensor<17x32xi32>
    %267 = "linalg.transpose"(%48, %266) <{permutation = array<i64: 1, 0>}> ({
    ^bb0(%arg4: i32, %arg5: i32):
      "linalg.yield"(%arg4) : (i32) -> ()
    }) : (tensor<32x17xi32>, tensor<17x32xi32>) -> tensor<17x32xi32>
    %268 = "index.bool.constant"() <{value = true}> : () -> i1
    %269 = "index.ceildivs"(%19, %34) : (index, index) -> index
    %270 = "arith.constant"() <{value = 2 : index}> : () -> index
    %271 = "affine.load"(%76, %24, %30) <{map = affine_map<(d0, d1) -> (d0, d1)>}> : (memref<?x?xf32>, index, index) -> f32
    %272:2 = "vector.scan"(%189, %223) <{inclusive = true, kind = #vector.kind<minsi>, reduction_dim = 0 : i64}> : (vector<32x17xi1>, vector<17xi1>) -> (vector<32x17xi1>, vector<17xi1>)
    %273 = "arith.mulf"(%114, %11) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %274 = "tensor.collapse_shape"(%61) <{reassociation = [[0, 1]]}> : (tensor<32x17xi1>) -> tensor<544xi1>
    "memref.assume_alignment"(%75) <{alignment = 4 : i32}> : (memref<?xi1>) -> ()
    %275 = "index.bool.constant"() <{value = true}> : () -> i1
    %276 = "arith.divf"(%163, %107) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %277 = "memref.load"(%76, %16, %16) : (memref<?x?xf32>, index, index) -> f32
    %278 = "memref.alloc"(%30, %269) <{operandSegmentSizes = array<i32: 2, 0>}> : (index, index) -> memref<?x?xi16>
    "memref.tensor_store"(%63, %278) : (tensor<?x?xi16>, memref<?x?xi16>) -> ()
    "linalg.yield"(%151) : (i16) -> ()
  }) : (memref<?xi16>, tensor<?xi16>) -> tensor<?xi16>
  %238 = "tensor.generate"(%47, %43) ({
  ^bb0(%arg3: index, %arg4: index):
    %246 = "vector.flat_transpose"(%223) <{columns = 17 : i32, rows = 1 : i32}> : (vector<17xi1>) -> vector<17xi1>
    %247 = "vector.shuffle"(%222, %123) <{mask = [0, 1, 2, 4, 6]}> : (vector<5xi64>, vector<5xi64>) -> vector<5xi64>
    %248 = "index.sizeof"() : () -> index
    %249 = "arith.divui"(%168, %176) : (i1, i1) -> i1
    "tensor.yield"(%9) : (i1) -> ()
  }) : (index, index) -> tensor<?x?xi1>
  "scf.parallel"(%42, %37, %33, %18, %29, %28) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg3: index, %arg4: index):
    %246:2 = "vector.scan"(%189, %223) <{inclusive = true, kind = #vector.kind<minui>, reduction_dim = 0 : i64}> : (vector<32x17xi1>, vector<17xi1>) -> (vector<32x17xi1>, vector<17xi1>)
    %247 = "tensor.empty"() : () -> tensor<5x5xi64>
    %248 = "arith.constant"() <{value = dense<324534398> : vector<32x17xi64>}> : () -> vector<32x17xi64>
    %249 = "arith.constant"() <{value = dense<468005861> : vector<32x17xi32>}> : () -> vector<32x17xi32>
    %250 = "vector.gather"(%247, %25, %28, %249, %189, %248) : (tensor<5x5xi64>, index, index, vector<32x17xi32>, vector<32x17xi1>, vector<32x17xi64>) -> vector<32x17xi64>
    %251 = "arith.constant"() <{value = false}> : () -> i1
    "affine.store"(%1, %67, %28) <{map = affine_map<(d0) -> (d0)>}> : (i64, memref<17xi64>, index) -> ()
    %252 = "tensor.cast"(%98) : (tensor<?xi1>) -> tensor<1xi1>
    %253 = "tensor.empty"() : () -> tensor<544xi1>
    %254 = "tensor.unpack"(%61, %253, %33) <{inner_dims_pos = array<i64: 0>, outer_dims_perm = array<i64: 0>, static_inner_tiles = array<i64: -9223372036854775808>}> : (tensor<32x17xi1>, tensor<544xi1>, index) -> tensor<544xi1>
    %255 = "arith.floordivsi"(%141, %141) : (i16, i16) -> i16
    %256 = "arith.remf"(%arg0, %174) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %257 = "math.fma"(%120, %84, %119) <{fastmath = #arith.fastmath<none>}> : (f32, f32, f32) -> f32
    %258 = "math.ctpop"(%49) : (tensor<?x5xi64>) -> tensor<?x5xi64>
    %259:2 = "vector.scan"(%189, %219) <{inclusive = true, kind = #vector.kind<and>, reduction_dim = 1 : i64}> : (vector<32x17xi1>, vector<32xi1>) -> (vector<32x17xi1>, vector<32xi1>)
    %260 = "arith.constant"() <{value = dense<324534398> : vector<17xi64>}> : () -> vector<17xi64>
    %261:2 = "vector.scan"(%248, %260) <{inclusive = true, kind = #vector.kind<xor>, reduction_dim = 0 : i64}> : (vector<32x17xi64>, vector<17xi64>) -> (vector<32x17xi64>, vector<17xi64>)
    %262 = "math.tan"(%195) <{fastmath = #arith.fastmath<none>}> : (tensor<5x5xf16>) -> tensor<5x5xf16>
    %263 = "tensor.expand_shape"(%53) <{reassociation = [[0], [1, 2]]}> : (tensor<?x5xi64>) -> tensor<?x5x1xi64>
    %264 = "index.castu"(%38) : (index) -> i32
    %265 = "arith.subi"(%172, %168) : (i1, i1) -> i1
    "scf.yield"() : () -> ()
  }) : (index, index, index, index, index, index) -> ()
  "vector.print"(%189) <{punctuation = #vector.punctuation<newline>}> : (vector<32x17xi1>) -> ()
  %239 = "arith.divui"(%173, %9) : (i1, i1) -> i1
  "memref.tensor_store"(%236, %71) : (tensor<?xi16>, memref<?xi16>) -> ()
  %240 = "tensor.splat"(%144) : (f32) -> tensor<32x17xf32>
  %241 = "vector.mask"(%102) ({
    "vector.yield"(%101) : (vector<1xi1>) -> ()
  }) : (vector<1xi1>) -> vector<1xi1>
  %242 = "arith.shrui"(%187, %6) : (i1, i1) -> i1
  %243 = "tensor.empty"() : () -> tensor<5x5xf16>
  %244 = "linalg.map"(%72, %243) ({
  ^bb0(%arg3: f16):
    %246 = "memref.realloc"(%73) : (memref<5xf32>) -> memref<17xf32>
    %247 = "arith.constant"() <{value = false}> : () -> i1
    %248 = "vector.multi_reduction"(%234, %151) <{kind = #vector.kind<and>, reduction_dims = [0]}> : (vector<5xi16>, i16) -> i16
    %249:2 = "vector.scan"(%189, %223) <{inclusive = true, kind = #vector.kind<or>, reduction_dim = 0 : i64}> : (vector<32x17xi1>, vector<17xi1>) -> (vector<32x17xi1>, vector<17xi1>)
    %250 = "math.exp"(%103) <{fastmath = #arith.fastmath<none>}> : (f32) -> f32
    %251 = "tensor.collapse_shape"(%58) <{reassociation = [[0, 1]]}> : (tensor<?x?xi32>) -> tensor<?xi32>
    %252 = "vector.insert"(%168, %87) <{static_position = array<i64: 1>}> : (i1, vector<5xi1>) -> vector<5xi1>
    %253 = "arith.xori"(%173, %205) : (i1, i1) -> i1
    %254 = "arith.cmpf"(%180, %5) <{predicate = 12 : i64}> : (f32, f32) -> i1
    %255 = "index.floordivs"(%80, %44) : (index, index) -> index
    %256 = "arith.floordivsi"(%arg1, %153) : (i32, i32) -> i32
    "vector.print"(%203) <{punctuation = #vector.punctuation<newline>}> : (vector<1xi16>) -> ()
    %257 = "vector.extract"(%234) <{static_position = array<i64: 1>}> : (vector<5xi16>) -> i16
    %258 = "arith.remf"(%174, %152) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %259 = "vector.broadcast"(%207) : (f32) -> vector<17xf32>
    %260 = "vector.maskedload"(%74, %16, %16, %223, %259) : (memref<?x?xf32>, index, index, vector<17xi1>, vector<17xf32>) -> vector<17xf32>
    "linalg.transpose"(%238, %64) <{permutation = array<i64: 1, 0>}> ({
    ^bb0(%arg4: i1, %arg5: i1):
      "linalg.yield"(%arg4) : (i1) -> ()
    }) : (tensor<?x?xi1>, memref<?x?xi1>) -> ()
    %261 = "index.shru"(%31, %200) : (index, index) -> index
    %262 = "affine.load"(%164, %22) <{map = affine_map<(d0) -> (d0)>}> : (memref<5xi32>, index) -> i32
    %263 = "index.divu"(%194, %44) : (index, index) -> index
    %264 = "arith.addf"(%131, %169) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %265 = "arith.ceildivsi"(%13, %134) : (i1, i1) -> i1
    %266 = "affine.apply"(%44, %149, %42) <{map = affine_map<(d0, d1, d2) -> (d1 - d2 floordiv 128)>}> : (index, index, index) -> index
    %267 = "arith.shli"(%161, %161) : (i1, i1) -> i1
    %268 = "arith.constant"() <{value = false}> : () -> i1
    %269 = "arith.constant"() <{value = true}> : () -> i1
    %270 = "arith.muli"(%248, %151) : (i16, i16) -> i16
    %271 = "math.ctpop"(%133) : (i32) -> i32
    %272 = "arith.addi"(%177, %262) : (i32, i32) -> i32
    %273 = "tensor.expand_shape"(%190) <{reassociation = [[0], [1], [2, 3]]}> : (tensor<32x17x1xi16>) -> tensor<32x17x1x1xi16>
    %274 = "vector.insert"(%1, %217) <{static_position = array<i64: 4>}> : (i64, vector<5xi64>) -> vector<5xi64>
    %275 = "linalg.dot"(%70, %70, %90) <{operandSegmentSizes = array<i32: 2, 1>}> ({
    ^bb0(%arg4: i64, %arg5: i64, %arg6: i64):
      %276 = "arith.muli"(%arg4, %arg5) : (i64, i64) -> i64
      %277 = "arith.addi"(%arg6, %276) : (i64, i64) -> i64
      "linalg.yield"(%277) : (i64) -> ()
    }) {linalg.memoized_indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>]} : (memref<5xi64>, memref<5xi64>, tensor<i64>) -> tensor<i64>
    "linalg.yield"(%94) : (f16) -> ()
  }) : (memref<5x5xf16>, tensor<5x5xf16>) -> tensor<5x5xf16>
  %245 = "arith.constant"() <{value = 30.7132416 : f32}> : () -> f32
  "linalg.yield"(%10) : (i32) -> ()
}) : (memref<?x32xi32>, memref<?x32xi32>, tensor<?x32xi32>) -> tensor<?x32xi32>
mlir-opt: /home/jacob/projects/MLIRSmith/llvm/include/llvm/ADT/SmallVector.h:294: T& llvm::SmallVectorTemplateCommon<T, <template-parameter-1-2> >::operator[](llvm::SmallVectorTemplateCommon<T, <template-parameter-1-2> >::size_type) [with T = mlir::Attribute; <template-parameter-1-2> = void; llvm::SmallVectorTemplateCommon<T, <template-parameter-1-2> >::reference = mlir::Attribute&; llvm::SmallVectorTemplateCommon<T, <template-parameter-1-2> >::size_type = long unsigned int]: Assertion `idx < size()' failed.
PLEASE submit a bug report to https://github.com/llvm/llvm-project/issues/ and include the crash backtrace.
Stack dump:
0.	Program arguments: ../build/bin/mlir-opt --affine-parallelize --shape-bufferize --test-vector-transferop-opt --async-func-to-async-runtime --convert-pdl-to-pdl-interp --test-loop-permutation --buffer-deallocation --test-pdl-bytecode-pass --test-convert-call-op --mlir-disable-threading generated/temp261854397533981204480.mlir
Stack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):
0  mlir-opt        0x00005618aa657bdb llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) + 59
1  mlir-opt        0x00005618aa6549cc
2  libpthread.so.0 0x00007fdce7330980
3  libc.so.6       0x00007fdce5f5de87 gsignal + 199
4  libc.so.6       0x00007fdce5f5f7f1 abort + 321
5  libc.so.6       0x00007fdce5f4f3fa
6  libc.so.6       0x00007fdce5f4f472
7  mlir-opt        0x00005618ac27ee38
8  mlir-opt        0x00005618ac2b29a3
9  mlir-opt        0x00005618ac2c1270
10 mlir-opt        0x00005618acfc34de mlir::Operation::fold(llvm::ArrayRef<mlir::Attribute>, llvm::SmallVectorImpl<mlir::OpFoldResult>&) + 62
11 mlir-opt        0x00005618acfc387d mlir::Operation::fold(llvm::SmallVectorImpl<mlir::OpFoldResult>&) + 733
12 mlir-opt        0x00005618acf0c841 mlir::OpBuilder::tryFold(mlir::Operation*, llvm::SmallVectorImpl<mlir::Value>&) + 193
13 mlir-opt        0x00005618ace7c5d3
14 mlir-opt        0x00005618ace7d3a8
15 mlir-opt        0x00005618ace80e70 mlir::applyPartialConversion(mlir::Operation*, mlir::ConversionTarget const&, mlir::FrozenRewritePatternSet const&, llvm::DenseSet<mlir::Operation*, llvm::DenseMapInfo<mlir::Operation*, void>>*) + 96
16 mlir-opt        0x00005618ac80ba0d
17 mlir-opt        0x00005618acdcf301 mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) + 1297
18 mlir-opt        0x00005618acdd0223
19 mlir-opt        0x00005618acdd08c1 mlir::PassManager::run(mlir::Operation*) + 1153
20 mlir-opt        0x00005618acdc0efc
21 mlir-opt        0x00005618acdc28b5
22 mlir-opt        0x00005618acdc2a71
23 mlir-opt        0x00005618acec8260 mlir::splitAndProcessBuffer(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>, llvm::raw_ostream&, bool, bool) + 96
24 mlir-opt        0x00005618acdbbbc9 mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, mlir::DialectRegistry&, mlir::MlirOptMainConfig const&) + 185
25 mlir-opt        0x00005618acdc2d9a mlir::MlirOptMain(int, char**, llvm::StringRef, mlir::DialectRegistry&) + 746
26 mlir-opt        0x00005618aa5aa0c5 main + 165
27 libc.so.6       0x00007fdce5f40c87 __libc_start_main + 231
28 mlir-opt        0x00005618aa62b95a _start + 42
Aborted
